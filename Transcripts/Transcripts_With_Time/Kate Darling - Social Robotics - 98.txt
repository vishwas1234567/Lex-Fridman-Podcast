0:00:00
Speaker 1 :the following is a conversation with
 Kate darling a researcher at MIT interested in social robotics roboticsand generally how technology intersects
 with society she explores the emotional connection between human beings andlifelike machines which for me is one of
 the most exciting topics in all of artificial intelligence as she writes inher bio she's a caretaker of several
 domestic robots including her plio dinosaur robots named Yochai Peter andmr. spaghetti she is one of the funniest
 and brightest minds I've ever had the fortune to talk to this conversation wasrecorded recently but before the
 outbreak of the pandemic for everyone feeling the burden of this crisisI'm sending love your way this is the
 artificial intelligence podcast if you enjoy it subscribe on YouTube review itwith five stars an apple podcast
 supported on patreon are simply connect with me on Twitter Alex Friedman spelledFri D ma n as usual I'll do a few
 minutes of ads now and never any ads in the middle that can break the flow ofthe conversation I hope that works for
 you and doesn't hurt the listening experience quick summary of the ads tosponsors masterclass and expressvpn
 please consider supporting the podcast by signing up to master class and masterclass complex and getting expressvpn and
 expressvpn comm slash flex pod this show is sponsored by master class sign-up andmaster class comm / flex to get a
 discount and to support this podcast when I first heard about master class Ithought it was too good to be true for
 $180 a year you get an all-access pass to watch courses from the list some ofmy favorites Chris Hatfield on space
 exploration Neil deGrasse Tyson on scientific thinking and communicationwill write creator SimCity and sims love
 those games on game design Carlos Santana on guitar garry kasparov onchess daniel negreanu on poker and many
 more Chris had explaining how Rockets work and theexperience of being launched into space
 alone is worth the money by the way you can watch it on basically any deviceonce again sign up on master class comm
 / flex to get a discount and to support this podcast this show sponsored byExpress vpm get it at expressvpn comm /
 FlexPod to get a discount and to support this podcast I've been using expressvpnfor many years I love it it's easy to
 use press the big power on button and your privacy is protected and if youlike you can make it look like your
 locations anywhere else in the world I might be in Boston now but it can makeit look like I'm in New York London
 Paris or anywhere else this has a large number of obvious benefits certainly itallows you to access international
 versions of streaming websites like the Japanese Netflix or the UK Huluexpressvpn works on any device you can
 imagine I use it on Linux shout-out to bond to 2004 Windows Android but it'savailable everywhere else to once again
 get it at expressvpn comm / luxe pod to get a discount and to support thispodcast and now here's my conversation
 with Kate darling Kota robot ethics at Harvard what aresome ethical issues that arise in the


0:03:38
Speaker 0 :world with robots yeah that was a
 reading group that I did when I like at the very beginning first becameinterested in this topic so I think if I
 taught that class today would look very very different robot ethicsit sounds very science fictiony
 especially did back then but I think that some of the issues that people inrobot ethics are concerned with her just
 around the ethical use of robotic technology in general so for exampleresponsibility for harm automated weapon
 systems things like privacy and data security things like and automation andlabor markets and then personally I'm
 really interested in some of the social issues that come out of our socialrelationships with robot one-on-one


0:04:24
Speaker 1 :relationship with robot yeah I think
 most of stuff we have to talk about is like one-on-one social stuff that's whatI love and I think that's what you're
 you know as well and they're expert in but a societal oh there's like there's apresidential candidate now and Joo yang
 running concerned about automation and robots and AI and general taking awayjobs he has a proposal of ubi universal
 basic income of everybody gets a thousand bucks yeah as a way to sort ofsave you if you lose your job from
 automation to allow you time to discover what it is that you would like to or

0:05:04
Speaker 0 :even love to do yes so I lived in
 Switzerland for 20 years and universal basic income has been more of a topicthere separate from the whole robots and
 jobs issue so it's so interesting to me to see kind of these Silicon Valleypeople latch on to this concept that
 came from a very kind of left-wing socialist you knowkind of a different place in Europe
 but on the automation labor markets topic I think that it's very is sosometimes in those conversations I think
 people overestimate where robotic technology is right now and we also havethis fallacy of constantly comparing
 robots to humans and thinking of this as a one-to-one replacement of jobs so evenlike Bill Gates a few years ago said
 something about you know maybe we should have a system that taxes robots fortaking people's jobs and it just I I
 mean I'm sure that was taken out of context you know he's a really smart guybut that sounds to me like kind of
 viewing it as a one to one replacement versus viewing this technology as kindof a supplemental tool that of course is
 going to shake up a lot of stuff it's gonna change the job landscape but Idon't see you know robots taking all the
 jobs in the next 20 years that's just not how it's gonna work all right so

0:06:30
Speaker 1 :maybe drifting into the land of more
 personal relationships with robots and interaction and so on I gotta warn you Igo I may ask some silly philosophical
 questions I apologize so please do okay do you think humanswill abuse robots in their interaction
 so you've you've had a lot of and we'll talk about it sort of anthropomorphize assin and and work you know this this
 intricate dance emotional dance between human and robot but this seems to bealso a darker side what people when they
 treat the other as servants especially they can be a little bit abusive or alot abusive do you think about that do


0:07:15
Speaker 0 :you worry about that yeah I do you think
 about that so I mean one of my one of my main interests is the fact that peoplesubconsciously treat robots like living
 things and even though they know that they're interacting with a machine andwhat it means in that context to behave
 you know violently I don't know if you could say abuse because you're notactually you know abusing the the inner
 mind of the robot that robot isn't doesn't have any feelings as far as you

0:07:42
Speaker 1 :

0:07:43
Speaker 0 :know well yeah
 it was depends on how we define feelings andconsciousness but I think that's another
 area where people kind of overestimate where we currently are with thetechnology like the robots are not even
 as smart as insects right now and so I'm not worried about abuse in that sensebut it is interesting to think about
 what does people's behavior towards these things mean for our own behavioris it desensitizing the people to you
 know be verbally abusive to a robot or even physically abusive and we don't

0:08:16
Speaker 1 :know is a similar connection from like
 if you play violent video games what connection does that have to desensitizeation to violence as I haven't haven't
 read literature on that I wonder about that because everything I've heardpeople don't seem to any longer be so
 worried about violent video games correct

0:08:38
Speaker 0 :we've seemed the the research on it is
 it's a difficult thing to research so it's sort of inconclusive but we seem tohave gotten a sense at least as a
 society that people can compartmentalize when it's something on a screen andyou're like you know shooting a bunch of
 characters or running over people with your car that doesn't necessarilytranslate to you doing that in real life
 we do however have some concerns about children playing violent video games andso we do restrict it there I'm not sure
 that's based on any real evidence either but it's just the way that we've kind ofdecided you know we want to be a little
 more cautious there and the reason I think robots are a little bit differentis because there is a lot of research
 showing that we respond differently to something in our physical space thansomething on a screen we will treat it
 much more viscerally much more like a physical actor and so I it's it'stotally possible that this is not a
 problem and it's the same thing as violence in video games you know maybeyou know restrict it with kids to be
 safe but adults can do what they want but we just need to ask the questionagain because we don't have any evidence


0:09:49
Speaker 1 :at all yet maybe there's an intermediate
 place to I did my research on twitter by research I mean scrolling through yourTwitter feed
 you mentioned that you were going at some point to an animal law conferenceso I have to ask do you think there's
 something that we can learn from animal rights the guys are thinking about

0:10:13
Speaker 0 :robots oh I think there is so much to
 learn from that I'm actually writing a book on it right now that's why I'mgoing is conference so I'm I'm writing a
 book that looks at the history of animal domestication and how we've used animalsfor work for weaponry for companionship
 and you know one of the things the books the book tries to do is move away fromthis fallacy that I talked about of
 comparing robots in humans because I don't think that's the right analogy butI do think that on a social level even
 on a social level there's so much that we can learn from looking at thathistory because throughout history we've
 treated most animals like tools like products and then some of them we'vetreated differently and we're starting
 to see people treat robots in really similar ways so I think it's a reallyhelpful predictor to how we're going to


0:10:58
Speaker 1 :interact with the robots do you think
 we'll look back at this time like a hundred years from now and see what wedo to animals is like some of the way we
 view like the Holocaust with the world

0:11:13
Speaker 0 :war two that's a great question I mean I
 hope so I am not convinced that we will but I often wonder you know what are mygrandkids gonna view as you know
 abhorrent that my generation did that they would never do and I'm like wellwhat's the big deal you know it's it's a
 fun question to ask yourself

0:11:34
Speaker 1 :there's always seems that there's
 atrocities that we discover later so the things that at the time people didn'tsee as you know you look at everything
 from slavery to any kinds of abuse throughout history so I think the kindof insane wars that were happening to
 the way war was carried out and rape and the kind of violence that was happeningduring war in that we now you know we
 see his atrocities but at the time perhaps didn't as much and so nowI have this intuition that I have this
 worry maybe I'm you're going to probably criticize me but I do anthropomorphizerobots I have I don't see a fundamental
 philosophical difference in a robot in a human being in terms of once thecapabilities are matched so the fact
 that we're really far away doesn't in terms of capabilities and then that fromfrom natural language processing
 understanding generation to just reasoning and all that stuff I thinkonce you solve it I see though this is a
 very great area and I don't feel comfortable the kind of abuse thatpeople throw robots
 subtle but I can see it becoming I can see basically a civil rights movementfor robots in the future do you think
 let me put it in the form of a question do you think robots should have some

0:13:10
Speaker 0 :kinds of rights well it's interesting
 because I came at this originally from your perspective I was like you knowwhat there's no fundamental difference
 between technology and like human consciousness like we we can probablyrecreate anything we just don't know how
 yet and so there's no reason not to give machines the same rights that we haveonce like you say they're kind of on an
 equivalent level but I realized that that is kind of a far future question Istill think we should talk about it
 because I think it's really interesting but I realized that it's actually wemight need to ask the robot rice
 question even sooner than that um well the machines are still you know quoteunquote really you know dumb and not on
 our level because of the way that we perceive them and I think one of thelessons we learn from looking at the
 history of animal rights and one of the reasons we may not get to a place in ahundred years where we view it as wrong
 to you know eat or otherwise you know use animals for our own purposes isbecause historically we've always
 protected those things that we relate to the most so one example is whales no onegave a [ __ ] about the whales am I
 allowed to swear freedom yeah no one gave a [ __ ] aboutthe whales until someone recorded them
 singing and suddenly people were like oh this is a beautiful creature and now weneed to save the whales and that started
 the whole save the whales movement in the 70s so I'm as much as I and and Ithink a lot of people want to believe
 that we care about consistent biological criteria that's not historically how weformed our alliances yeah so what why do


0:14:54
Speaker 1 :we why do we believe that all humans are
 created equal killing of a human being no matter whothe human being is that's what I meant
 by equality is bad and then because I'm connecting that to robots and I'mwondering whether mortality so the
 killing Act is what makes something that's the fundamental first right soI'm I am currently allowed to take a
 shotgun and shoot a Roomba I think I'm not sure but I'm pretty sure it's notconsidered murder right or even shutting
 them off so that's that's where the line appears to be right is is mortality acritical thing here I think here again


0:15:41
Speaker 0 :like the animal analogy is really useful
 because you're also allowed to shoot your dog but people won't be happy aboutit
 so we give we do give animals certain protections from like you know you'renot allowed to torture your dog and you
 know set it on fire at least in most states and countries you know but you'restill allowed to treat it like a piece
 of property in a lot of other ways and so we draw these you know arbitrarylines all the time and you know there's
 a lot of philosophical thought on why viewing humans is something unique isnot is just speciesism and not you know
 based on any criteria that would actually justify making a differencebetween us and other species do you


0:16:34
Speaker 1 :think in general people
 most people are good do you think do you think there's evil and good in all of usthat's revealed through our
 circumstances and through our

0:16:52
Speaker 0 :interactions I like to view myself as a
 person who like believes that there's no absolute evil and good and thateverything is you know gray but I do
 think it's an interesting question like when I see people being violent towardsrobotic objects you said that bothers
 you because the robots might someday you know be smart and it is that what well

0:17:17
Speaker 1 :it bothers me because it reveals so I
 personally believe because I've studied way to my some Jewish I studied theHolocaust in World War two exceptionally
 well I personally believe that most of us have evil in us that what bothers meis the abuse of robots reveals the evil
 and human beings yeah and it's I think it doesn't but justbother me it's I think it's an
 opportunity for roboticists to make help people be find the better sides theangels of their nature right yeah that
 abuse isn't just a fun side thing that's a you revealing a dark part that youshouldn't there should be hidden deep


0:18:03
Speaker 0 :inside yeah I mean molasse but some of
 our research does indicate that maybe people's behavior towards robots revealssomething about their tendencies for
 empathy generally even using very simple robots that we have today that likeclearly don't feel anything so you know
 West world is maybe you know not so far often it's like you know depicting thebad characters as willing to go around
 and shoot and rape the robots and the good characters is not wanting to dothat even without assuming that the
 robots have consciousness so there's a

0:18:37
Speaker 1 :opportunity at Cynthia's opportunity to
 almost practice empathy the on robots is an opportunity to practice empathy I

0:18:47
Speaker 0 :agree with you some people would say
 why are we practicing empathy on robots instead of you know on our fellow humansor on animals that are actually alive
 and experienced the world and I don't agree with them because I don't thinkempathy is a zero-sum game and I do
 think that it's a muscle that you can train and that we should be doing thatbut some people disagree so the


0:19:09
Speaker 1 :interesting thing you've heard you know
 raising kids sort of asking them or telling them to be nice to the smartspeakers to Alexa and so on saying
 please and so on during the requests I don't know if I'm a huge fan of thatidea because yeah that's towards the
 idea of practicing empathy I feel like politeness I'm always politeto all the all the systems that we build
 especially anything that speech interaction-based like when we talk tothe car I will always have a pretty good
 detector for please - I feel like there should be a room for encouraging empathyin those interactions yeah okay so I


0:19:53
Speaker 0 :agree with you so I'm gonna play devil's
 advocate so what is then what is the

0:19:58
Speaker 1 :

0:20:00
Speaker 0 :dose our argument there the devil's
 advocate argument is that if you are the type of person who has abusivetendencies or needs to get some sort of
 like behavior like that out needs an outlet for it that it's great to have arobot that you can scream at so that
 you're not screaming at a person and we just don't know whether that's truewhether it's an outlet for people or
 whether it just kind of as my friend once said trains their cruelty musclesand makes them more cruel in other


0:20:27
Speaker 1 :situations oh boy yeah in that expanse
 to other topics which they I don't know that you know there's a is a topic ofsex which is weird one that I tend to
 avoid is from robotics perspective and mostly general public doesn't they talkabout sex robots and so on is that an
 area you've touched at all research-wise like the way because that's what peopleimagine sort of any kind of interaction
 between human and robot that shows any kind of compassion they immediatelythink from
 product perspective in the near term is sort of expansion of what pornography isand all that kind of stuff yeah that's


0:21:13
Speaker 0 :kind of you to like characterize it as
 though there's thinking rationally about product I feel like sex robots are justsuch a like titillating news hook for
 people that they become like the story and it's really hard to not get fatiguedby it when you're in the space because
 you tell someone you do human robot interaction of course the first thingthey want to talk about is sex robots
 really yeah it happens a lot and it's it's unfortunate that I'm so fatigued byit because I do think that there are
 some interesting questions that become salient when you talk about you know sex

0:21:48
Speaker 1 :with robots see what I think would
 happen when people get sex robots like if you let some guys okay guys getfemale sex robots what I think there's
 an opportunity for is an actual like like they'll actually interact what I'mtrying to say they won't outside of the
 sex would be the most fulfilling part like the interaction it's like the folkswho this movies on this right who pray
 pay a prostitute and then end up just talking to her the whole time so I feellike there's an opportunity it's like
 most guys and people in general joke about the sex act but really people arejust lonely inside and I'm looking for
 connection many of them and it'd be unfortunate if that it's that connectionis established through the sex industry
 I feel like it should go too into the front door of like people arelonely and they want a connection well I


0:22:47
Speaker 0 :also feel like we should kind of deep
 you know D stigmatize the sex industry because you know even prostitution likethey're prostitutes that specialize in
 disabled people who don't have the same kind of opportunities to explore theirsexuality so it's I I feel like we
 should like D stigmatize all of that generally yeah but yeah that connectionand that loneliness is an interesting
 you know topic that you bring up because while people are Constwe worried about robots replacing humans
 and oh if people get sex robots and the sex is really good then they won't wanttheir you know partner or whatever but
 we rarely talk about robots actually filling a hole where there's nothingyeah and what benefit that can provide


0:23:34
Speaker 1 :to people yeah I think that's an
 exciting there's a whole giant there's a giant hole that's not unfillable byhumans it's asking too much of your of
 people you your friends and people you're in a relationship with in yourfamily to fill that hole there's a
 because you know it's exploring the full like people you know exploring the fullcomplexity and richness of who you are
 like who are you really like the people your family doesn't have enough patienceto really sit there and listen to who
 are you really and I feel like there's an opportunity to really make thatconnection with robots


0:24:10
Speaker 0 :I just really were complex as humans and
 we're capable of lots of different types of relationships so whether that's youknow with family members with friends
 with our pets or with robots I feel like there's space for all of that and all ofthat can provide value in a different


0:24:27
Speaker 1 :way yeah absolutely so I'm jumping
 around currently most of my works and autonomous vehicles so the most populartopic amongst is the trolley problem so
 most most most robots just uh kind of hate this question but what do you thinkof this thought experiment what do you
 think we can learn from it outside of the silliness of the actual applicationof it to the autonomous vehicle I think
 it's still an interesting ethical question and that's in itself just likemuch of the interaction with robots has
 something to teach us but from your perspective do you thinkthere's anything there well I think


0:25:11
Speaker 0 :you're right that it does have something
 to teach us because but but I think what people are forgetting in all theseconversations is the origins of the
 trolley problem and what it was meant to show us which is that there is no rightanswer and that sometimes our moral
 intuition that comes to us instinctively is not actually whatwe should follow if we care about
 creating systematic rules that apply to everyone so I think that as aphilosophical concept it could teach us
 at least that but that's not how people are using it right now like we have andthese are friends of mine and like I
 love them dearly and their project adds a lot of value but if we're viewing themoral machine project as what we can
 learn from the trolley problems the moral machine is I'm sure you'refamiliar it's this website that you can
 go to and it gives you different scenarios like oh you're in a car youcan decide to run over you know these
 two people or this child you know what do you choose do you choose the homelessperson do you choose the person who's
 jaywalking and so it pits these like moral choices against each other andthen tries to crowdsource the
 quote-unquote correct answer which is really interesting and I think valuabledata but I don't think that's what we
 should base our rules in autonomous vehicles on because it is exactly whatthe trolley problem is trying to show
 which is your first instinct might not be the correct one if you look at rulesthat then have to apply to everyone and


0:26:45
Speaker 1 :everything so how do we encode these
 ethical choices in interaction with robots so for example Lata knowsvehicles there is a serious ethical
 question of do I protect myself but that's my life I have higher prioritythan the life of another human being
 because that changes certain control decisions that you make so if your lifematters more than other human beings
 then you'd be more likely to swerve out of your current lane so currentlyautomated emergency braking systems that
 just break they don't ever swerve right so swerving into oncoming traffic or orno just in a different Lane can cause
 significant harm to others but it's possible that it causes less harm to youso that's a difficult ethical question
 do you you do you do you have a hope that like the trolley problem is notsupposed to have a right answer
 do you hope that when we have robots at the table we'll be able to discover theright answer for some of these questions


0:27:52
Speaker 0 :well what's happening right now I think
 is this this question that we're facing of you know what ethical rules should webe programming into the machines is
 revealing to us that our ethical rules are much less programmable than we youknow probably thought before and so
 that's a really valuable insight I think that these issues are very complicatedand that in in a lot of these cases it's
 you can't really make that call like not even as a legislator and so what's gonnahappen in reality I think is that you
 know car manufacturers are just gonna try and avoid the problem and avoidliability in any way possible or like
 they're gonna always protect the driver because who's gonna buy a car if it'syou know programmed to kill someone kill
 kill you instead of someone else so that's what's gonna happen in realitybut what did you mean by like once we
 have robots at the table like do you mean when they can help us figure out

0:28:54
Speaker 1 :what to do no I mean when robots are
 part of the ethical decisions so no no not they help us well oh you mean when

0:29:05
Speaker 0 :it's like should I run over a robot or a


0:29:09
Speaker 1 :person right that kind of thing so what
 no what no no no so when you it's exactly what you said which is when youhave to encode the ethics into an
 algorithm you start to try to really understand what are the fundamentals ofthe decision making process you make
 just make certain decisions should you like capital punishment should you takea person's life or not to punish them
 for a certain crime sort of you can use you can develop an algorithm to makethat decision right and the hope is that
 the act of making that algorithm however you make it so there's a few approacheswill help us actually get to the core of
 what what is right and what is wrong under our current societal standards

0:30:00
Speaker 0 :isn't that what's happening right now
 and we're realizing that we don't have a consensus on what's right and wrong I

0:30:06
Speaker 1 :mean in politics in general well like


0:30:08
Speaker 0 :when we're thinking about these trolley
 problems and autonomous vehicles and how to program ethics into machines and howto you know make make AI algorithms fair
 and equitable where we're realizing that this is so complicated and it'scomplicated in part because there is
 doesn't seem to be a one right answer in

0:30:30
Speaker 1 :any of these cases do you hope for like
 one of the ideas of the moral machine is that crowdsourcing can help us convergetowards like democracy can help us
 converge towards the right answer do you have a hope for crowdsourcing well yes

0:30:44
Speaker 0 :and no so I think that in general you
 know I have a legal background and policymaking is often about trying tosuss out you know what rules does this
 society this particular Society agree on and then trying to codify that so thelaw makes these choices all the time and
 then tries to adapt according to changing culture but in the case of themoral machine project I don't think that
 people's choices on that website necessarily necessarily reflect whatlaws they would want in place if given I
 think you would have to ask them a series of different questions in orderto get up what their consensus is I


0:31:20
Speaker 1 :agree but that that has to do more with
 the artificial nature of I mean they're showing some cute icons on a screenthat's that's almost so if you for
 example we do a lot of work in virtual realityand so if you make if you put those same
 people into virtual reality where they have to make that decision they shouldbe very different I think I agree with


0:31:42
Speaker 0 :that that's one aspect and the other
 aspect is it's a different question to ask someone would you run over thehomeless person or the doctor in this
 scene or do you want cars to always run

0:31:57
Speaker 1 :over the homeless people yeah so let's
 talk about anthropomorphism do me at the prom or fizzell if I can pronounce itcorrectly is is one of the most
 fascinating phenomena from like both the engineering perspectiveand psychology perspective machine
 learning perspective in robotics in general can you step back and define atthe prom or fizzle how you see it in
 general terms in your in your work

0:32:25
Speaker 0 :sure so anthropomorphism is this
 tendency that we have to project human-like traits and behaviors andqualities onto nonhumans and we often
 see it with animals like well will project emotions on animals that may ormay not actually be there okay we often
 see that we're trying to interpret things according to our own behaviorwhen we get it wrong but we do it with
 more than just animals we do it with objects you know teddy bears we see youknow faces in the headlights of cars and
 we do it with robots very very extremely

0:32:59
Speaker 1 :you think that can be engineered can
 that be used to enrich an interaction Oh in and they a system in the human ohyeah for sure and do you and do you see
 it being used that way often like I don't I haven't seen whether it's Alexaor any of the smart speaker systems
 often trying to optimize for the ethical or physician you said you haven't seen I

0:33:26
Speaker 0 :

0:33:27
Speaker 1 :haven't seen they they keep moving away
 from that I think they're afraid of that

0:33:32
Speaker 0 :they they actually so I only recently
 found out but did you know that Amazon has like a whole team of people who arejust there to work on Alexis personality


0:33:43
Speaker 1 :so I know that depends on UI personality
 I didn't know I didn't know that exact thing but I do know that the how thevoice is perceived has worked on a lot
 whether that if it's a pleasant feeling about the voice but that has to do morewith the texture of the sound and the
 audience on what personality is more

0:34:07
Speaker 0 :like it's like what's her favorite beer
 when you ask her and and the personality team is different for every country tolike there's a different personality for
 a German Alexa than there is for American Alexa that's it I think it'svery difficult to
 you know use the are really really harness the anthropomorphism with thesevoice assistance because the voice
 interface is still very primitive and I think that in order to get people toreally suspend their disbelief and treat
 a robot like it's alive less is sometimes more you you want themto project onto the robot and you want
 the robot to not disappoint their expectations for how it's going toanswer or behave in order for them to
 have this kind of illusion and with Alexa I don't think we're there yet orSiri that just they're just not good at
 that but if you look at some of the more animal-like robots like the baby sealthat they use with the dementia patients
 so much more simple design doesn't try to talk to you you can't disappoint youin that way it just makes little
 movements and sounds and people stroke it and it responds to their touch andthat is like a very effective way to
 harness people tendency to kind of treat

0:35:27
Speaker 1 :the robot like a living thing yeah so
 you bring up some interesting ideas in your paper chapter I guess at the poemOrphic framing human robot interaction
 that I read the last time we scheduled

0:35:41
Speaker 0 :

0:35:42
Speaker 1 :this a long time what are some good and
 bad cases event them for morphism and in your perspective like one is the goodone is it bad well I just start by


0:35:52
Speaker 0 :saying that you know while design can
 really enhance the end the premiere film it doesn't take a lot to get people totreat a robot like it's alive like
 people will over 85% of rumbas have a name which I'm I don't know the numbersfor your regular type of vacuum cleaner
 but they're not that high right so people will feel bad for the room butwhen it gets stuck they'll send it in
 for repair and want to get the same one back and that's that one is not evendesigned to like make you do that so I
 think that some of the cases where it's maybe a little bit concerning thatanthropomorphism is happening is when
 you have something that's supposed to function like a tool and people areusing it in the wrong way
 and one of the concerns is military robots we're so gosh mm like early 2000swhich is a long time ago
 iRobot the room a company made this robot called the pack bot that wasdeployed in Iraq and and Afghanistan
 with the bomb disposal units that were there and the soldiers became veryemotionally attached to the robots and
 that's you know fine until a soldier risks his life to save a robot which youreally don't want but they were treating
 them like pets like they would name them they would give them funerals with gunsalutes they would get really upset and
 traumatized when the robot got broken so you in situations where you want a robotto be a tool in particular when it's
 supposed to like do a dangerous job that you don't want a person doing it it canbe hard when people get emotionally
 attached to it that's maybe something that you wouldwant to discourage another case for
 concern is maybe when companies try to leverage the emotional attachment toexploit people so if it's something
 that's not in the consumers interest trying to like sell them products orservices or exploit an emotional
 connection to keep them you know paying for a cloud service for a social robotor something like that might be I I
 think that's a little bit concerning as

0:38:00
Speaker 1 :well yeah the emotional manipulation
 which probably happens behind the scenes now with some like social networks andso on but making it more explicit what's
 your favorite robot like you know a real no real real robot which you have felt aconnection with or not like not not at
 the core morphic connection but I mean like you just sit back as a damn this isan impressive system Wow


0:38:32
Speaker 0 :so two different robots so the the plio
 baby dinosaur robot that is no longer sold that came out in 2007 that one Iwas very impressed with it was but but
 from an anthropomorphic perspective I was impressed with how much I bondedwith it how much I like wanted to
 believe that it had this inner life

0:38:52
Speaker 1 :can you describe Cleo the can you
 describe what what it is how big is it what can actually do ya plio is about

0:38:58
Speaker 0 :the size of a small cat it had a lot of
 like motors that gave it this kind of lifelike movement it had things liketouch sensors and an infrared camera so
 it had all these like cool little technical features even though it was atoy and the thing that really struck me
 about it was that it could mimic pain and distress really well so if you heldit up by the tail it had a tilt sensor
 that you know told it what direction it was facing and it would start to squirmand cry out if you hit it too hard it
 would start to cry so it was very impressive in design and what's the

0:39:38
Speaker 1 :second robot that you were you said
 there might have been two that you liked

0:39:43
Speaker 0 :yeah so the Boston Dynamics robots are
 just impressive feats of engineering

0:39:49
Speaker 1 :have you met them in person


0:39:51
Speaker 0 :yeah I recently got a chance to go visit
 and I you know I was always one of those people who watched the videos and waslike this is super cool but also it's a
 product video like I don't know how many times that they had to shoot this to getit right but visiting them I you know
 I'm pretty sure that I was very impressed let's put it that way yeah in

0:40:09
Speaker 1 :terms of the control I think that was a
 transformational moment for me when I met spot many in person because okaymaybe this is a psychology experiment
 but I anthropomorphised the crap out of it so I immediately itwas like my best friend right I mean


0:40:31
Speaker 0 :it's really hard for anyone to watch
 spot move and not feel like it has

0:40:36
Speaker 1 :agency yeah did this movement especially
 the arm on spot mini really obvi obviously looks like a head yeah thatand they say no wouldn't mean it that
 way but it obviously it looks exactly like that and so it's almost impossibleto not think of it as a almost like the
 baby dinosaur but slightly larger and in this movement of the of coursethe intelligence is that their whole
 idea is that it's not supposed to be intelligent it's a platform on which youbuild higher intelligence it's actually
 really really dumb it's just a basic movement platform yeah but even dumb

0:41:13
Speaker 0 :robots can like we can immediately
 respond to them in this visceral way

0:41:19
Speaker 1 :what are your thoughts about Sofia the
 robot this kind of mix of some basic natural language processing andbasically an art experiment yeah an art


0:41:31
Speaker 0 :experiment is a good way to characterize
 it I'm much less impressed with Sofia than I am with Boston Dynamics she said

0:41:38
Speaker 1 :she likes you she says she admires you


0:41:40
Speaker 0 :she yeah she followed me on Twitter at


0:41:43
Speaker 1 :some point yeah as she tweets about how


0:41:46
Speaker 0 :much she likes you so so wouldn't that
 mean I have to be nicer not I was

0:41:49
Speaker 1 :emotionally manipulating it no how do
 you think of the whole thing that happened with Sofia is quite a largenumber of people kind of immediately had
 a connection and thought that maybe we're far far more advanced withrobotics than we are all right she
 didn't even think much I'm surprised how little people cared that they kind ofassumed that
 well of course AI can do this yeah and then they if they assume that I feltthey should be more impressed well you


0:42:26
Speaker 0 :know what I mean like really
 overestimate where we are and so in something I don't even I don't eventhink Sofia was very impressed over it
 is very impressive I think she's kind of a puppet to be honest but yeah I thinkpeople have are a little bit influenced
 by science fiction pop culture to think that we should be further along than we

0:42:46
Speaker 1 :are so what's your favorite robots and
 movies in fiction wall-e wall-e what do you like about wall-e the humor thecuteness the the perception control
 systems operating and wallahi that makes it all just in general the design of

0:43:04
Speaker 0 :wall-e the robot I think that animators
 figured out you know starting in like Ben1940's how to create characters that
 don't look real but look like something that's even better than real that wereally respond to and think is really
 cute they figured out how to make them move and look in the right way andwall-e is just such a great example of
 that

0:43:29
Speaker 1 :you think eyes big eyes or big something
 that's kind of AI ish so it's always playing on some aspect of the human face

0:43:38
Speaker 0 :right often yeah
 so big eyes well I think one of the one of the first like animations to reallyplay with this was Bambi and they
 weren't originally gonna do that they were originally trying to make the deerlook as lifelike as possible like they
 brought deer into the studio and had a little zoo there so the animators couldwork with them and then at some point
 they were like hmm if we make really big eyes and like a small nose and like bigcheeks kind of more like a baby face
 then people like it even better than if

0:44:07
Speaker 1 :it looks real do you think the future of
 things I collects are in the home has possibility to take advantage of that tobuild on that to create these systems
 that are better than real that created closed human connection I can pretty

0:44:27
Speaker 0 :much guarantee you without having any
 knowledge that those companies are working on that on that design behindthe scenes like pretty sure I totally


0:44:37
Speaker 1 :disagree with you really so that's what
 I'm interested in I'd like to build such a company I know a lot of those folksand they're afraid of that because you
 don't well how do you make money off of

0:44:49
Speaker 0 :it well but even just like making a lexa
 look a little bit more interesting than just like a cylinder would do so much

0:44:56
Speaker 1 :it's it's an interesting thought but I
 don't think people are from Amazon perspective looking for that kind ofconnection they want you to be addicted
 to the services provided by Alexa not to the device so the the device itself it'sfelt that you can lose a lot because if
 you create a connection and then if there's it creates more opportunity forfrustration for
 for negative stuff then it does for positive stuff is I think the way theythink about it that's interesting


0:45:30
Speaker 0 :like I agree that there is it's very
 difficult to get right and you have to get it exactly right otherwise you windup with Microsoft's Clippy okay easy now


0:45:40
Speaker 1 :what's your problem with Clippy oh you


0:45:44
Speaker 0 :like clip these clothes your friends


0:45:46
Speaker 1 :yeah I'll just I just I just talked to
 the would just had this argument and they Microsoft CTO and they and he saidhe said he's not bringing Clippy back
 they're not bringing Clippy back and that's very disappointing is I think itwas clip II was the greatest assistance
 we've ever built it was a horrible attempt of course but it's the bestwe've ever done because it was in real
 attempt you haven't like a actual personality and I mean it was obviouslytechnology was way not there at the time
 of being able to be a recommender system for assisting you in anything and typingin Word or any kind of other application
 but still was an attempt of personality that was legitimate I'm sure I thought

0:46:34
Speaker 0 :was brave yes oh yes okay you know
 you've convinced me I'll be slightly less hard unclick and I know I have like

0:46:40
Speaker 1 :an army of people behind me who also


0:46:44
Speaker 0 :miss Clippy so really I want to meet
 these people who are these people it's

0:46:47
Speaker 1 :the people who like to hate stuff when
 it's there and and miss it when it's gone

0:46:54
Speaker 0 :[Laughter]


0:46:56
Speaker 1 :exactly
 alright so Anki and Gebo the two companies two amazing companies socialrobotics companies that have recently
 been closed down yeah why do you think it's so hard to create a personalrobotics company so making a business
 out of essentially something that people would anthropomorphize have a deepconnection with why is it so hard to
 make it work the business case not there or what is it I think it's a number of

0:47:29
Speaker 0 :different things I don't think it's
 going to be this way forever I think at this current point in time itso much work to build something that
 only barely meets people's like minimal expectations because of science fictionand pop-culture giving people this idea
 that we should be further than we already are like when people think abouta robot assistant in the home they think
 about Rosie from the Jetsons or something like that and on key and andgiba did such a beautiful job with the
 design and getting that interaction just right but I think people just wantedmore they wanted more functionality I
 think you're also right that you know the business case isn't really therebecause there hasn't been a killer
 application that's useful enough to get people to adopt the technology in greatnumbers I think what we did see from the
 people who did you know get geebo is a lot of them became very emotionallyattached to it but that's not I mean
 it's kind of like the Palm Pilot back in the day most people are like why do Ineed this why would I they don't see how
 they would benefit from it until they you know have it or some other companycomes in and makes it a little better


0:48:42
Speaker 1 :yet like how how far away are we do you
 think I mean how hard is this problem

0:48:48
Speaker 0 :it's a good question and I think it has
 a lot to do with people's expectations and those keep shifting depending onwhat science fiction that is popular but


0:48:55
Speaker 1 :also it's two things it's people's
 expectation and people's need for an emotional connection yeah and then Ibelieve the need is pretty high yes but


0:49:07
Speaker 0 :I don't think we're aware of it


0:49:09
Speaker 1 :that's right there's like it I really
 think we're this is like the life as we know it so we've just kind of gottenused to it of really I hate to be dark
 because I have close friends but we've gotten used to really never being closeto anyone all right and we're deeply I
 believe okay this is hypotheses I think we're deeply lonely all of us even thosein deep fulfilling relationships in fact
 what makes us relationship fulfilling I think is that they at least tap intothat deep loneliness a little bit but I
 feel like there's more opportunity to explore that that doesn't interfere withthe human relationship
 you have it expands more on the that yeah the the rich deep unexploredcomplexity that's all of us weird apes
 okay right do you think it's possible to fall in love with a robot oh yeah

0:50:06
Speaker 0 :totally


0:50:08
Speaker 1 :do you think it's possible to have a
 long-term committed monogamous relationshipoh the robot well yeah there are lots of


0:50:15
Speaker 0 :different types of long-term committed
 monogamous relationships I think

0:50:20
Speaker 1 :monogamous implies like you're not going
 to see other humans and sexually or like you basically on Facebook have to sayI'm in a relationship with this person
 this robot I just don't like again I

0:50:33
Speaker 0 :think this is comparing robots to humans
 when I would rather compare them to pets like you get a robot it fulfills youknow this loneliness that you have in us
 maybe not the same way as a pet maybe in a different way that is even you knowsupplemental in a different way but you
 know I'm not saying that people won't like do this be like oh I want to marrymy robot or I want to have like a you
 know sexual relation monogamous relationship with my robot but I don'tthink that that's the main use case for


0:51:09
Speaker 1 :them well you think that there's still a
 gap between human and pet so between husband and pet there's a relationearring so that that's a gap that can be


0:51:27
Speaker 0 :closed but I think it could be closed
 someday but why would we close that like I I think it's so boring to think aboutrecreating things that we already have
 when we could when we could create something that's different I know you'rethinking about the people who like don't
 have a husband and like what could we give them yeah but but let's I guess

0:51:48
Speaker 1 :what I'm getting at is maybe not so like
 the movie her yeah right so a better husband well may be better in some ways

0:52:01
Speaker 0 :like it's I I do think that robots are
 going to continued to be a different type ofrelationship even if we get them like
 very human looking or when you know the voice interactions we have with themfeel very like natural and human like I
 think they're still gonna be differences and there were in that movie too liketowards the end yeah it goes off the


0:52:23
Speaker 1 :rails it's just a movie so that your
 intuition is that that because because you kind of said two things right so oneis why would you want to basically
 replicate the husband Yeah right and the other is kind of implying that it's kindof hard to do so you like anytime you
 try you might build something very impressive but it'll be different Iguess my question is about human nature
 it's like how hard is it to satisfy that role of the husband so removing any ofthe sexual stuff aside is the is more
 like the mystery detention the dance of relationships you think with robotsthat's difficult to build what's you I


0:53:13
Speaker 0 :

0:53:16
Speaker 1 :think that well it also depends I'm not


0:53:16
Speaker 0 :reading about robots now in 50 years in
 like indefinite amount of time where I'm

0:53:24
Speaker 1 :thinking abilities five or ten years


0:53:26
Speaker 0 :five or ten years I think that robots at
 best will be like a more similar to the relationship we have with our pets thanrelationship that we have with other


0:53:36
Speaker 1 :people I got it so what do you think it
 takes to build a system that exhibits greater and greater levels ofintelligence like it impresses us with
 its intelligence you know a Roomba so you talk about ethical moral izationthat doesn't i think intelligence is not
 required if i can tell us probably gets in the way sometimes like you mentionedbut what do you think it takes to create
 a system where we sense that it has a human level intelligence something thatobviously something conversational human
 level intelligence that problem is it'd be interesting to sort of hear yourperspective not
 just purely that talked to a lot of people how hard is the conversationalagents yeah how hard is it to pass a
 Turing test but my sense is it's it's easier thanjust solving it's easier than solving
 the pure and natural language processing problem because I feel like you cancheat yeah so yeah so how hard is it to
 pass the Turing test any of you I well I

0:54:43
Speaker 0 :think again it's all about expectation
 management if you set up people's expectations to think that they'recommunicating with what was it a 13 year
 old boy from the Ukraine yeah that's rightthen they're not going to expect perfect
 English they're not going to expect perfect you know understanding ofconcepts or even like being on the same
 wavelength in terms of like conversation flow so it's much easier to pass in that

0:55:09
Speaker 1 :case do you think you kind of alluded
 this to with audio do you think it needs to have a body I think that we

0:55:17
Speaker 0 :definitely have so we treat physical
 things with more social agency because we're very physical creatures I think abody can be useful does it get in the


0:55:32
Speaker 1 :way is there negative aspects like yeah


0:55:37
Speaker 0 :there can be so if you're trying to
 create a body that's too similar to something that people are familiar withlike I have this robot cat at home that
 Hasbro makes and it's very disturbing to watch because I'm constantly assumingthat it's gonna move like a real cat and
 it doesn't because it's like a 100 dollar piece of technology so it's verylike disappointing and it's very hard to
 treat it like it's alive so you can get a lot wrong with the body too but youcan also use tricks same as you know the
 expectation management of the 13 year old boy from the Ukraine if you pick ananimal that people aren't intimately
 familiar with like the baby dinosaur like the baby seal that people havenever actually held in their arms you
 can get away with much more because they don't have these preformed expectations

0:56:24
Speaker 1 :yeah I'm thinking a TED talk or
 something that clicked for me that nobody actually knows what a dinosaurlooks
 so you can actually get away with a lot more that was greatdo you think he needs so what do you
 think about consciousness and mortality being displayed in a robot so notactually having consciousness but having
 these kind of human elements that are much more than just the interaction muchmore than just like you mentioned with a
 dinosaur moving kind of interesting ways but really being worried about its owndeath and really acting as if it's aware
 and self-aware and identity have you seen that done in robotics what do youthink about doing that I think it does
 is that a is that a powerful good thing

0:57:24
Speaker 0 :well it's a I think it can be a design
 tool that you can use for different purposes so I can't say whether it'sinherently good or bad but I do think it
 can be a powerful tool the fact that the you know Cliomimics distress when you quote-unquote
 would hurt it his is a really powerful tool to get people to engage with it ina certain way I had a research partner
 that I did some of the empathy work with named Kailash Nandi and he had built arobot for himself that had like a life
 span and that would stop working after a certain amount of time just because hewas interested in like whether he
 himself would treat it differently and we know from you know Tamagotchis thoselike those little games that that we
 used to have that we're extremely primitive that like people respond tolike this idea of mortality and you know
 you can get people to do a lot with little design tricks like that nowwhether it's a good thing depends on
 what you're trying to get them to do

0:58:24
Speaker 1 :have a deeper relationship have a deeper
 connection sign a relationship if it's

0:58:29
Speaker 0 :for their own benefit that sounds great
 okay a lot of other reasons I see so

0:58:36
Speaker 1 :what kind of stuff are you worried about
 so is this a mostly about manipulation of your emotions for like advertisementso on things like that


0:58:44
Speaker 0 :yeah or data collect
 I mean you could think of governments misusing this to extract informationfrom people it's you know just just like
 any other technological tool just raises a lot of questions

0:58:58
Speaker 1 :what's if you if you look at Facebook if
 you look at Twitter and social networks there's a lot of concern of datacollection now how what's from the legal
 perspective or in general how do we prevent the violation of sort of thesecompanies crossing a line it's a gray
 area but crossing a line they shouldn't in terms of manipulating like we'retalking about a manipulating our emotion
 manipulating our behavior using tactics that are not so savory yeah it's it's

0:59:31
Speaker 0 :really difficult because we are starting
 to create technology that relies on data collection to provide functionality andthere's not a lot of incentive even on
 the consumer side to curb that because the other problem is that the harmsaren't tangible they're not really
 apparent to a lot of people because they kind of trickle down on a societal leveland then suddenly we're living in like
 1984 which you know sounds extreme but that book was very prescient and I'm notworried about you know these systems you
 know I I I have you know Amazon's echo at home and and you know tell Alexa allsorts of stuff and and it helps me
 because you know Alexa knows what you know brand of diaper we use and so I canjust easily order it again so I don't
 have any incentive to like ask a lawmaker to curb that but when I thinkabout that data then being used against
 you know low income people to target them for you know scammy loans oreducation programs that's then a
 societal effect that I think is very severe and you know legislators shouldbe thinking about well yeah there's the


1:00:47
Speaker 1 :the Garrett gray area is the removing
 ourselves from consideration of like of explicitly defining objectives andmore saying well we want to maximize
 engagement in our social network yeah and and then just because you're notactually doing a bad thing it makes
 sense you want people to to keep aconversation going to have more
 conversations to keep coming back again and again to have conversations andwhatever happens after that you're kind
 of not exactly directly responsible you're only indirectly responsible soit's I think it's a really hard problem
 do I are you optimistic about us ever being able to solve it you mean the

1:01:35
Speaker 0 :problem of capitalists like because the
 problem is that the companies are acting in the company's interests and not inpeople's interest and when those
 interests are aligned that's great but the completely free market doesn't seemto work because of this information


1:01:55
Speaker 1 :asymmetry but it's hard to know how to
 so say you would try to do the right thing I guess I guess what I'm trying tosay is I'm it's not obvious for these
 companies what the good thing for society is to do like I don't think theysit there and with I don't know whether
 it with a glass of wine and a cat like petting a cat evil cat and and there'stwo decisions and one of them is good
 for society one is good for the for the profit and they choose the profit Ithink they actually there's a lot of
 money to be made by doing the the right thing for society like that becauseGoogle Facebook have so much cash that
 day actually was especially Facebook was significantly benefit for makingdecisions that are good for society it's
 good for their brand right so but I don't know if they know what societythat's the we I don't think we know
 what's good for society in terms of how yeah how we manage the conversation onTwitter or how we design will talk about
 robots like should we emotionally manipulate youinto having a deep connection with Alexa
 or not yeah yeah you have optimism that

1:03:13
Speaker 0 :

1:03:13
Speaker 1 :we'll be able to solve some of these
 questions well I'm gonna say something

1:03:18
Speaker 0 :that's controversial like in my circles
 which is that I don't think that companies who are reaching out toethicists and trying to create
 interdisciplinary ethics boards I don't think that that's totally just trying towhitewash the problem and and and so
 that they look like they've done something I think that a lot ofcompanies actually do like you say care
 about what the right answer is they don't know what that is and they'retrying to find people to help them find
 them not in every case but I think I you know it's much too easy to just vilifythe companies as like you said sitting
 there with their cat going her 1 million dollars that's not what happens a lot ofpeople are well-meaning even within
 companies I think that what we do absolutely need is moreinterdisciplinarity both within
 companies but also within the policy-making space because we're youknow we've hurdled into the world where
 technological progress is much faster it seems much faster than it was and thingsare getting very complex and you need
 people who understand the technology but also people who understand what thesocietal implications are and people who
 are thinking about this in a more systematic way to be talking to eachother there's no other solution I think


1:04:40
Speaker 1 :you've also done work on intellectual
 property so if you look at the algorithms these companies are usinglike YouTube Twitter Facebook so on and
 that's kind of the those are mostly secretive in the recommender systemsbehind behind these algorithms do you do
 you think about it IP and transparency about how goes like this like what theresponsibility these companies to
 open-source the algorithms or at least reveal to the public what's how thesealgorithms work so I personally don't


1:05:13
Speaker 0 :work on that there are a lot of people
 who do though and there are a lot of people calling for transparency in factEurope's even trying to legislate
 transparency maybe they even have at this point where like if if analgorithmic system makes some sort of
 decision that affects someone's life that you need to be able to see how thatdecision was made I you know it's it's a
 it's a tricky balance because obviously companies need to have you know somesort of competitive advantage and you
 can't take all that away or you stifle innovation but yeah for some of the waysthat these systems are already being
 used I think it it is pretty important that people understand how they work

1:05:54
Speaker 1 :what are your thoughts in general on
 intellectual property in this weird age of software AI robotics oh that it's

1:06:02
Speaker 0 :broken I mean the system is just broken


1:06:06
Speaker 1 :so did can you describe I actually I
 don't even know what intellectual property is in the space of softwarewhat it means to I mean I so I believe I
 have a patent on a piece of software from my PhD you believe you don't knowno we went through a whole process yeah
 I do

1:06:26
Speaker 0 :the spam emails like will frame your


1:06:30
Speaker 1 :patent for you yes much like a thesis so
 uh but that's useless right or not what word is IP stand in this age what whatis what's the right way to do it what's
 the right way to protect and own ideas and why don't when it's just code and inthis mishmash of something that feels
 much softer than a piece of machinery

1:06:54
Speaker 0 :yeah idea I mean it's hard because you
 know there are different types of intellectual property and they're kindof these blunt instruments they're
 they're like it's like patent law is like a wrench like it works really wellfor an industry like the pharmaceutical
 industry but when you try and apply it to something else it's like I don't knowI'll just like hit this thing with the
 wrench and hope it works so software you know software you have a coupledifferent options software like any code
 that's written down in some tangible form is automatically copyrighted so youhave that protection but that doesn't do
 much because if someone takes the basic idea that thecode is executing and just does it in a
 slightly different way they can get around the copyright so that's not a lotof protection then you can patent
 software but that's kind I mean getting a patent cost I don't know if youremember what yours costs or like was it
 an institution yes during university

1:07:51
Speaker 1 :yeah they it was insane there's so many
 lawyers so many meetings and it made me feel like it must have been hundreds ofthousands of dollars yeah crazy it's


1:08:03
Speaker 0 :it's insane the costs of getting a
 patent and so this idea of like protecting the like inventor in theirown garage like came up with a great
 idea is kind of that's the thing of the past it's all just companies trying toprotect things and it costs a lot of
 money and then with code it's oftentimes like you know by the time the patent isissued which can take like five years
 you probably your code is obsolete at that point so it's it's a very again avery blunt instrument that doesn't work
 well for that industry and so you know at this point we should really havesomething better but we don't do like


1:08:38
Speaker 1 :open source yeah it's open so it's good
 for society you think all of us should open source code well so at the Media

1:08:44
Speaker 0 :Lab at MIT we have an open source
 default because what we've noticed is that people will come in there likewrite some code and they'll be like how
 do I protect this and we're like mmm like that's not your problem right nowyour problem isn't that someone's gonna
 steal your project your problem is getting people to use it at all likethere's so much stuff out there like we
 don't even know if you're gonna get traction for your work and so opensourcing can sometimes help you know get
 people's work out there but ensure that they get attribution for it for the workthat they've done so I like I'm a fan of
 it in a lot of contexts obviously it's not like a one-size-fits-all solution so

1:09:23
Speaker 1 :what I gleaned from your Twitter is your
 mom I saw a quote a reference to baby bot what have you learned about roboticsand AI from raising a human baby bot


1:09:42
Speaker 0 :well I think that my child has made it
 more apparent and that the systems we're currentlycreating aren't like human intelligence
 like it there's not a lot to compare there it's just you he has learned anddeveloped in such a different way than a
 lot of the AI systems were creating that that's not really interesting to me tocompare but what is interesting to me is
 how these systems are going to shape the world that he grows up in and so I'mlike even more concerned about kind of
 the societal effects of developing systems that you know rely on massiveamounts of data collection for example


1:10:23
Speaker 1 :so is you going to be allowed to use
 like Facebook or Facebook is over kids

1:10:28
Speaker 0 :don't use that at snapchat what do they


1:10:32
Speaker 1 :use Instagram Jets over to I don't know


1:10:34
Speaker 0 :I just heard that tick tock is over
 which I've never even seen so I don't know no we're old we don't know twitch

1:10:41
Speaker 1 :and you just I'm gonna start gaming and
 streaming my my gameplay so what do you see is the future of personal roboticsocial robotics interaction with our
 robots like what are you excited about if you were to sort of philosophizeabout what might happen the next 5-10
 years that would be cool to see oh I

1:11:03
Speaker 0 :really hope that we get kind of a home
 robot that makes it that's a social robot and not just Alexa like it's youknow I really loved the Anki products
 I thought Gebo was had some really great aspect so I'm hoping that a company

1:11:21
Speaker 1 :cracks that meet you so okay it was
 wonderful talking today likewise thank

1:11:26
Speaker 0 :

