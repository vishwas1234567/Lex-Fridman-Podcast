0:00:00
Speaker 0 :the following is a conversation with
 Stuart Russell he's a professor of computer science at UC Berkeley and aco-author of a book that introduced me
 and millions of other people to the amazing world of AI called artificialintelligence a modern approach so it was
 an honor for me to have this conversation as part of MIT course andartificial general intelligence and the
 artificial intelligence podcast if you enjoy it please subscribe on youtubeitunes or your podcast provider of
 choice or simply connect with me on twitter at Lex Friedman spelled Fri Dand now here's my conversation with


0:00:40
Speaker 1 :Stuart Russell so you've mentioned in
 1975 in high school you've created one year first AI programs that play chesswere you ever able to build a program
 that beat you a chess or another board

0:00:57
Speaker 0 :game so my program never beat me at
 chess I actually wrote the program at Imperial College so I used to take thebus every Wednesday with a box of cards
 this big and shove them into the card reader and they gave us eight seconds ofCPU time
 it took about five seconds to read the cards in and compile the code so we hadthree seconds of CPU time which was
 enough to make one move you know with a not very deep search and then we wouldprint that move out and then we'd have
 to go to the back of the queue and wait to feed the cards in again how do you

0:01:35
Speaker 1 :post a search well I would talk to no I


0:01:38
Speaker 0 :think we got we got an eight move eight
 you know depth eight with alpha beta and we had some tricks of our own about moveordering and some pruning of the tree


0:01:52
Speaker 1 :and we were still able to beat that


0:01:54
Speaker 0 :program yeah yeah I I was a reasonable
 chess player in my youth I did Anna fellow program and a backgammon programso when I go to Berkley I worked a lot
 on what we call meta reasoning which really means reasoning about reasoningand in the case of a game playing
 program you need to reason about what parts of the search tree you're actuallygoing to explore because the search tree
 is enormous or you know bigger than the number of atoms in the universe and theway programs succeed and the way humans
 succeed is by only looking at a small fraction of the search tree and if youlook at the right fraction you play
 really well if you look at the wrong fraction if you waste your time thinkingabout things that are never gonna happen
 the moves that no one's ever gonna make then you're gonna lose because you youwon't be able to figure out the right
 decision so that question of how machines canmanage their own computation either how
 they decide what to think about is the meta-reasoning question wedeveloped some methods for doing that
 and very simply a machine should think about whatever thoughts are going toimprove its decision quality we were
 able to show that both for a fellow which is a standard to play game and forbackgammon which includes dice for also
 it's a two-player game with uncertainty for both of those cases we could come upwith algorithms that were actually much
 more efficient than the standard alpha beta search which chess programs at thetime we're using and that those programs
 could beat me and I think you can see same basic ideas in alphago and alphazero today the way they explored the
 tree is using a former meta reasoning to select what to think about based on howuseful it is to think about it is there


0:03:56
Speaker 1 :any insights you can describe without
 Greek symbols of how do we select which paths to go down there's really two

0:04:04
Speaker 0 :kinds of learning going on so as you say
 alphago learns to evaluate board position so it can it can look at a goboard and it actually has probably a
 superhuman ability to instantly tell how promising that situation is to me theamazing thing about alphago is not that
 it can be the world champion with its hands tied behind his back but the factthat if you stop it from searching
 altogether so you say okay you're not allowed to do any thinking aheadright you can just consider each of your
 legal moves and then look at the resulting situation and evaluate it sowhat we call a depth one search so just
 the immediate outcome of your moves and decide if that's good or badthat version of alphago can still play
 at a professional level right and human professionals are sitting there for fiveten minutes deciding what to do and
 alphago in less than a second instantly into it what is the right moveto make based on its ability to evaluate
 positions and that is remarkable because you know we don't have that level ofintuition about go we actually have to
 think about the situation so anyway that capability that alphago has is one bigpart of why it beats humans the other
 big part is that it's able to look ahead 40 50 60 moves into the future mm-hmmand you know if it was considering all
 possibilities 40 or 50 or 60 moves into the future that would be you know 10 tothe 200
 possibility so wait way more than you know atoms in the universe and and so onso it's very very selective about what
 it looks at so let me try to give you an intuitionabout how you decide what to think about
 it's a combination of two things one is how promising it is right so if you'realready convinced that a move is
 terrible there's no point spending a lot more time convincing yourself that it'sterrible because it's probably not gonna
 change your mind so the the real reason you think is because there's somepossibility of changing your mind about
 what to do mm-hmm right and is that changing your mindthat would result then in a better final
 action in the real world so that's the purpose of thinking is to improve thefinal action in the real world and so if
 you think about a move that is guaranteed to be terrible you canconvince yourself is terrible and you're
 still not gonna change your mind all rightbut on the other hand you I suppose you
 had a choice between two moves one of them you've already figured out isguaranteed to be a draw let's say and
 then the other one looks a little bit worse like it looks fairly likely thatif you make that move you're gonna lose
 but there's still some uncertainty about the value of that move there's stillsome possibility that it will turn out
 to be a win all right then it's worth thinking about that so even though it'sless promising on average than the other
 move which is guaranteed to be a draw there's still some purpose in thinkingabout it because there's a chance that
 you will change your mind and discover that in fact it's a better move so it'sa combination of how good the move
 appears to be and how much I'm certainty there is about its value the moreuncertainty the more it's worth thinking
 about because there's a higher upside if

0:07:51
Speaker 1 :you want to think of it that way and of
 course in the beginning especially in the alphago 0 formulation it'severything is shrouded in uncertainty so
 you're really swimming in a sea of uncertainty so it benefits you too Imean actually following the same process
 as you described but because you're so uncertain about everything you youbasically have to try a lot of different


0:08:15
Speaker 0 :directions yeah so so the early parts of
 the search tree a fairly bushy that it will when looking a lot ofdifferent possibilities but fairly
 quickly the degree of certainty about some of the moves I mean if a movies arereally terrible you'll pretty quickly
 find out right you lose half your pieces or half your territory and and thenyou'll say okay this this is not worth
 thinking about any more and then so a further down the tree becomes very longand narrow and you're following various
 lines of play you know 10 20 30 40 50 moves into the future and you knowthat's again it's something that human
 beings have a very hard time doing mainly because they just lacked theshort-term memory you just can't
 remember a sequence of moves that's 50 movies long and you can't you can'timagine the board correctly for that
 money moves into the future of course

0:09:13
Speaker 1 :the top players I'm much more familiar
 with chess but the top players probably have they have echoes of the same kindof intuition instinct that in a moment's
 time alphago applies when they see a boardI mean they've seen those patterns human
 beings have seen those patterns before at the top at the Grandmaster level itseems that there is some similarities or
 maybe it's it's our imagination creates a vision of those similarities but itfeels like this kind of pattern
 recognition that the alphago approaches are using is similar to what humanbeings at the top level or using I think


0:09:56
Speaker 0 :there's there's some truth to that but


0:10:00
Speaker 1 :

0:10:02
Speaker 0 :not entirely yeah I mean I think the the
 extent to which a human Grandmaster can reliably wreak instantly recognize theright move instantly recognize the value
 of a position I think that's a little

0:10:16
Speaker 1 :bit overrated but if you sacrifice a
 queen for exam I mean there's these there's these beautiful games of chesswith Bobby Fischer somebody where it's
 seeming to make a bad move and I'm not sure there's aa perfect degree of calculation involved
 were they've calculated all the possible things that happen but there's aninstinct there right that somehow adds


0:10:40
Speaker 0 :up to the yeah so I think what happens
 is you you you get a sense that there's some possibility in the position even ifyou make a weird-looking move that it
 opens up some some lines of of calculation that otherwise would bedefinitely bad and and is that intuition
 that there's something here in this position that might might yield a windown the side and then you follow that
 right and and in some sense when when a chess player is following a line and inhis or her mind they're they mentally
 simulating what the other person is gonna do while the opponent is gonna doand they can do that as long as the
 moves are kind of forced right as long as there's a you know there's a fourthwe call a forcing variation where the
 opponent doesn't really have much choice how to respond and then you see if youcan force them into a situation where
 you win you know we see plenty of mistakes even even in Grandmaster gameswhere they just miss some simple three
 four five move combination that you know wasn't particularly apparent in in theposition but we're still there that's


0:12:00
Speaker 1 :the thing that makes us human
 yeah so when you mentioned that in a fellow those games were after some metareasoning improvements and research I
 was able to beat you how did that make

0:12:14
Speaker 0 :you feel part of the meta reasoning
 capability that it had was based on learning and and you could sit down thenext day and you could just feel that it
 had got a lot smarter boom you know and all the sudden you really felt like yousort of pressed against
 the wall because it was it was much more aggressive and was totally unforgivingof any minor mistake that you might make
 and and actually it seemed understood the game better than I did and you knowGary Kasparov has this quote weary
 during his match against deep blue he said he suddenly felt that there was anew kind of intelligence across the


0:12:59
Speaker 1 :board do you think that's a scary or an
 exciting possibility that's prevent for yourself in in the context of chesspurely sort of in this like that feeling


0:13:13
Speaker 0 :whatever that is I think it's definitely
 an exciting feeling you know this is what made me work on AI in the firstplace was as soon as I really understood
 what a computer was I wanted to make it smart you know I started out with thefirst program I wrote was for the
 sinclair programmable calculator and i think you could write a 21 stepalgorithm that was the biggest program
 you could write something like that and do little arithmetic calculations so Isay think I implemented Newton's method
 for square roots and a few other things like thatum but then you know I thought okay if I
 just had more space I could make this thing intelligentand so I started thinking about AI and
 and I think the the the thing that's scary is not is not the chess programbecause you know chess programs they're
 not in they're taking over the world business but if you extrapolateyou know there are things about chess
 that don't resemble the real world right we know we know the rules of chesschess board is completely visible to the
 programmer of course the real world is not most you most the real world is notvisible from wherever you're sitting so
 to speak and to overcome those kinds of problemsyou need qualitatively different
 algorithms another thing about the real world is that you know we we regularlyplan ahead on the timescales involving
 billions or trillions of steps now we don't plan that was in detail but youknow when you choose to do a PhD at
 Berkeley that's a five-year commitment and thatamounts to about a trillion motor
 control steps that you will eventually be committed to including going up the

0:15:24
Speaker 1 :stairs opening doors drinking water type


0:15:28
Speaker 0 :yeah I mean every every finger movement
 while you're typing every character of every paper and the thesis andeverything else so you're not commuting
 in advance to the specific motor control steps but you're still reasoning on atimescale that will eventually reduce to
 trillions of motor control actions and so for all these reasonsyou know alphago and and deep blue and
 so on don't represent any kind of threat to humanity but they are a step towardsit right near that and progress in AI
 occurs by essentially removing one by one these assumptions that make problemseasy like the assumption of complete
 observability of the situation right we remove that assumption you need a muchmore complicated kind of a computing
 design and you need something that actually keeps track of all the thingsyou can't see and tries to estimate
 what's going on and there's inevitable uncertainty in that so it becomes a muchmore complicated problem but you know we
 are removing those assumptions we are starting to have algorithms that cancope with much longer timescales
 they can cope with uncertainty they can cope with partial observabilityand so each of those steps sort of
 magnifies by a thousand the range of things that we can do with AI systems so

0:16:55
Speaker 1 :the way I started me I wanted to be a
 psychiatrist for long time to understand the mind in high school and of courseprogram and so on and then I showed up
 University of Illinois to an AI lab and they said okay I don't have time for youbut here's a book AI a modern approach I
 think was the first edition at the time mmm here go go learn this and I rememberthe lay of the land was well it's
 incredible that we solve chess but we'll never solve go I mean it was prettycertain that go in the way we thought
 about systems that reason was impossible to solve and now we've solved this as a

0:17:34
Speaker 0 :very I think I would have said that it's
 unlikely we could take the kind of algorithm that was used for chess andjust get it to scale up and work well
 for go and at the time what we thought was thatin order to solve go we would have to do
 something similar to the way humans manage the complexity of go which is tobreak it down into kind of sub games so
 when a human thinks about a go board they think about different parts of theboard as sort of weakly connected to
 each other and they think about okay within this part of the board here's howthings could go and that part about his
 how things could go and now you try to sort of couple those two analysestogether and deal with the interactions
 and maybe revise your views of how things are going to go in each part andthen you've got maybe five six seven ten
 parts of the board and that actually resembles the real world much more thanchess does because in the real world you
 know we have work we have home life we have sport you know whatever differentkinds of activities you know shopping
 these all are connected to each other but they're weakly connected so when I'mtyping a paper you know I don't simul
 taneous Li have to decide which order I'm gonna get the you know the milk andthe butter you know that doesn't affect
 the typing but I do need to realize okay better finish this before the shopsclosed because I don't have anything you
 don't have any food at home all right right so there's some weak connectionbut not in the way that chess works
 where everything is tied into a single stream of thought so the thought wasthat go just sort of go we'd have to
 make progress on stuff that would be useful for the real world and in a wayalphago is a little bit disappointing
 right because the the program designed for alphago was actually not thatdifferent from from deep blue or even
 from Arthur Samuels checker playing program from the 1950sand in fact the so the two things that
 make alphago work is one one is is amazing ability ability to evaluate thepositions and the other is the
 meta-reasoning capability which which allows it to to explore some paths inthe tree very deeply and to abandon
 other paths very quickly so this word

0:20:06
Speaker 1 :meta-reasoning while technically correct
 inspires perhaps the the wrong degree of power that alphago has for example theword reasonings as a powerful word let
 me ask you sort of so you were part of the symbolic AI world for a while likewhatever the AI was there's a lot of
 excellent interesting ideas there that unfortunately met a winter and so it do

0:20:39
Speaker 0 :you think it really emerges well I would
 say yeah it's not quite as simple as that so the the AI winter so for thefirst window that was actually named as
 such was the one in the late 80s and that came about because in the mid80s there was a really a concerted
 attempt to push AI out into the real world using what was called expertsystem technology and for the most part
 that technology was just not ready for primetimethey were trying in many cases to do a
 form of uncertain reasoning judge you know judgment combinations of evidencediagnosis those kinds of things which
 was simply invalid and when you try to apply invalid reasoning methods to realproblems you can fudge it for small
 versions of the problem but when it starts to get larger the thing justfalls apart so many companies found that
 the stuff just didn't work and they were spending tons of money on consultants totry to make it work and
 there were you know other practical reasons like you know they they wereasking the companies to buy incredibly
 expensive lisp machine workstations which were literally between fifty and ahundred thousand dollars in you know in
 1980s money which was would be like between a hundred and fifty and threehundred thousand dollars per workstation
 in current prices so then the bottom

0:22:20
Speaker 1 :line they weren't seeing a profit from


0:22:23
Speaker 0 :it yeah
 they in many cases I think there were some successes there's no doubt aboutthat but people I would say over
 invested every major company was starting an AI department just like nowand I worry a bit that we might see
 similar disappointments not because the technology is invalid but it's limitedin its scope and it's almost the the
 dual of the you know the scope problems that expert systems had so what have you

0:23:01
Speaker 1 :learned from that hype cycle and what
 can we do to prevent another winter for

0:23:07
Speaker 0 :example yeah so when I'm giving talks
 these days that's one of the warnings that I give to to pot warning slide oneis that you know rather than data being
 the new oil data is the new snake oil that's a good line and then and then theother is that we might see a kind of
 very visible failure in some of the major application areas and I thinkself-driving cars would be the flagship
 and I think when you look at the history so the first self-driving car was on thefreeway driving itself changing lanes
 overtaking in 1987 and so it's more than 30 years and that kind of looks likewhere we are today right you know
 prototypes on the freeway changing lanes and overtaking now I think significantprogress has been made particularly on
 the perception side so we worked a lot on autonomous vehicles in the early mid90s at Berkley you know and we had our
 own big demonstrations you know we we put congressmen into yourself drivingcars and and had them zooming along the
 freeway and the problem was clearly perception

0:24:37
Speaker 1 :at the time the problem that perception


0:24:38
Speaker 0 :yeah so in simulation with perfect
 perception you could actually show that you can drive safely for a long timeeven if the other cars are misbehaving
 and and so on but simultaneously we worked on machine vision for detectingcars and tracking pedestrians and so on
 and we couldn't get the reliability of detection and tracking up to a highenough particular level particularly in
 bad weather conditions nighttime

0:25:11
Speaker 1 :rainfall good enough for demos but
 perhaps not good enough to cover the general the general yeah the thing about

0:25:16
Speaker 0 :driving is you know suppose you're a
 taxi driver you know and you drive every day eight hours a day for ten yearsright that's a hundred million seconds
 of driving you know and any one of those seconds you can make a fatal mistake soyou're talking about eight nines of
 reliability right now if your vision system only detects ninety eight pointthree percent of the vehicles right and
 that's sort of you know one on a bit nines and reliability so you haveanother seven orders of magnitude to go
 and and this is what people don't understand they think oh because I had asuccessful demo I'm pretty much done but
 you know you're not even within seven orders of magnitude of being done andthat's the difficulty and it's it's not
 there can I follow a white line that's not the problem right we follow a whiteline all the way across the country
 but it's the it's the weird stuff that happens it's some of the edge cases yeah

0:26:21
Speaker 1 :

0:26:21
Speaker 0 :the edge case other drivers doing weird
 things you know so if you talk to Google right so they had actually veryclassical architecture where you know
 you had machine vision which would detect all the other cars andpedestrians and the white lines and the
 road signs and then basically that was fed into a logical database and then youhad a classical 1970s rule-based expert
 system telling you okay if you're in the middle lane and there's a bicyclist inthe right lane who is signaling this
 then then then don't need to do that yeah right and what they found was thatevery day they go out and there'd be
 another situation that the rules didn't cover you know so they they come to atraffic circle and there's a little girl
 riding a bicycle the wrong way around a traffic circle okay what do you do wedon't have a rule oh my god okay stop
 and then you know they come back and had more rules and they just found that thiswas not really converging
 and and if you think about it right how how do you deal with an unexpectedsituation meaning one that you've never
 previously encountered and the sort of the the reasoning required to figure outthe solution for that situation has
 never been done it doesn't match any previous situation in terms of the kindof reasoning you have to do well you
 know in chess programs this happens all the timeyou're constantly coming up with
 situations you haven't seen before and you have to reason about them you haveto think about okay here are the
 possible things I could do here the outcomes here's how desirable theoutcomes are and then pick the right one
 you know in the 90s we were saying okay this is how you're gonna have to doautomated vehicles they're gonna have to
 have a look ahead capability but the look ahead for driving is more difficultthan it is for chess because Huysmans
 the other right there's humans and they're less predictable than just astandard well then will you have an
 opponent in chess who's also somewhat unpredictable but for example in chessyou always know the opponent's intention
 they're trying to beat you right whereas in driving you don't know is this guytrying to turn left or has he just
 forgotten to turn off his tone signal or is he drunk or is he you know changingthe channel on his radio or whatever it
 might be you got to try and figure out the mental state the intent of the otherdrivers to forecast the possible
 evolutions of their trajectories and then you've got to figure out okay whichis the directory for me that's going to
 be safest and those all interact with each other because the other driversgoing to react to your trajectory and so
 on so you know they've got the classic merging onto the freeway a problem whereyou're kind of racing a vehicle that's
 already on the freeway and you are you gonna pull ahead of them or you're gonnalet them go first and pull in behind and
 you get this sort of uncertainty about who's going firstso all those kinds of things
 mean that you need decision-making architecture that's very different fromeither a rule-based system or it seems
 to me a kind of an end-to-end neural network system you know so just asalphago is pretty good when it doesn't
 do any look ahead but it's way way way way better when it does I think the sameis going to be true for driving you can
 have a driving system that's pretty good when it doesn't do any look ahead butthat's not good enough you know and
 we've already seen multiple deaths caused by poorly designed machinelearning algorithms that don't really
 understand what they're doing yeah and

0:30:09
Speaker 1 :on several levels I think it's on the
 perception side there's mistakes being made by those algorithms were theperception is very shallow on the
 planning side to look ahead like you said and the thing that we come come upagainst that's really interesting when
 you try to deploy systems in the real world is you can't think of anartificial intelligence system as a
 thing that responds to the world always you have to realize that it's an agentthat others will respond to as well so
 in order to drive successfully you can't just try to do obstacle avoidance you

0:30:47
Speaker 0 :can't pretend that you're invisible
 thank you right you're the invisible car right just look that way I mean but you

0:30:53
Speaker 1 :have to assert yet others have to be
 scared of you just we're all there's this tension there's this game so if westudied a lot of work with pedestrians
 if you approach pedestrians as purely an obstacle avoidance so you either doinglook ahead isn't modeling the intent
 that you're you they're not going to they're going to take advantage of youthey're not going to respect you at all
 there has to be a tension a fear some amount of uncertainty that's how we have

0:31:23
Speaker 0 :create we or at least just a kind of a
 resoluteness right so you have you have to display a certain amount ofresoluteness you can't you can't be too
 tentative and yeah so the right the the solutionsthen become pretty complicated right you
 get into game theoretic yes analyses and so we're you know Berkeley now we'reworking a lot on this kind of
 interaction between machines and humans

0:31:52
Speaker 1 :and that's exciting yeah and so my


0:31:53
Speaker 0 :colleague and could drag an actually you
 know if you if you formulate the problem game theoretically and you just let thesystem figure out the solution you know
 it does interesting unexpected things like sometimes at a stop signif no one is going first right the car
 will actually back up a little all right and just to indicate to the other carsthat they should go and that's something
 it invented entirely by itself that's interesting you know we didn't say thisis the language of communication at stop
 signs it figured it out that's really

0:32:31
Speaker 1 :interesting
 so let me one just step back for a second just this beautiful philosophicalnotion so Pamela I'm a quartic in 1979
 wrote AI began with the ancient wish to forge the gods so when you think aboutthe history of our civilization do you
 think that there is an inherent desire to create let's not say gods but tocreate super intelligence is it inherent
 to us is it in our genes that the natural arc of human civilization is tocreate things that are of greater and
 greater power and perhaps no echoes of ourselves so to create the gods as

0:33:22
Speaker 0 :Pamela said
 if the maybe I mean you know we're all we're all individualscertainly we see over and over again in
 history individuals who thought about this possibility hopefully when I'm not

0:33:39
Speaker 1 :being too philosophical here but if you
 look at the arc of this you know where this is going and we'll talk about AIsafety we'll talk about greater and
 greater intelligence do you see that there in when you created the earthAllah program and you felt this
 excitement what was that excitement was itexcitement of a tinkerer who created
 something cool like a clock or was there a magic or was it more like a childbeing born that yeah you know yeah so I


0:34:10
Speaker 0 :mean I certainly understand that
 viewpoint and if you look at the light he'll report which was commit so in the70s there was a lot of controversy in
 the UK about AI and you know whether it was for real and how much the moneymoney the government should invest and
 there was a lot long story but the government commissioned a report byby light Hill who was a physicist and he
 wrote a very damning report about AI which I think was the point and he saidthat that these are you know frustrated
 men who unable to have children would like to create and you know create lifeyou know as a kind of replacement you
 know which I which I think is really pretty unfairbut there is I mean there there is a
 kind of magic I would say you when you you build somethingand what you're building in is really
 just you're building in some understanding of the principles oflearning and decision-making and to see
 those principles actually then turn into intelligent behavior in in specificsituations it's an incredible thing and
 you know that is naturally going to make you think okay where does this end and

0:35:59
Speaker 1 :so there's a there's magical optimistic
 views of word and whatever your view of optimism is whatever your view of utopiais it's probably different for everybody
 yeah but you've often talked about concerns you have of how things might gowrong so I've talked to max tegmark
 there's a lot of interesting ways to think about AI safety you're one of theseminal people thinking about this
 problem among sort of being in the weeds of actually solving specific AI problemsyou also think about the big picture of
 where we're going so can you talk about several elements of it let's just talkabout maybe the control problem so this
 idea of losing ability to control the behavior and of a AI system so how doyou see that how do you see that coming
 about what do you think we can do to

0:37:05
Speaker 0 :manage it well so it doesn't take a
 genius to realize that if you make something that's smarter than you youmight have a problem you know in Turing
 Alan Turing you know wrote about the gave lectures about this you know 191951 painted a lecture on the radio and
 he basically says you know once the machine thinking method stops you knowvery quickly they'll outstrip humanity
 and you know if we're lucky we might be able to I think he says if we may beable to turn off the power at strategic
 moments but even so a species would be humbled yeah you can actually I thinkwas wrong about that right here is you
 you know if it's a sufficiently intelligent machine is not gonna let youswitch it off so it's actually in
 competition with you so what do you

0:38:00
Speaker 1 :think is meant just for a quick tangent
 if we shut off this super intelligent machine that our species will be humbled

0:38:09
Speaker 0 :I think he means that we would realize
 that we are inferior right that we we only survive by the skin of our teethbecause we happen to get to the off
 switch just in time you know and if we hadn't then we wouldhave lost control over the earth


0:38:31
Speaker 1 :so do you are you more worried when you
 think about this stuff about super intelligent AI or are you more worriedabout super powerful AI that's not
 aligned with our values so the paperclip

0:38:46
Speaker 0 :scenario is kind of I think so the main
 problem I'm working on is is the control problem the the problem of machinespursuing objectives that are as you say
 not aligned with human objectives and and this has been it has been the waywe've thought about I eyes since the
 beginning you you build a machine for optimizing and then you put in someobjective and it optimizes right and and
 you know we we can think of this as the the King Midas problem right because ifyou know so King Midas put in this
 objective right everything I touch you turned to gold and the gods you knowthat's like the machine they said okay
 done you know you now have this power and of course his food and his drink andhis family all turned to gold and then
 he's sighs misery and starvation and this is you know it's it's a warningit's it's a failure mode that pretty
 much every culture in history has had some story along the same lines you knowthere's the the genie that gives you
 three wishes and you know third wish is always you know please undo the firsttwo wishes because I messed up
 and you know and when author Samuel wrote his chest his checkup layingprogram which learned to play checkers
 considerably better than Martha Samuel could play and actually reached a prettydecent standard
 Norbert Wiener who was a one of the major mathematicians of the 20th centurysort of a father of modern automation
 control systems you know he saw this and he basicallyextrapolated you know as Turing did and
 said okay this is how we could lose control and specifically that we have tobe certain that the purpose we put into
 the machine as the purpose which we really desire and the problem is wecan't do that right you mean we're not


0:40:56
Speaker 1 :it's a very difficult to encode so to
 put our values on paper is really difficult or you're just saying it's

0:41:06
Speaker 0 :impossible your line is writing this so
 it's it theoretically it's possible but in practice it's extremely unlikely thatwe could specify correctly in advance
 the full range of concerns of humanity

0:41:25
Speaker 1 :that you talked about cultural
 transmission of values I think is how humans to human transmission of valueshappens right what we learned yeah I


0:41:32
Speaker 0 :mean as we grow up we learn about the
 values that matter how things how things should go what is reasonable to pursueand what isn't reasonable to pursue


0:41:44
Speaker 1 :machines can learn in the same kind of


0:41:46
Speaker 0 :way yeah so I think that what we need to
 do is to get away from this idea that you build an optimizing machine and youput the objective into it
 because if it's possible that you might put in a wrong objective and we alreadyknow this is possible because it's
 happened lots of times alright that means that the machine should never takean objective that's given as gospel
 truth because once it takes them the theobjective is gospel truth alright then
 it's the leaves that whatever actions it's taking in pursuit of that objectiveare the correct things to do so you
 could be jumping up and down and saying no you know no no no you're gonnadestroy the world but the machine knows
 what the true objective is and it's pursuing it and tough luck to you youknow and this is not restricted to AI
 right this is you know I think many of the 20th century technologies right soin statistics you you minimize a loss
 function the loss function is exogenously specified in control theoryyou minimize a cost function in
 operations research you maximize a reward function and so on so in allthese disciplines this is how we
 conceive of the problem and it's the wrong problem because we cannot specifywith certainty the correct objective
 right we need uncertainty we the machine to be uncertain about a subjective whatit is that it's post it's my favorite


0:43:20
Speaker 1 :idea of yours I've heard you say
 somewhere well I shouldn't pick favorites but it just sounds beautifulwe need to teach machines humility yeah


0:43:30
Speaker 0 :I mean it's a beautiful way to put it I


0:43:32
Speaker 1 :love it


0:43:34
Speaker 0 :that they humble oh yeah they know that
 they don't know what it is they're supposed to be doing and that thosethose objectives I mean they exist they
 are within us but we may not be able to explicate them we may not even know youknow how we want our future to go so
 exactly and the Machine you know a machine that's uncertain he's going tobe deferential to us so if we say don't
 do that well now the machines learn something a bit more about our trueobjectives because something that it
 thought was reasonable in pursuit of our objectives turns out not to be so nowit's learn something so it's going to
 defer because it wants to be doing what we really wantand you know that that point I think is
 absolutely central to solving the control problem and it's a differentkind of AI when you when you take away
 this idea that the objective is known then in fact a lot of the theoreticalframeworks that we're so familiar with
 you know Markov decision processes goal based planning you know standard gamesresearch all of these techniques
 actually become inapplicable and you get a more complicated problem becausebecause now the interaction with the
 human becomes part of the problem because the human by making choices isgiving you more information about the
 'true objective and that information helps you achieve the objective betterand so that really means that you're
 mostly dealing with game theoretic problems where you've got the machineand the human and they're coupled
 together rather than a machine going off by itself with a fixed objective which

0:45:39
Speaker 1 :is fascinating on the machine and the
 human level that we when you don't have an objective means you're togethercoming up with an objective I mean
 there's a lot of philosophy that you know you could argue that life doesn'treally have meaning we we together agree
 on what gives it meaning and we kind of culturally create things that give whythe heck we are in this earth anyway we
 together as a society create that meaning and you have to learn thatobjective and one of the biggest I
 thought that's what you were gonna go for a secondone of the biggest troubles we've run
 into outside of statistics and machine learning and AI and just humancivilization is when you look at I came
 from the south was born in the Soviet Union and the history of the 20thcentury we ran into the most trouble us
 humans when there was a certainty about the objective and you do whatever ittakes to achieve that objective whether
 you talking about in Germany or communist Russia oh yeah I get the

0:46:44
Speaker 0 :trouble I would say with you know
 corporations in fact some people argue that you know we don't have to lookforward to a time when AI systems take
 over the world they already have and they call corporations right thatcorporations happen to be using people
 as components right now but they are effectively algorithmic machines andthey're optimizing an objective which is
 quarterly profit that isn't aligned with overall well-being of the human race andthey are destroying the world they are
 primarily responsible for our inability to tackle climate change right so Ithink that's one way
 of thinking about what's going on with with cooperations but I think the pointyou're making you is valid that there
 are there are many systems in the real world where we've sort of prematurelyfixed on the objective and then
 decoupled the the machine from those that's supposed to be serving and Ithink you see this with government right
 government is supposed to be a machine that serves people but instead it tendsto be taken over by people who have
 their own objective and use government to optimize that objective regardless of

0:48:08
Speaker 1 :what people want do you have do you find
 appealing the idea of almost arguing machines where you have multiple Isystems with a clear fixed objective we
 have in government the red team and the blue team that are very fixed on theirobjectives and they argue and it kind of
 maybe it would disagree but it kind of seems to make it work somewhat that thethe duality of it okay let's go a
 hundred years back when there was still was going on or at the founding of thiscountry there was disagreement and that
 disagreement is where so there's a balance between certainty and forcedhumility because the power was


0:48:53
Speaker 0 :distributed yeah I think that the the
 the nature of debate and disagreement argument takes as a premise the ideathat you could be wrong right which
 means that you're not necessarily absolutely convinced that your objectiveis the correct one right if you were
 absolutely Guiness there'll be no point in having any discussion or argumentbecause you would never change your mind
 and there wouldn't be any any sort of synthesis or or anything like that so soI think you can think of argumentation
 as a as an implementation of a form of uncertain reasoningand you know I I've been reading
 recently about utilitarianism in the history of efforts to define in a sortof clear mathematical way a
 I feel like a formula for moral or political decision-making and it'sreally interesting that the parallels
 between the philosophical discussions going back 200 years and what you seenow in discussions about existential
 risk because you it's almost exactly the same so someone would say okay wellhere's a formula for how we should make
 decisions right so utilitarianism you know each person has a utilityfunction and then we make decisions to
 maximize the sum of everybody's utility mm-hmm right and then people point outwell you know in that case the best
 policy is one that leads to the enormous lis vast population all of whom areliving a life that's barely worth living
 right and this is called the repugnant conclusion and you know another versionis you know that we we should maximize
 pleasure and that's what we mean by utility and then you'll get peopleeffectively saying well in that case you
 know we might as well just have everyone hooked up to a heroin drip yeah you knowand they didn't use those words but that
 debate you know what's happening in the 19th century as it is now about AI thatif we get the formula wrong you know
 we're going to have AI systems working towards an outcome that in retrospectwould be exactly wrong do you think


0:51:23
Speaker 1 :there's it has beautifully put so the
 the echoes are there but do you think I mean if you look at sam Harris is ourimagination worries about the AI version
 of that because of the speed at which the things going wrong in theutilitarian context could happen yeah is
 that is that a worry for you yeah I I

0:51:47
Speaker 0 :think that
 you know it in most cases not in all but you know if we if we have a wrongpolitical idea you know we see it
 starting to go wrong and we're you know we're not completely stupid and so wesaid okay that was maybe that was a
 mistake let's try something different and andalso we're very slow and inefficient
 about implementing these things and so on so you have to worry when you havecorporations or political systems that
 are extremely efficient but when we look at AI systems or evenjust computers in general right they
 have this different characteristic from ordinary human activity in the past solet's say you were a surgeon you had
 some idea about how to do some operation right well and let's say you were wrongall right that that way of doing the
 operation would mostly kill the patient well you'd find out pretty quickly likeafter three maybe three or four tries
 right butthat isn't true for pharmaceutical
 companies because they don't do three or four operations they they manufacturethree or four billion pills and they
 sell them and then they find out maybe six months or a year later that ohpeople are dying of heart attacks or
 getting cancer from this drug and so that's why we have the FDA right becauseof the scalability of pharmaceutical
 production and you know and there have been some unbelievably bad episodes inthe history of pharmaceuticals and and
 adulteration of of products and so on that that have killed tens of thousandsor paralysed hundreds of thousands of
 people now with computers we have that same scalability problem that you cansit there and type for I equals 1 to 5
 billion do right and all of a sudden you're having an impact on a globalscale and yet we have no FDA right
 there's absolutely no controls at all it's over what a bunch of undergraduateswith too much caffeine can do to the
 world and you know we look at what happened with Facebook well social mediain general and click-through
 optimization so you have a simple feedback algorithm that's trying to justoptimize click-through that sounds
 reasonable right because you don't want to be feeding people ads that they don'tcare about I'm not interested in
 and you might even think of that process as simply adjusting the the feeding ofads or news articles or whatever it
 might be to match people's preferences right which sounds like a good idea butin fact that isn't how the algorithm
 works right you make more money the algorithm makes more money if it couldbetter predict what people are going to
 click on because then it can feed them exactly that right so the way tomaximize click-through is actually to
 modify the people to make them more predictable and one way to do that is tofeed them information which will change
 their behavior and preferences towards extremes that make them predictable nowwhatever is the nearest extreme or the
 nearest predictable point that's where you're going to end upthe machines will force you there now
 and then I think there's a reasonable argument to say that this among otherthings is contributing to the
 destruction of democracy in the world and where was the oversight of thisprocess where were the people saying
 okay you would like to apply this algorithm to five billion people on theface of the earth can you show me that
 it's safe can you show me that it won't have various kinds of negative effectsno there was no one asking that question
 there was no one placed between you know the undergrads were too much caffeineand the human race well it's just they


0:56:18
Speaker 1 :just did it and but some way outside the
 scope of my knowledge so economists would argue that the what is it theinvisible hand so the the capitalist
 system it was the oversight so if you're goingto corrupt society with whatever
 decision you make is a company then that's going to be reflected in peoplenot using your product sort of one
 that's one model of oversight so we

0:56:38
Speaker 0 :shall see but you know in the meantime
 you know that but you you might even have broken the political system thatenables capitalism to function well


0:56:51
Speaker 1 :you've changed it and so we should see


0:56:53
Speaker 0 :

0:56:54
Speaker 1 :yeah change changes often painful so my
 question is uh absolutely it's fascinatingyou're absolutely right that there is
 ZERO oversight on algorithms that can have a profound civilization changingeffect so do you think it's possible I
 mean I haven't have you seen government so do you think it's possible to createregulatory bodies oversight over AI
 algorithms which are inherently such cutting edge set of ideas and

0:57:29
Speaker 0 :technologies yeah but I think it takes
 time to figure out what kind of oversightwhat kinds of controls I mean took time
 to design the FDA regime you know and some people still don't like it and theywant to fix it
 and I think there are clear ways that it could be improved but the whole notionthat you have stage 1 stage 2 stage 3
 and here are the criteria for what you have to do to pass a stage 1 trial rightwe haven't even thought about what those
 would be for algorithms so I mean I think thereare there are things we could do right
 now with regard to bias for example we we have a pretty good technical handleon how to detect algorithms that are
 propagating bias that exists in data sets how to D by us those algorithms andand even what it's going to cost you to
 do that so I think we could start having some standards on that I think there arethere are things to do with
 impersonation of falsification that we could we could work on so I thanks ya or

0:58:42
Speaker 1 :

0:58:43
Speaker 0 :you know in a very simple point so
 impersonation ISM is a machine acting as if it was a person I can't see a realjustification for why we shouldn't
 insist that machines self-identify as machines you know where is the socialbenefit in in fooling people into
 thinking that this is really a person when it isn't you know I I don't mind ifit uses a human-like voice that's easy
 to understand that's fine but it should just say I'm a machine insome some form


0:59:20
Speaker 1 :people are speaking to that I would
 think relatively obvious factors I think mostly yeah I mean there is actually a

0:59:26
Speaker 0 :law in California that bans
 impersonation but only in certain restricted circumstances so for thepurpose of engaging in a for Geling
 transaction and for the purpose of modifying someone's voting behavior sothose are those are the circumstances
 where machines have to self-identify but I think this is you know arguably itshould be in all circumstances and then
 when you talk about deep fakes you know we're just beginning but already it'spossible to make a movie of anybody
 saying anything in ways that are pretty hard to detect including yourself

1:00:12
Speaker 1 :because you're on camera now and your
 voice is coming through with high

1:00:15
Speaker 0 :resolution so you could take what I'm
 saying and replaces it with it pretty much anything else you wanted me to besaying yeah and even it will change my
 lips and expression expressions to fit and there's actually not much in the wayof real legal protection against that I
 think in the commercial area you could say yeah that's you're using my brandand so on that there there are rules
 about that but in the political sphere I think it's at the moment it's you knowanything goes so like that could be
 really really damaging and let me just

1:00:53
Speaker 1 :try to make not an argument but try to
 look back at history and say something dark in essence is while regulationseems to be oversight seems to be
 exactly the right thing to do here it seems that human beings what theynaturally do is they wait for something
 to go wrong if you're talking about nuclear weaponsyou can't talk about nuclear weapons
 being dangerous until somebody actually like the United States drops the bomb orChernobyl melting do you think we will


1:01:31
Speaker 0 :have to wait for things going wrong in a


1:01:33
Speaker 1 :way that's obviously damaging to society
 not an existential risk but obviously

1:01:40
Speaker 0 :damaging or do you have faith that I I


1:01:42
Speaker 1 :

1:01:43
Speaker 0 :hope not but I mean I think we do have
 to look at history and when you know so the two examples you gave nuclearweapons and nuclear power are very very
 interesting because you know in nuclear weapons we knew in the early years ofthe 20th century that atoms contained a
 huge amount of energy right we had e equals mc-squared we knew the the massdifferences between the different atoms
 and their components and we knew that you might be able to make an incrediblypowerful explosive so HG Wells wrote
 science fiction book I think in 1912 Frederick Soddy who was the guy whodiscovered isotopes so Nobel Prize
 winner he gave a speech in 1915 saying thatthis new explosive would be the
 equivalent of 150 tons of dynamite which turns out to be about right and you knowKenton this was in World War one right
 so he was imagining how much worse the world would be if we were using thatkind of explosive but the physics
 establishment simply refused to believe that these things could be made

1:03:04
Speaker 1 :including the people who are making it
 well so they were doing the nuclear

1:03:06
Speaker 0 :

1:03:09
Speaker 1 :physics I mean eventually were the ones
 who made it and Rockwell for me or

1:03:13
Speaker 0 :whoever well so up to the the
 development was was mostly theoretical so it was people using sort of primitivekinds of particle acceleration and doing
 experiments at the at the level of single particles or collections ofparticles they they they want
 yet thinking about how to actually make a bomb or anything like that they butthey knew the energy was there and they
 figured if they understood it better it might be possible but the physicsestablishment their view and I think
 because they did not want it to be true their view was that it could not be truethat this could not provide a way to
 make a super weapon and you know there was this famous speech given byRutherford who was the sort of leader of
 nuclear physics and I was on September 11th 1933 and he he said you know anyonewho talks about the possibility of
 obtaining energy from transformation of atoms is talking complete moonshine andthe next the next morning Leo Szilard
 read about that speech and then invented the nuclear chain reaction and so assoon as he invented he soon as he had
 that idea that you could make a chain reaction with neutrons because neutronswere not repelled by the nucleus so they
 could enter the nucleus and then continue the reaction as soon as he hasthat idea he instantly realized that the
 world was in deep doo-doo because this is 1933 right you know Hitler hadrecently come to power in Germany
 Zil odd was in London and eventually became a refugee and and came to the USand the in the process of having the
 idea about the chain reaction he figured out basically how to make a bomb andalso how to make a reactor and he
 patented the reactor 2:34 but because of the situation thegreat power conflict situation that he
 could see happening he kept that a secret and so between then and thebeginning of World War two people were
 working including the Germans on how to actually create Neutron sources rightwhat specific fission reactions would
 produce neutrons of the right energy to continue the reaction and and that wasdemonstrated in Germany I think in 1938
 if I remember correctly the first nuclear weapon patent was 1939 by theFrench so this was actually you know
 this was actually going on you know well before World Wartwo really got going and then you know
 the British probably had the most advanced capability in this area but forsafety reasons among others and blush
 which is sort of just resources they moved the program from Britain to the USand then that became Manhattan Project
 so the the the reason why we couldn't have any kind of oversight of nuclearweapons and nuclear technology was
 because we were basically already in an arms race in a war and but you you've

1:06:54
Speaker 1 :mentioned then in the 20s and 30s so
 what are the echoes yeah the way you've described this story I mean there'sclearly echoes why do you think most a I
 researchers folks who are really close to the metalthey really are not concerned about it
 and they don't think about it whether they don't want to think aboutit it's but what are the yeah why do you
 think that is what are the echoes of the nuclear situation to the currentsituation and what can we do about it I


1:07:28
Speaker 0 :think there is a you know a kinda modak
 motivated cognition which is a term in psychology means that you believe whatyou would like to be true rather than
 what is true and you know it's it's unsettling to think that what you'reworking on might be the end of the human
 race obviously so you would rather instantly deny itand come up with some reason why it
 couldn't be true and the you know I have I collected a long list of reasons thatextremely intelligent competent AI
 scientists have come up with for why we shouldn't worry about this you know forexample calculators are super human at
 arithmetic and they haven't taken over the world so there's nothing to worryabout well okay my five-year-old you
 know could have figured out why that was an unreasonable and and really quiteweak argument you know another one was
 you know you while it's theoretically possible that you could have superhumanAI destroy the world you know it's also
 theoretically possible that a black hole could materialize right next to theearth and destroy humanity I mean yes
 it's theoretically possible quantum theoretically extremely unlikely that itwould just materialize right there but
 that's a completely bogus analogy because you know if the whole physicscommunity on earth was working to
 materialize a black hole in near Earth orbit right wouldn't you ask them isthat a good idea is that gonna be safe
 you know what if you succeed all right right and that's the thing right the AIis sort of refused to ask itself what if
 you succeed and initially I think that was because it was too hard but you knowAlan Turing asked himself that and he
 said we'd be toast right if we were lucky we might be able to switch off thepower but probably we'd be toast but


1:09:39
Speaker 1 :there's also an aspect that because
 we're not exactly sure what the future holds it's not clear exactly sotechnically what to worry about sort of
 how things go wrong and so there is something it feels like maybe you cancorrect me if I'm wrong but there's
 something paralyzing about worrying about something that logically isinevitable but you don't really know
 what that will look like yeah I think

1:10:11
Speaker 0 :that's that's it's a reasonable point
 and you know the you know it's certainly in terms of existential risks it'sdifferent from you know asteroid
 collides with the earth right right which again is quite possible you knowit's happened in the past it'll probably
 happen again we don't right we don't know right now but if we did detect anasteroid that was going to hit the earth
 in 75 years time we'd certainly be doing something about it well it's clear

1:10:41
Speaker 1 :there's got big rocks we'll probably
 have a meeting you see what do we do about the big rock

1:10:46
Speaker 0 :will they I write with a I I mean the
 very few people who think it's not gonna happen within the next 75 yearsI know rod Brooks doesn't think it's
 gonna happen maybe and ruing doesn't think it's happened but you know a lotof the people who work day-to-day you
 know as you say at the rock face they think it's gonna happen I think themedian estimate from AI researchers is
 somewhere in forty to fifty years from from now or maybe a little you know Ithink in Asia they think it's gonna be
 even faster than that I am I'm a little bit more conservative I think probablytake longer than that but I think it's
 you know as happened with nuclear weaponswell I went overnight it can happen
 overnight that you have these breakthroughs and we need more than onebreakthrough but you know the it's on
 the order of half a dozen this is a very rough scale but so half a dozenbreakthroughs of that nature it would
 have to happen for us to reach the superhuman AI but the you know the AIresearch community is vast now the
 massive investments from governments from corporations tons of really reallysmart people you know you just have to
 look at the rate of progress in different areas of AI to see that thingsare moving pretty fast so to say oh it's
 just gonna be thousands of years I don't see any basis for that you know I seeyou know for example the the Stanford
 hundred year AI project right which is supposed to be sort of you know theserious establishment view their most
 recent report actually said it's probably not even possibleWow right which if you want a perfect
 example of people in denial that's it because you know for the wholehistory of AI we've been saying to
 philosophers who said it wasn't possible well you have no idea what you'retalking about of course it's possible
 right give me an give me an argument for why it couldn't happen and there isn'tone all right and now because people are
 worried that maybe a oh it might get a bad name or or I just don't want tothink about this they're saying okay
 well of course it's not really possible you know and we imagine right imagine ifyou know the the leaders of the cancer
 biology community got up and said well you know of course curing cancer it'snot really possible complete outrage and
 dismay and you know I I find this really a strange phenomenon so okay so if youaccept it as possible
 and if you accept that it's probably going to happenthe point that you're making that you
 know how does it go wrong a valid question without that without an answerto that question then you stuck with
 what I call the gorilla problem which is you know the problem that the gorillasface right they made something more
 intelligent than them namely us a few million years ago and now now they're indeep doo-doo yeah so there's really
 nothing they can do they've lost the control theater they failed to solve thecontrol problem of controlling humans
 and so they've lost so we don't want to be in that situation and if the gorillasproblem is is the only formulation you
 have there's not a lot you can do right other than to say okay we should try tostop you know we should just not make
 the humans or right in this case not make the AI and I think that's reallyhard to do
 - I'm not actually proposing that that's a feasible course of action I also thinkthat you know if properly control a I
 could be incredibly beneficial so the but it seems to me that there's athere's a consensus that one of the
 major failure modes is this loss of control that we create AI systems thatare pursuing incorrect objectives and
 because the AI system believes it knows what the objective is it has noincentive to listen to us anymore
 so to speak right it it's just carrying out the the strategy that it it hascomputed as being the optimal solution
 and you know it may be that in the process it needs to acquire moreresources to increase the possibility of
 success or prevent various failure modes by defending itself against interferenceand so that collection of problems I
 think is something we can address yes that the other problems are roughlyspeaking you know misuse right so even
 if we solve the control problem we make perfectly safe controllable AI systemswell why you know why does dr. evil
 going to use those right he wants to just take over the world and he'll makeunsafe AI system said but then get out
 of control so that's one problem which is sort of a you know a partly apolicing problem
 partly a-- a sort of a cultural problem for the profession of how we teachpeople what kinds of AI systems are safe
 you talk about autonomous weapon system

1:16:24
Speaker 1 :and how pretty much everybody agrees
 there's too many ways that that can go horribly wrong if this great slaughterBOTS movie that kind of illustrates that


1:16:35
Speaker 0 :beautifully I want to talk that's
 another there's another topic I I'm happy talking about the I just want tomention that what I see is the third
 major failure mode which is overuse not so much misuse but overuse of AIthat we become overly dependent so I
 call this the wooly problems if you seen wall-e the movie all right all thehumans are on the spaceship and the
 machines look after everything for them and they just watch TV and drink biggulps and they're all sort of obese and
 stupid and they sort of totally lost any notion of human autonomy andyou know so a in effect right this would
 happen like the slow boiling frog right we would gradually turn over more andmore of the management of our
 civilization to machines as we are already doing in this you know this ifthis process continues you know we sort
 of gradually switch from sort of being the Masters of Technology to just beingthe guests right so so we become guests
 on a cruise ship you know which is fine for a week but not not further the restof eternity right you know and it's
 almost irreversible right once you once you lose the incentive to for exampleyou know learn to be an engineer or a
 doctor or a sanitation operative or or any other of the the infinitely manyways that we maintain and propagate our
 civilization you know if you if you don't have theincentive to do any of that you won't
 and then it's really hard to recover and of course there's just one of the

1:18:19
Speaker 1 :technologies that could that third
 failure mode result in that there's probably other technology in generaldetaches us from it does a bit but the


1:18:28
Speaker 0 :the the difference is that in terms of
 the knowledge to to run our civilization you know up to now we've had noalternative but to put it into people's
 heads right and if you oh it's not we're

1:18:42
Speaker 1 :with Google I mean so software in
 general so I probably if computers in

1:18:45
Speaker 0 :general but but the you know the
 knowledge of how you know how a sanitation system works you know that'san the AI has to understand that it's no
 good putting it into Google so I mean we we've always put knowledge in on paperbut paper doesn't run our civilization
 it only runs when it goes from the paper into people's heads again right so we'vealways propagated civilization through
 human minds and we've spent about a trillion person years doing thatliterature right you you can work it out
 yeah but right is about just over a hundred billion people who've ever livedand each of them has spent about ten
 years learning stuff and to keep their civilization going and so that's atrillion person years we put into this
 effort beautiful way to describe all of

1:19:32
Speaker 1 :

1:19:34
Speaker 0 :civilization and now we're you know
 we're danger of throwing that away so this is a problem that AI console it'snot a technical problem it's a you know
 if we do our job right the AI systems will say you know the human race doesn'tin the long run want to be passengers in
 a cruise ship the human race wants autonomy this is part of humanpreferences so we the AI systems are not
 going to do this stuff for you you've got to do it for yourself right I'm notgoing to carry you to the top of Everest
 in an autonomous helicopter you have to climb it if you want to get the benefitand so on so
 but I'm afraid that because we are short-sighted and lazy we're gonnaoverride the AI systems and and there's
 an amazing short story that I recommend to everyone that I talk to about thiscalled the machine stops
 written in 1909 by Ian Foster who you know wrote novels about the BritishEmpire and sort of things that became
 costume dramas on the BBC but he wrote this one science fiction story which isan amazing vision of the future it has
 it has basically iPads it has video conferencing it has MOOCsit has computer and computer induced
 obesity I mean literally the whole thing it's what people spend their time doingis giving online courses or listening to
 online courses and talking about ideas but they never get out there in the realworld that they don't really have a lot
 of face-to-face contact everything is done onlineyou know so all the things we're
 worrying about now were described in this story and and then the human racebecomes more and more dependent on the
 Machine loses knowledge of how things really run and then becomes vulnerableto collapse and so it's a it's a pretty
 unbelievably amazing story for someone writing in 1909 to imagine all this lossyeah so there's very few people that


1:21:41
Speaker 1 :represent artificial intelligence more


1:21:46
Speaker 0 :than you Russell so it's all my fault


1:21:54
Speaker 1 :right you're often brought up as the
 person well Stuart Russell like the AI person is worried about this that's whyyou should be worried about it do you
 feel the burden of that I don't know if you feel that at all but when I talk topeople like from you talk about set
 people outside of computer science when they think about this still Russell isworried about AI safety you should be
 worried too do you feel the burden of

1:22:23
Speaker 0 :that I mean in a practical sense yeah
 because I'd yet you know a dozen sometimes 25 invitations a dayto talk about it to give interviews to
 write press articles and so on so in that very practical sense I'm seeingthat people are concerned and really
 interested about this are you worried

1:22:49
Speaker 1 :that you could be wrong as all good
 scientists are of course I worry about

1:22:53
Speaker 0 :that all the time I mean that's that's
 always been the way that I I've worked you know is like I have an argument inmy head with myself right so I have some
 idea and then I think okay how could that be wrong or did someone elsealready have that idea so I'll go and
 you know search and as much literature as I can't to see whether someone elsealready thought of that or or even
 refuted it so you know I right now I'm I'm reading a lot of philosophy becauseyou know in in the form of the debate so
 V over utilitarianism and other kinds of moral moral formulas shall we say peoplehave already thought through some of
 these issues but you know what one of the things I'm I'm not seeing in a lotof these debates is this specific idea
 about the importance of uncertainty in the objective that this is the way weshould think about machines that are
 beneficial to humans so this idea of provably beneficial machines based onexplicit uncertainty in the objective
 you know it seems to be you know my gut feeling is this is the core of it it'sgonna have to be elaborated in a lot of
 different directions and there are a lot

1:24:22
Speaker 1 :of lis beneficial yeah but they're I


1:24:23
Speaker 0 :mean it has to be right we can't afford
 you know hand-wavy beneficial yeah because there are you know whenever wedo hand wavy stuff there are loopholes
 and the thing about super intelligent machines is they find the loopholes youknow just like you know tax evaders if
 you don't write your tax law properly that people will find loopholes and endup paying no taxes and
 and so you should think of it this way and in getting those definitions rightyou know it is really a long process you
 know so you can you can define mathematical frameworks and within thatframework you can prove mathematical
 theorems that yes this will you know this this theoretical entity will beproven beneficial to that theoretical
 entity but that framework may not match the real world in some crucial way so

1:25:20
Speaker 1 :long process thinking through it of
 iterating and so on the last question yep you have ten seconds to answer itwhat is your favorite sci-fi movie about
 AI I would say interstellar has my

1:25:31
Speaker 0 :favorite robots or beat it Space Odyssey
 yeah yeah yeah so so tars the robots one of the robots in interstellar is the waya robot should behave
 and I would say ex machina is in some ways the one like the one that makes youthink in a nervous kind of way about a
 lot where we're going well Stuart thank

1:25:59
Speaker 1 :

