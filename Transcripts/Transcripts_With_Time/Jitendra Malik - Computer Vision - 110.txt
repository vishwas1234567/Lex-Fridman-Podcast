0:00:00
Speaker 1 :the following is a conversation with
 jitendra malik a professor at berkeley and one of theseminal figures in the field of computer
 vision the kind before the deep learningrevolution and
 the kind after he has been cited over 180 thousand times and has mentoredmany world-class researchers in computer
 science quick summary of the ads two sponsorsone new one
 which is better help and an old goody expressvpn please consider supportingthis podcast by going to betterhelp.com
 lex and signing up at expressvpn.com lexpod click the links buy the stuffit really is the best way to support
 this podcast and the journey i'm on if you enjoy this thing subscribe onyoutube review it with 5 stars on apple
 podcast support it on patreon or connect with meon twitter
 at lex friedman however the heck you spell thatas usual i'll do a few minutes of ads
 now and never neons in the middle that can break the flow of the conversationthis show is sponsored by better help
 spelled h-e-l-p help check it out at betterhelp.com lexthey figure out what you need and match
 you with a licensed professional therapistin under 48 hours it's not a crisis line
 it's not self-help it's professional counseling done securelyonline i'm a bit from the david goggins
 line of creatures as you may know and so have some demonsto contend with
 usually on long runs or all nights workingforever and possibly full of self-doubt
 it may be because i'm russian but i think suffering is essential forcreation
 but i also think you can suffer beautifully in a way that doesn'tdestroy you
 for most people i think a good therapist can help in thisso it's at least worth a try check out
 their reviews they're good it's easy privateaffordable
 available worldwide you can communicate by text anytimeand schedule weekly audio and video
 sessions i highly recommend that you check themout at betterhelp.com
 lex this show is also sponsored by expressvpn get it at expressvpn.comto support this podcast and to get an
 extra three months free on a one-year package i've beenusing expressvpn for many years
 i love it i think expressvpn is the best vpn out there they told me to say itbut it happens to be true it doesn't log
 your data it's crazy fast and it's easy to useliterally just
 one big sexy power on button again for obvious reasons it's reallyimportant that they don't log your data
 it works on linux and everywhere else too but reallywhy use anything else shout out to my
 favorite flavor of linux ubuntu mate 2004 once again get it atexpressvpn.comlexpod
 to support this podcast and to get an extra three months free and a one yearpackage
 and now here's my conversation with jitendrain 1966 seymour papper
 at mit wrote up a proposal called the summer vision project to be givenas far as we know to 10 students to work
 on and solve that summer so that proposal outlined many of thecomputer vision tasks we still work on
 today why do you think we underestimate andperhaps we did underestimate and perhaps
 still underestimate

0:03:44
Speaker 0 :how hard computer vision is because
 most of what we do in vision we do unconsciously or subconsciouslyin human vision in human vision so that
 gives us this that effortlessness gives us thesense that oh
 this must be very easy to implement on a computernow this is why
 the early researchers in ai got it so wronghowever if you go into neuroscience or
 psychology of human vision then the complexitybecomes very clear
 the fact is that a very large part of thethe cerebral cortex is devoted to visual
 processing i mean and this is true in otherprimates as well
 so once we looked at it from a neuroscience or psychology perspectiveit it becomes quite clear that the
 problem is very challenging and it will

0:04:38
Speaker 1 :take some time
 you said the higher level parts are the

0:04:42
Speaker 0 :harder parts
 i think vision appears to to be easy becauseuh most of what visual processing
 is subconscious or unconscious right so we underestimate the difficultywhereas
 uh when you are like proving a mathematical theorem orplaying chess
 the difficulty is much more evident so because it is your conscious brain whichis processing
 uh various aspects of the problem-solvingbehavior whereas in vision all this is
 happening but it's not in your awareness it's in your it's operatingbelow that


0:05:25
Speaker 1 :but it's it still seems strange yes
 that's true but it seems strange that as computer vision researchers forexample
 the community broadly is time and time again makes the mistake of umthinking the problem is easier than it
 is or maybe it's not a mistake we'll talk a little bit about autonomousdriving for example how hard of a vision
 task that is it do do you think i mean whatis it just human nature or is there
 something fundamental to the vision problem that we we underestimatewe're still not able to be cognizant of
 how hard the problem is

0:06:05
Speaker 0 :yeah i think in the early days it could
 have been excused because in the early days all aspects of ai wereregarded as too easy
 but i think today it is much less excusableand i think why people
 fall for this is because of what i call the fallacy of the successful firststep there are many problems in
 vision where getting 50 of the solution you can get in oneminute getting to 90 percent
 can take you a day getting to 99 percent may take you five years and99.99 may be not in your lifetime


0:06:49
Speaker 1 :i wonder if that's a unique division
 that it seems that language people are not soconfident about so natural language
 processing people are a little bit more cautious about our abilityto to solve that problem
 i think for language people intuit that we have to be able to donatural language understanding for
 vision it seems that we're not cognizant or wedon't think about how much
 understanding is required it's probably still an open problembut in your sense how much understanding
 is required to solve vision like this put another way how muchsomething called common sense reasoning
 is required to really be able to interpreteven static scenes yeah so vision


0:07:39
Speaker 0 :operates at
 uh at all levels and there are parts which arewhich can be solved with what we could
 call maybe peripheral processing so in the in the human vision literaturethere used to be these terms
 sensation perception and cognition which roughly speaking referred to likethe front end of processing
 middle stages of processing and higher level of processingand i think they made a big deal out of
 out of this and they wanted to just study only perception and thendismiss certain certain problems as
 being quote cognitive but really i think these are artificialdivides
 the problem is continuous at all level and there are challenges at all levelsthe techniques that we have today
 they work better at the lower and mid levels of the problemi think the higher levels of the problem
 quote the cognitive levels of the problemare there and we
 in many real applications we have to confront themnow how much that is necessary will
 depend on the application for some problems it doesn't matter forsome problems it matters a lot
 so i am for example a pessimist on fully autonomous drivingin the near future
 and the reason is because i think there will bethat 0.01 percent of the cases
 where quite sophisticated cognitive reasoning is called forhowever there are tasks where you can
 first of all they are much more they are robust so in the sense thaterror rates error is not so much of a
 problem for example uh uh let's say we areyou're doing uh
 image search you're trying to get images based on somesome some description some visual
 description we are very tolerant of errors thereright i mean when google image search
 gives you some images back and a few of them arewrong it's okay it doesn't hurt anybody
 there's no there's not a matter of life and deathbut
 making mistakes when you're driving at 60 miles per hour and youcould potentially kill somebody


0:10:05
Speaker 1 :is much more important so just for the
 for the fun of it since you mentioned let's go there brieflyabout autonomous vehicles so one of the
 companies in the space tesla is work with andre karpathy and elonmusk are working on
 a system called autopilot which is primarily a vision-based system witheight cameras
 and uh basically a single neural network a multi-task neural networkthey they call it hydro net multiple
 heads so it does multiple tasks but is formingthe same representation
 at the core do you think driving can be convertedin this way to uh purely a vision
 problem and then solved within you with learningor even more specifically in the current
 approach what do you think about what teslaautopilot team is doing


0:10:56
Speaker 0 :so the way i think about it is that
 there are certainly subset subsets of the visual baseddriving problem which are quite solvable
 so for example driving in freeway conditionsis quite a solvable problem i think
 there were demonstrations of that going back to the 1980s bysomeone called ernst stickmans in munich
 in the 90s there were approaches from carnegie mellon there were approachesfrom
 our team at berkeley in the 2000s there were approaches from stanfordand so on so autonomous driving in
 certain settings is very doable the challenge is to have an autopilotwork
 under all kinds of driving conditions at that point it's not just a questionof vision
 or perception but really also of control and dealing with all the edge cases

0:11:53
Speaker 1 :so where do you think most of the
 difficult cases to me even the highway driving is anopen problem because
 uh it applies the same 50 90 95 99 rule or the first step the fallacy of thefirst step i forget how you put
 it we fall victim to i think even highway driving has a lot of elementsbecause to solve autonomous driving you
 have to completely relinquish the the fat help of a human beingyou're always in control so that you're
 really going to feel the edge cases so i i think even highway driving isreally difficult but
 in terms of the general driving task do you thinkvision is the fundamental problem or is
 it also your action the the interactionwith the environment
 the ability to uh and then like the middle ground i don't know if you putthat under vision which is
 trying to predict the behavior of others which is a little bitin the world of understanding the scene
 but it's also trying to form a model of the actors in the scene

0:13:00
Speaker 0 :and predict their behavior yeah i
 include that in vision because to me perception blends into cognitionand building predictive models of other
 agents in the world which could be other agents could bepeople other agents could be other cars
 that is part of the task of perception becauseperception always has to uh not tell us
 what is now but what will happen because what's now is boring it's doneit's over with
 okay yeah we care about the future because weact in the future and we care about the


0:13:33
Speaker 1 :past
 and as much as it informs what's going to happen in the future

0:13:37
Speaker 0 :so i think we have to build predictive
 models of of of behaviors of people and and those canget quite
 complicated so uh uh i mean uh i i've seen examples of thisin
 uh actually i mean i own a tesla and it has various safety features builtin
 and uh what i see are these examples wherelet's say there is some uh skateboarder
 i mean this i and i i don't want to be toocritical because
 obviously this is these are the systems are always being improvedand any specific criticism i have
 maybe the system six months from now will not have thatthat that particular failure mode
 so uh it it had it it had the wrong response andit's because it couldn't predict
 what what this skateboarder was going to dookay and because it really required that
 higher level cognitive understanding of what skateboarders typically do asopposed to a normal pedestrian
 so what might have been the correct behavior for a pedestriana typical behavior for pedestrian was
 not the typical behavior for a skateboarder right yeahand uh so so therefore
 to do a good job there you need to have enough data whereyou have pedestrians you also have
 skateboarders you've seen enough skateboarders to seewhat
 uh what kinds of patterns or behavior they haveso it is it is in principle with enough
 data that problem could be solved but uh i think our currentsystems computer vision systems they
 need far far more data than humans do forlearning those


0:15:33
Speaker 1 :same capabilities so say that there is
 going to be a system that solves autonomous drivingdo you think it will look similar to
 what we have today but have a lot more data perhaps morecompute but the fundamental
 architectures involved like neuro well in the case of teslaautopilot is
 neural networks do you think it will look similarin that regard and we'll just have more


0:15:56
Speaker 0 :data that's a
 scientific hypothesis as which way is it going to gouh i will tell you what i would bet on
 uh so and this is at my generalphilosophical position on how these
 uh learning systems have been uh what we have found currently veryeffective in
 computer vision uh with in in the deep learning paradigm issort of tabula rasa learning and tabular
 us are learning in a supervised way with lots and lotsof what's going on
 in the sense that blank slate we just have the system which isgiven a series of experiences in this
 setting and then it learns there now if let's think about human drivingit is not tabular assad learning
 so at the age of 16 in high school uh a teenager goes into uhgoes into driver ed class right and now
 at that point they learn but at the age of 16 they are already visual geniusesbecause from 0 to 16 they have built a
 certain repertoire of vision in fact most of it has probably beenachieved by
 age 2 right in in this period of age up to age 2 they know that the world isthree-dimensional they know how
 objects look like from different perspectivesthey know about occlusion they
 know about common dynamics of humans and other bodiesthey have some notion of intuitive
 physics so they they built that up from theirobservations and interactions
 in early childhood and of course reinforced throughtheir their growing up to age 16. so
 then at age 16 when they go into driver edwhat are they learning they're not
 learning afresh the visual world they have a mastery of the visual worldwhat they are learning
 is control okay they are learning how to be smoothabout control about steering and brakes
 and so forth they're learning a sense of typicaltraffic situations
 now the the that education process can be quite short because they arecoming in as visual geniuses
 and of course in their future they're going to encounter situations which arevery novel
 right so during my driver ed class that i may not have had to deal with askateboarder i may not have had to deal
 with a truck driving in front of me who's fromwho's where the back opens up and some
 junk gets dropped from the truck and i have to deal with it right but ican deal with this
 as a driver even though i did not encounter this in my driver atclass and the reason i can deal with it
 is because i have all this general visual knowledge and expertise

0:18:54
Speaker 1 :and uh do you think the learning
 mechanisms we have today can do that kind of long-termaccumulation of knowledge
 or do we have to uh do some kind of you know in the the the work that led upto expert systems with knowledge
 representation you know the broader field of what ofartificial intelligence
 uh worked on this kind of accumulation of knowledgedo you think neural networks can do the


0:19:21
Speaker 0 :same i think uh
 i don't see any in principle problem with neural networks doing itbut i think the learning techniques
 would need to evolve significantly so the current uh the currentlearning techniques that we have yeah is
 our supervised learning you're given lots of examplesxiy pairs and you you learn the
 functional mapping between them i think that human learning is farricher than that
 it includes many different components there arethere is a a child explores the world
 and sees as for example a child takes an object and manipulates itin his or her hand and therefore gets to
 see the object from different points of viewand the child has commanded the movement
 so that's a kind of learning data but the learning data has beenarranged by the child and this is a very
 rich kind of data the child can do variousexperiments with the world so
 so there are many aspects of sort of human learning and these have beenstudied in
 in child development by psychologists and they what they tell us is thatsupervised learning is a very small part
 of it there are many different aspects oflearning
 and what we would need to do is to develop models ofall of these and then
 train our systems in that with that kind of

0:21:01
Speaker 1 :uh protocol so new new methods of
 learning yes some of which might imitate thehuman brain but you also
 in your talks have mentioned some of the compute side of thingsthe in terms of the difference in the
 human brain or referencing marvik hans marvel the sodo you do you think there's something
 interesting valuable to consider about the differencein the computational power of the human
 brain versus the computers of today in terms ofinstructions


0:21:33
Speaker 0 :per second yes so if we go back
 uh so so this is a point i've been making for 20 years nowand i think once upon a time the way i
 used to argue this was that we just didn't havethe computing power of the human brain
 our computers were uh were not quite there and i mean there is awell well-known trade-off which we know
 that the that neurons are slow compared totransistors but uh but we have a lot of
 them and they have a very high connectivitywhereas in silicon you have much faster
 devices transistors switch at on the order of nanoseconds but theconnectivity is usually smaller
 right at this point in time i mean we are now talking about2020 we do have if you consider the
 latest gpus and so on amazing computing power and if we lookback at enhanced modex type of
 calculations which he did in the 1990s we may be there today in terms ofcomputing power comparable to the brain
 but it's not in the of the same style it's of a very different styleso i mean for example the the style of
 computing that we have in our gpus is far far more power hungry thanthe style of computing that is there in
 the human brain or other biological uh entities

0:23:04
Speaker 1 :yeah and that the efficiency part is uh
 we're gonna have to solve that in order to build actual real world systemsof large scale let me ask sort of
 the high level question step taking a step backhow would you articulate the general
 problem of computer vision does such a thing exist so if you lookat the computer vision conferences and
 the work that's been going on it's often separated into differentlittle segments
 breaking the problem of vision apart into whether segmentation3d reconstruction object detection
 i don't know image capturing whatever uh there's benchmarks for eachbut if you were to sort of
 philosophically say what is the big problem of computer vision doessuch a thing exist


0:23:54
Speaker 0 :yes but it's not in isolation so
 if we have to so for all intelligence tasks ialways go back to sort of biology or
 humans and if we think about vision or perception in that setting werealize that
 perception is always to guide action perceptionin a for a biological system does not
 give any benefits unless it is coupled with action so wecan go back
 and think about the first multicellular animalswhich arose in the cambrian era you know
 500 million years ago and uh these animals could moveand they could see in some ways and
 their two activities helped each other because uhuh how does movement help movement
 helps that because you can get food in different placesbut you need to know where to go and
 that's really about perception or seeing i mean i meanvision is
 perhaps the single most perception sense butall the others are equally are also
 important so uh so perception and action kind of growgo together
 so earlier it was in these very simple feedback loopswhich were about uh finding food
 or avoiding becoming food if there's a predator running uhtrying to you know eat you up
 and and so forth so so we must at the fundamental level connectperception to action then
 as we evolved uh perception became more and more sophisticatedbecause it served many more purposes and
 uh so today we have what seems like a fairly general purpose capabilitywhich can look at the external world and
 build and a model of the external world inside thehead
 we do have that capability that model is not perfectand psychologists have great fun in
 pointing out the ways in which the model in your head is not a perfectmodel of the external world
 and they have create various illusions toshow the ways in which it is imperfect
 but it's amazing how far it has come from avery simple
 perception action loop that you exists in you knowan animal 500 million years ago once we
 have this these very sophisticated visual systemswe can then
 impose a structure on them it's we as scientists who are imposing thatstructure
 where we have chosen to characterize this part of the system as thiscode module of object detection or quote
 this module of 3d reconstruction what's going on is really all of theseprocesses are running
 simultaneously and uh and and they are running simultaneouslybecause originally their purpose was
 in fact to help guide action so

0:27:00
Speaker 1 :as a guiding general statement of a
 problem do you think we can say that the the general problemof computer vision
 you said in humans it was tied to action do you think we should also say thatultimately the the goal
 the problem of computer vision is to sense the worldin the way that helps you act in the


0:27:26
Speaker 0 :world
 yes i think that's the most fundamental uhthat's the most fundamental purpose
 we have by now hyper evolved so we have this visual system which canbe used for other things
 for example judging the aesthetic value of a paintingand this is not guiding action maybe
 it's guiding action in terms of how much money you will put in your auction bidbut that's a bit stretched
 but the basics are in fact in terms of actionbut we have we've evolved
 really this hyper uh we have hyper evolved our visual system

0:28:08
Speaker 1 :actually just too uh sorry to interrupt
 but perhaps it is fundamentally about action you kind ofjokingly said about spending
 but perhaps the capitalistic uh drive that drives a lot of thedevelopment in this world
 is is about to exchange your money and the fundamental action is money if youwatch
 netflix if you enjoy watching movies you're using your perception system tointerpret the movie
 ultimately your enjoyment of that movie means you'll subscribe to netflixso the action is this uh
 this extra layer that we've developed in modern society perhaps this isfundamentally tied to the action of
 spending money

0:28:46
Speaker 0 :well certainly with respect to uh
 you know interactions with firms so so in this homo economics rolewhen you're interacting with firms it
 does become uh it does become that that's what else

0:29:01
Speaker 1 :is there
 uh that was a rhetorical question okay soto to linger on the division between the
 static and the dynamic so much of the work in computer visionso many of the breakthroughs that you've
 been a part of have been in the static world inlooking at static images and then you've
 also worked on starting but it's a muchsmaller degree the community is looking
 at dynamic and video at dynamic scenes and then there isrobotic vision
 which is dynamic but also where you actually have a robot in the physicalworld
 interacting based on that vision which problem is harderthe the the intuit sort of the the
 trivial first answers well of course one image is harder butso if you look at a deeper question
 there are we um what's the term cuttingourselves
 cutting ourselves at the knees or like making the problem harder by focusing on

0:30:07
Speaker 0 :the images that's a fair question i
 think sometimes we we can simplify our problemso much
 that we essentially lose part of the juice that could enable usto solve the problem
 and one could reasonably argue that to some extent this happens when we go fromvideo to single images
 now historically uh you have to consider the limits ofimposed by the competition capabilities
 we had so if we many of the choices made in thecomputer vision community
 uh through the 70s 80s 90s can be understood aschoices which were forced upon us by
 the fact that we just didn't have access to computeenough compute not enough memory none of


0:31:02
Speaker 1 :hard drives not


0:31:03
Speaker 0 :exactly not enough not enough compute
 not enough storage so so think of these choices so one ofthe choices is
 focusing on single images rather than video okayclear questions storage and compute
 we had to focus on we did we used to detect edges and throw away theimage right so you have an image
 which i say 256 by 256 pixels and instead of keeping around the grayscalevalue what we did was we detected edges
 find the places where the brightness changes a lotso now that and now and then throw away
 the rest so this was a major compression deviceand the hope was that this makes it
 that you can still work with it and the logic was humans can interpret a linedrawing
 and uh and yes and this will save us a competition so many of the choices weredictated by that
 i think uh today we are no longer detecting edges rightwe
 process images with convnets because we don't need to we don't have thatthose compute restrictions anymore now
 video is still under studied because video compute isstill quite challenging
 if you are a university researcher i thinkvideo computing is not so challenging if
 you are at google or facebook or amazon still super

0:32:28
Speaker 1 :challenging i've
 just spoke with the vp of engineering google head ofthe youtube search and discovery and
 they still struggle doing stuff on video it's very difficult except doingexcept using techniques that are
 essentially the techniques you used in in the 90s some very basic computervision techniques


0:32:48
Speaker 0 :no that's when you want to do things at
 scale so if you want to operate at the scale of allthe content of youtube it's very
 challenging and there's similar issues infacebook but as a researcher you
 you have you have more uh you know opportunities

0:33:06
Speaker 1 :you can train large you know that works
 with relatively large

0:33:10
Speaker 0 :uh video data sets yeah yes so i think
 that this is part of the reason why we haveso emphasized static images
 i think that this is changing and over the next few yearsi see a lot more progress happening in
 in video so i have this generic statement thatto me video recognition feels like 10
 years behind object recognition and you can quantifythat because
 you can take some of the challenging video data sets andtheir performance on action
 classification is like say 30 which is kind of what we used to havearound
 2009 in object detection you know so it's like about 10 years behindand uh whether it'll take 10 years to
 catch up is a different question hopefully it will take less than that

0:34:01
Speaker 1 :let me ask a similar question i've
 already asked but once again so for dynamic scenesdo you think do you think some kind of
 injection of knowledge basis and reasoning isrequired
 to help improve like action recognition like if if if umif we solve the general action
 recognition problem what do you think the solution wouldlook like it's another way yeah


0:34:30
Speaker 0 :so i i completely
 agree that knowledge is called for and that knowledge can bequite sophisticated so the way i would
 say it is that perception blends into cognition andcognition brings in
 issues of memory and this notion of a schema from psychologywhich is
 uh let me use the classic example which isyou go to a restaurant right now the
 things that happen in a certain order you walk insomebody takes you to a table
 a waiter comes gives you a menu takes the order food arrives eventuallya
 bill arrives etc etc this is a classic example of ai from the 1970suh it was called there was the term
 frames and scripts and schemas these are all quitesimilar ideas
 okay in the 70s the way the ai of the time dealt with it was bybuild hand coding this
 so they hand coded in this notion of a script and the variousstages and the actors and so on and so
 forth and use that to interpret for examplelanguage
 i mean if there's a description of a of a story involvingsome people eating at a restaurant there
 are way all these inferences you can make because you knowwhat happens typically at a restaurant
 so i think this kind of uh this kind of knowledge is absolutelyessential so i think
 that when we are going to do long-form video understandingwe are going to need to do this i think
 the kinds of technology that we have right now with3d convolutions over a couple of seconds
 of clip or video it's very much tailored towardsshort-term video understanding
 not that long-term understanding long-term understandingrequires a notion of
 this notion of schemas that i talked about perhaps some notions ofgoals intentionality functionality
 and so on and so forth now how will we bring that in so we couldeither revert back to the 70s and say
 okay i'm going to hand code in a script or we mighttry to learn it so i
 tend to believe that we have to find learning ways of doing thisbecause i think learning ways to land up
 being more robust and there must be a learning version ofthe story because
 uh children acquire a lot of this knowledgeby uh sort of just observation so
 at no moment in a child's life there's a it's possible but i think it's not sotypical that
 somebody that a mother coaches a child through all the stages of what happensin a restaurant
 they just go as a family they they they go to the restaurant they eat comeback and the child goes through 10 such
 experiences and the child has has got a schema ofwhat happens when you go to a restaurant
 so we somehow need to we need to provide that capability to our systems

0:37:48
Speaker 1 :you mentioned the following line from
 the end of the alan turing paper uh computing machinery and intelligencethat many people
 like you said many people know and very few have readwhere he proposes the turing test this
 is this is how you know because it's towards the end of the paperinstead of trying to produce a program
 to simulate the adult mind why not rather try to produce one whichsimulates the child's
 so that's a really interesting point if i think about the benchmarks we havebefore us the the tests
 of our computer vision systems they're often kind of trying toget to the adult so what kind of
 benchmarks should we have what kind of tests for computer visiondo you think we should have
 that mimic the child's in computer vision yeah

0:38:38
Speaker 0 :i think we should have those and we
 don't have those today and i think uh the part of thatthe challenge is that we should really
 be collecting data of the type that a child uh that thechild experiences
 right so that gets into issues of you know privacy and so on and so forthbut there are attempts in this direction
 to sort of try to collect the kind of datathat a child
 encounters growing up so what's the child's linguisticenvironment what's the child's visual
 environment so if we could collect that kind of dataand then develop learning schemes based
 on that data that would be one way to do it ii think that's a very promising
 direction myself there might be people who would arguethat we could just short circuit this in
 some way and uh sometimes we haveimitated uh we have not
 we have had success by not imitating nature in detail sothe usual example is airplanes right we
 don't build flapping winds flapping wings so uhyes that's uh that's one of the points
 of debate uh in my mind i i i would i would bet onthis this learning like a child approach


0:40:05
Speaker 1 :so one of the fundamental aspects of
 learning like a child is the interactivityso the child gets to play with the data
 set it's learning from yes it's against the select i mean youcan call that active learning you can
 you know in the machine learning world you can call it a lot of termswhat are your thoughts about this whole
 space of being able to play with the data set or select what you're learning

0:40:29
Speaker 0 :yeah so i think that uh i
 i believe in that and i think that we could achieve it in in two ways and ithink we should use both
 so one is uh actually real robotics right so real uhyou know physical embodiments of agents
 who are interacting with the world and they have a physical body withdynamics and mass and moment of inertia
 and friction and all the rest and you learn your body the robot learns itsbody by
 doing a series of actions the second is that simulationenvironments
 so i think simulation environments are getting much much betterin my in my life in
 facebook ai research our group has worked on something called habitatwhich is a simulation environment
 which is a visually photorealistic environment ofyou know places like houses or interiors
 of various urban spaces and so forth and asyou move
 you get a picture which is a pretty accurate pictureso uh i i can now uh you can imagine
 that subsequent generations of thesesimulators will be accurate not just
 visually but with respect to you know forces and masses andhaptic interactions and so on
 and uh then then we have that environment to play withi think that let me state one reason why
 i think this active being able to act in theworld is
 important i think that this is one way to breakthe correlation versus causation barrier
 so this is something which is of a great deal of interest these days i meanpeople like judea pearl have
 talked a lot about uh why that we are neglecting causality and hedescribes the entire set of successes of
 deep learning as just curve fitting right because it's uh but i i don'tquite agree


0:42:45
Speaker 1 :about as a troublemaker he is but uh


0:42:46
Speaker 0 :causality
 is important but causality is not is not like a single silver bullet it'snot like one single principle there are
 many different aspects here and one of the ways in which uhone of our most reliable ways of
 establishing causal links and this is the wayfor example the the medical community
 does this is randomized control trials so you haveyou
 you pick some situation and now in some situation you perform an action andfor certain others you don't
 right so so you have a control experiment well the child is in factperforming controlled experiments all
 the time right right right okay small scale andin a small scale and
 but but that is a way that the child gets tobuild and refine its causal models of
 the world and my colleague alison gopnik hastogether with a couple of authors
 co-authors has this book called the scientist in the cribreferring to children so i like the part
 that i like about that is the scientist wants to do wants to buildcausal models
 and the scientist does control experiments and i think the child isdoing that
 so to enable that we will need to have these these active experimentsand i think this could be done some in
 the real world and some in simulation so

0:44:14
Speaker 1 :you have hope for simulation
 i have a hopeless solution that's an exciting possibility if we can get tonot just photo realistic but what's that
 called life realistic yeah uh simulationso you don't see any fundamental
 blocks to why we can't eventually simulatethe the principles of what it means to
 exist in the world

0:44:38
Speaker 0 :as a physical i i don't see any
 fundamental problems there i mean and look the computer graphics communityhas come a long way
 right so the in the early days back going back to the 80s and 90s they werethey were focusing on visual realism
 right and then they could do the easy stuff but they couldn't do stuff likehair or fur and so on
 okay well they managed to do that then they couldn't do physicalactions right like there's a bowl of
 glass and it falls down and it shatters but then they could start to do prettyrealistic models of that
 and so on and so forth so the graphics people have shown that they can dothis forward direction not just for
 optical interactions but also for physical interactionsso i think uh of course some of that is
 very computer intensive but i think by and by we will find ways ofmaking our models ever more realistic


0:45:35
Speaker 1 :you break vision apart into in one of
 your presentations early vision static scene understandingdynamics and understanding
 and raise a few interesting questions i thought i could just throw somesome at you just to see if you want to
 talk about them so early vision so it's what is ityou said um sensation
 perception and cognition so is this a sensation yeswhat can we learn from image statistics
 that we don't already know so at the lowest level what umwhat can we make from just this the the
 statistic the basics so there were the variations in the rock pixels thetextures and so on


0:46:18
Speaker 0 :yeah so what we seem to have learned is
 uh uh uh is that there's a lot ofredundancy in these images and
 as a result we are able to do a lot of compressionand and this compression is very
 important in biological settings right so you might have ten to the eightphotoreceptors and only ten to the six
 fibers in the optic nerve so you have to do this compression bya factor of hundreds to one and
 uh and uh so there are analogs of that which are happening inin our neural net artificial neural


0:46:55
Speaker 1 :network that's the early layer so you
 think there's a lot of compression that can bedone in the beginning
 yeah just just the statistics yeah um how much

0:47:07
Speaker 0 :how much well so i mean the the way to
 think about it is just how successful is image compressionright and we we and there are and that's
 been done with older technologies but it can be donewith there are
 several companies which are trying to usesort of these more advanced neural
 network type techniques for compression both for static images as well as forfor video
 one of my former students has a company which is trying to dostuff like this and
 i think i think that they are showing quiteinteresting results and i think that
 that's all the success of that's really about imagestatistics and video statistics but


0:47:53
Speaker 1 :that's still not doing
 compression of the kind when i see a picture of a catall i have to say is it's a cat that's
 another semantic kind of complication

0:48:02
Speaker 0 :yeah so this is this is at the lower
 level right so we are we are we as i said yeahthat's focusing on low level statistics


0:48:10
Speaker 1 :so to linger on that for a little bit
 uh you mentioned how far can bottom-up image segmentation goand in general what you mentioned
 that the central question for scene understanding is the interplay ofbottom-up and top-down information maybe
 this is a good time to elaborate on that maybe define whatis
 what is up what is top down in the comments yes the computer vision

0:48:36
Speaker 0 :uh right that's uh
 so today what we have are a are very interesting systems because they workcompletely bottom up
 how are they what does bottom bottom-up mean sorry so bottom-up means in thiscase means a feed-forward net neural
 network so starting from the raw pixels yeah

0:48:52
Speaker 1 :

0:48:53
Speaker 0 :they start from the raw pixels and they
 they end up with some something like cat or not a catright so our our systems are running
 totally feed forward they're trained in a very top-down wayso they're trained by saying okay this
 is a cat there's a cat there's a dog there's a zebra etcand i'm not happy with either of these
 choices fully we have gone into uh because we havecompletely separated these processes
 right so there is a so i would like the uh the process uhso what do we know compared to biology
 so in biology what we know is that the processesin at test time at run time
 those processes are not purely feed forward but they involve feedbackso and they involve much shallower
 neural networks so the kinds of neural networks we areusing in computer vision say a resnet 50
 has 50 layers well in in the brain in the visualcortex
 going from the retina to it maybe we have like sevenright so they're far shallower but we
 have the possibility of feedback so there are backward connectionsand this might enable us to uh
 to deal with the more ambiguous stimuli for exampleso the the biological solution seems to
 involve feedback the solution in in artificialvision seems to be just feed forward but
 with a much deeper network and the two are functionally equivalentbecause if you have a feedback network
 which just has like three rounds of feedbackyou can just unroll it and make it three
 times the depth and create it in a totally feed forwardway
 so this is something which i mean we have written some papers on thistheme but i really feel that this should
 this theme should be pursued further

0:50:55
Speaker 1 :have some kind of recurrence mechanism
 yeah

0:50:57
Speaker 0 :okay the other uh so that so that's uh
 so i so i want to have a little bit more topdown in the
 at test time okay then at training time we make use of a lot of top-downknowledge right now
 so basically to learn to segment an object we have to have all theseexamples of this is the boundary of a
 cat and this is the boundary of a chair and this is the boundary of a horse andso on and this is
 too much top-down knowledge how do humans do this we manage to we managewith far less supervision
 and we do it in a sort of bottom-up way because for examplewe're looking at a video stream and the
 horse moves and that enables me to say that allthese pixels are together
 yeah so the gestural psychologists used to call thisthe principle of common fate so there
 was a bottom-up process by which we were able to segmentout these objects
 and we have totally focused on this top-down training signalso in my view we have currently solved
 it in machine vision this top-downbottom-up interaction
 but i don't find the solution fully satisfactoryand i would rather have a bit of both in
 at both stages

0:52:20
Speaker 1 :for all computer vision problems which
 is not just segmentation

0:52:24
Speaker 0 :and and and and the question that you
 can ask is so for me i'm inspired a lot by humanvision and i care about that
 you could be a just a hard-boiled engineer not give a damnso to you i would then argue that uh you
 would need far less training data if you could make my uh research agendayou know fruitful okay so


0:52:47
Speaker 1 :maybe taking a step into uh segmentation
 static scene understanding what is the interaction betweensegmentation and recognition
 you mentioned the movement of objects so for people who don't know computervision
 segmentation is this weird activity that wethat computer vision folks have all
 agreed is very important uh of drawing outlines around objectsversus a bounding box or
 and then classifying that object what's what's the value of segmentationwhat is it
 as a problem in computer vision how is it fundamentally different fromdetection recognition any other problems


0:53:31
Speaker 0 :yeah so i think
 uh so so segmentation enables us to say thatsome set of pixels are an object without
 necessarily even being able to name that object or knowing properties of thatobject


0:53:47
Speaker 1 :oh so you mean segmentation purely as
 as as the act of separating an object from its background a blob of uhof that's united in some way from his


0:54:01
Speaker 0 :background yeah so identification if you
 were making an entity out of it and

0:54:05
Speaker 1 :justification yeah beautifully


0:54:07
Speaker 0 :so so i think that we have that
 capability and that is that enables usto uh as we are growing up to
 acquire uh names of objects with very little supervision so supposethe child
 lets posit that the child has this ability to separate outobjects in the world then when the
 there's a the mother says pick up your bottle orthe cat's behaving funny today
 [Laughter] the word cat suggests some object andthen the child sort of does the mapping
 right right the mother doesn't have to teacha specific object labels by pointing to
 them weak supervision works in the contextthat you have
 the ability to create objects so i think that uh so to me that's that's avery fundamental capability
 uh there are applications where this is very important uhfor example medical diagnosis so in
 medical diagnosis uh you have some uh brain scan i meansome
 this is some work that we did in my group where you have ct scans of peoplewho have
 had traumatic brain injury and what uh what the radiologist needs to do is toprecisely delineate various
 places where there might be bleeds for exampleand there's there are clear needs like
 that so they're certainly very practicalapplications of computer vision where
 segmentation is necessary but philosophically segmentationenables the task of recognition
 to proceed with much weaker supervision than we require today

0:55:58
Speaker 1 :and you think of segmentation as this
 kind of task that takes on a visual scene and breaks it apartinto into interesting entities yeah
 that might be useful for whatever the task is yeah

0:56:11
Speaker 0 :and and it is not semantics free so i
 think i i mean it it blends into it involvesperception and cognition it is not it is
 not i i think the mistake that we used tomake in the early days of computer
 vision was to treat it as a purely bottom-upperceptual task it is not just that
 because we do revise our notion of segmentation with more experience rightbecause
 for example there are objects which are non-rigid like animalsor humans and uh i think
 understanding that all the pixels of a human are one entity is actually quite achallenge
 because the parts of the human they can move independentlyand the human wears clothes so they
 might be differently colored so it's all sort of a challenge you

0:57:05
Speaker 1 :mentioned the three hours of computer
 vision are recognition reconstructionreorganization
 can you describe these three r's sure how they interact

0:57:15
Speaker 0 :yeah so uh so recognition is the easiest
 one because that's uh what i thinkpeople generally think of as computer
 vision achieving these days which is uh labelsso is this a cat is this a dog is this a
 chihuahua i mean you know it could be very fine grain likeyou know specific breed of a dog or a
 specific species or bird or it could be very abstract like animal

0:57:46
Speaker 1 :but given a part of an image or a whole
 image say put a label on that yeah so that's

0:57:52
Speaker 0 :that's recognition
 reconstruction is uh essentially it you can think of it asinverse
 graphics i mean that's one way to think about it so graphics is youryou have some internal computer
 representation and uh you have a computerrepresentation of some objects arranged
 in a scene and what you do is you produce a pictureyou produce the pixels corresponding to
 a rendering of that scene so uh so let'sdo the inverse of this we are given an
 image and we try to we we we say oh this imagearises from some objects in a scene
 looked at with a camera from this viewpoint and we might have moreinformation about the objects like their
 shape maybe their textures maybe you know color et cetera et cetera sothat's the reconstruction problem in a
 way that you are in your head creating amodel of the external world
 okay reorganization is to do with essentially finding these entities souh so it's uh organization or
 the word organization implies structure so uh that in in uh perceptionin psychology we use the term perceptual
 organization that uh the the world is not justan image is not just seen as is not
 internally represented as just a collection of pixels but wemake these entities we create these
 entities objects whatever you want to call in therelationship between the entities as


0:59:39
Speaker 1 :well or is it purely about the entities


0:59:42
Speaker 0 :it could be about the relationships but
 mainly we focus on the fact that there are entities

0:59:48
Speaker 1 :sometimes i'm trying to pinpoint what
 the organization means

0:59:53
Speaker 0 :so organization is that instead of like
 a uniform grid we have the structure ofobjects


1:00:02
Speaker 1 :so segmentation is a small part of that


1:00:06
Speaker 0 :so segmentation gets us going towards
 that

1:00:09
Speaker 1 :yeah and you kind of have this triangle
 where they all interact together yes so how do you see that interactionin uh sort of uh
 reorganization is yes defining the entities in the worldthe recognition is labeling those
 entities and then reconstruction is what fillingin the gaps


1:00:33
Speaker 0 :well to for example see
 impute some 3d objects corresponding to each of theseentities that would be part of adding


1:00:45
Speaker 1 :more information that's not
 there in the raw data correct

1:00:48
Speaker 0 :i mean i started pushing this kind of a
 view in the around 2010 or something like thatbecause at that time in computer vision
 the distinction that people were were justworking on many different problems but
 they treated each of them as a separate isolated problem with each with its owndata set and then you try to solve that
 and get good numbers on it so i wasn't i didn't like that approachbecause i wanted to see
 the connection between these and if people divided up vision intointo various modules the way they would
 do it is as low level mid-level and high-level visioncorresponding roughly to the
 psychologist's notion of sensation perception and cognitionand i didn't that didn't map to tasks
 that people cared about okay so therefore i tried to promotethis particular framework
 as a way of considering the problems that people in computer vision wereactually working on
 and trying to be more explicit about the fact that they actuallyare connected to each other and i was at
 that time just doing this on the basis ofinformation flow
 now it turns out in the last five years or soin the post the deep learning revolution
 that this this architecture has turned out to bevery conducive to that
 because basically in these neural networks we are trying tobuild multiple representations
 there can be multiple output heads sharing common representationsso in a certain sense today given the
 reality of what solutions people have to thesei i i i do not need to preach this
 anymore it is it is just there it's part of thesolution space


1:02:53
Speaker 1 :so speaking of neural networks how much
 of this uh problem of computer visionof the organization recognition
 can be um reconstruction how much of it can be learned end to enddo you think
 instead of uh set it and forget it just plug and playhave a giant data set multiple perhaps
 multi-modal and then just learn the entirety of it

1:03:25
Speaker 0 :well so i i think that currently what
 that end-to-end learning means nowadays is end-to-end supervised learningand and that i would argue is too narrow
 a view of the problem i would i like this child developmentview
 this lifelong learning view one where there are certain capabilities that arebuilt up and then there are certain
 capabilities which are built up on top of that so uhthat's that's what i i believe in
 so i think uh end-to-end learning in the supervisedsetting
 for a very precise task to me is a kind of is uhit's sort of a limited view of the of
 the learning process

1:04:18
Speaker 1 :got it so if we think about beyond
 purely supervised look at back to children you mentioned six lessonsthat we can learn from children uh of
 be multimodal be incremental be physical explore be social use language can youspeak to these perhaps picking one
 that you find most fundamental toward yeah time today

1:04:43
Speaker 0 :yeah so i mean i should say to give due
 credit this is from a paper by smith and gasser and it reflectsessentially i would say common wisdom
 among child development people it's just thatthese are this is not common wisdom
 among people in computer vision and ai and machinelearning so
 i view my role as uh trying to

1:05:13
Speaker 1 :bridge the worlds bridge the two worlds


1:05:14
Speaker 0 :so uh so let's take an example of a
 multi-modal i like that so multi-modal canonical example is uha child interacting with uh with an
 object so then the child so the child holds aball and plays with it
 so at that point it's getting a touch signalso the touch signal is
 is getting as the notion of 3d shape but it is sparseand then the child is also seeing a
 visual signal right and and these two so imagine these aretwo in totally different spaces
 right so one is the space of receptors on the skinof the fingers and the thumb and the
 palm right and then these map on to theseneuronal fibers are
 getting activated somewhere right these lead to some activation in somatosensorycortex
 i mean a similar thing will happen if we have a robothand okay and then we have the pixels
 corresponding to the visual view but we know that theycorrespond to the same
 object right so that's a very very strong cross calibrationsignal
 and it is self-supervisory which is beautiful rightthere's nobody assigning a label the
 mother doesn't have to come and assign a label the childdoesn't even have to
 know that this object is called a ball okay but the obj the child is learningsomething about the three-dimensional
 world from this signal uhi think tactile and visual there is some
 work on there is a lot of work currently onaudio and visual
 okay an audio visual so there is some event that happens in the worldand that event has a visual signature
 and it has a auditory signature so there is thisglass bowl on the table and it falls and
 breaks and i hear the smashing sound and i see the pieces ofglass
 okay i've built that connection between the tworight we have people uh i mean this has
 become a hot topic in computer vision in the last couple of yearsthere is there are problems like uh
 separating out multiple speakers right which was a classic problem in inaudition they call this the problem of
 source separation or the cocktail party effect and so on but justtry to do it visually
 when you also have it becomes so much easier and so much more useful

1:07:50
Speaker 1 :so the the multimodal i mean there's so
 much more signal with multimodal and you can usethat
 for some kind of weak supervision as well yesbecause they are occurring at the same


1:08:01
Speaker 0 :time in time yeah so you have time
 which links the two right so at a certain moment t1you've got a certain signal in the
 auditory domain and a certain signal in the visual domainbut they must be causally related yeah


1:08:15
Speaker 1 :it's an exciting area not well studied
 yet not yeah i mean we have a little bit of

1:08:19
Speaker 0 :work at this but uh but
 but so much more needs to be done yeah so so soso this this is this is a good example
 be physical that's to do with uh like the one thingwe talked about
 earlier that that there's a embodied world

1:08:36
Speaker 1 :to mention language use language so
 no chomsky believes that language may be at the core of cognition at the core ofeverything in the human mind
 what is the connection between language and vision to youlike what's more fundamental are they
 neighbors is one the parent and the child thechicken and the egg


1:08:58
Speaker 0 :oh it's very clear it is vision which is
 the appearance the fundament the permission is the fundamentalability okay well so
 uh it comes before you think vision is

1:09:08
Speaker 1 :more fundamental than language


1:09:12
Speaker 0 :correct and and and it and yeah
 you can think of it either in phylogeny or in ontogenyso phylogeny means if you look at
 evolutionary time right so you we have vision thatdeveloped 500 million years ago
 okay then something like when we get to maybe likefive million years ago you have the
 first bipedal primate so when we started towalk then the hands became free and so
 then manipulation the ability to manipulateobjects and build tools and
 so on and so forth so you said 500 000

1:09:46
Speaker 1 :years ago no no sorry


1:09:47
Speaker 0 :the the first multicellular animals
 which you can say had some intelligence arose 500 millionyears ago
 okay and now let's fast forward to say the lastseven million years which is the
 development of the hominid line right where from the other primates we havethe branch which leads on to modern
 humans now there are many of these hominidsbut the the ones which
 you know people talk about lucy because that's like a skeleton from threemillion years ago and we know that lucy
 walked okay so at this stage you have that the hand is free for manipulatingobjects
 and then the ability to manipulate objects buildtools and the brain size
 grew in this era so okay so now you have manipulationnow we don't know exactly when language
 arrows

1:10:50
Speaker 1 :but after that but after that because no


1:10:51
Speaker 0 :apes have i mean so i mean chomsky is
 correct in that that it is a uniquely human capabilityand we primates
 other primaries don't have that but so it developed somewhere in this erabut it developed i would
 i mean uh argue that it probably developed after we had this stage ofuh uh humans or i mean the
 human species already able to manipulate and a hands-free much bigger brain size

1:11:26
Speaker 1 :and for that there's a lot of vision
 has already had had to have developed yeah sothe sensation and the perception may be
 some of the cognition

1:11:37
Speaker 0 :yeah so we we so those
 so so that so the world so there so so these ancestors of usyou know three four million years ago
 they had uh they had spatial intelligence so theyknew that the world consists of objects
 they knew that the objects were in certain relationships to each otherthey had observed causal
 interactions among objects they could move in space so they had space and timeand all
 of that so language builds on that substrate so language hasa lot of
 i mean i mean the all human languages have constructs which depend ona notion of space and time where did
 that notion of space and time come from it had to come from perception andaction in the world we live in


1:12:31
Speaker 1 :yeah what you refer to as the spatial
 intelligence yeah yeah to linger a little bit we mentionedtouring and his uh mention of
 we should learn from children nevertheless language isthe fundamental piece of the test of
 intelligence that touring proposed what do you think is a good test ofintelligence are you
 what would impress the heck out of you is it fundamentallynatural language or is there something
 in vision

1:13:03
Speaker 0 :i i think uh i i wouldn't i
 i don't think we should have created a single test of intelligenceso just like i don't believe in iq as a
 single number i think generally there can be manycapabilities
 which are correlated perhaps so i think that there will beuh there will be accomplishments which
 are visual accomplishments accomplishments which areuh accomplishments in manipulation or
 robotics and then accomplishments in languagei do believe that language will be the
 hardest not to crack really yeah so what's what's harder to

1:13:41
Speaker 1 :pass
 the spirit of the touring test or like whatever formulation will make itnatural language convincingly in natural
 language like somebody you would want to have abeer with hang out and have a chat with
 or the general natural scene understandingyou think language is the type i think


1:14:01
Speaker 0 :i'm not a fan of the
 i think i think turing test that turing as he proposed the test in 1950was trying to solve a certain problem
 yeah imitation yeah and and i think it made a lot ofsense then
 where we are today 70 years later i think i think wewe should not worry about that i mean i
 think the turing test is no longer the right way to uh toto channel research in in ai because
 that it takes us down this path of this chat bot which can fool us for fiveminutes or whatever
 okay i think i would rather have a list of 10 different tasks i mean i thinktheir tasks which their tasks in the
 manipulation domain tasks and navigation tasks and visual scene understandingtasks in under reading a story and
 answering questions based on that i mean so my favoritelanguage understanding task would be
 you know reading a novel and being able to answer arbitrary questions from itokay right i i think that to me
 uh and this is not an exhausted list by any meansso i would uh i think that that's what
 we where we need to be going to and each ofthese
 on each of these axes there's a fair amount of work to be done

1:15:26
Speaker 1 :so on the visual understanding side in
 this intelligence olympics that we've set up yeah what's a goodtest for one of many
 of visual scene understanding uh do you think such benchmarks existsorry to interrupt no there


1:15:42
Speaker 0 :there aren't any i i think i think
 essentially to me a really uh goodaid to the blind so suppose there was a
 blind person and i needed to assist the blind person

1:15:57
Speaker 1 :so ultimately like we said vision that
 aids in the action in the survival in this world yeahmaybe in a simulated world


1:16:08
Speaker 0 :maybe easier to to measure performance
 in a simulated world what we are ultimately after isperformance in the real world


1:16:18
Speaker 1 :so david hilbert in 1900 proposed 23
 open problems in mathematics some of which are still unsolvedmost important famous of which is
 probably the riemann hypothesis you've thought about and presented aboutthe hilbert problems of computer vision
 so let me ask what to you today i don't know when the last year youpresented that 2015 but versions of it
 yeah you're kind of the the face and the spokesperson for computer visionyeah it's your job to just to state what
 the problem the open problems are for the field sowhat today
 are the hilbert problems of computer vision do you think

1:16:56
Speaker 0 :let me pick pick one to which i regard
 as uh clearly clearly unsolvedwhich is what i would call long-form
 video understanding so so we have a video clip and we wantto
 understand the behavior in there in terms ofagents their goals
 intentionality and uh make predictions about what might happenyou know so so that that kind of
 understanding which goes away from atomic visual action soso in the short range the question is
 are you sitting are you standing are you catching a ballright that we can do now or we even if
 we can't do it fully accurately if we can do it at 50 percent maybe nextyear we'll do it at 65 and so forth
 but i think the long range video understandingi don't think we we we can do today well


1:18:02
Speaker 1 :today and that means so long and it


1:18:03
Speaker 0 :blends into cognition that's the reason
 why it's challenging

1:18:07
Speaker 1 :and so you have to track you have to
 understand the entities you have to understand the sds you haveto track them
 and you have to have some kind of model of their behaviorcorrect and their and if their behavior


1:18:18
Speaker 0 :might be
 these are these are agents so they are not just like passiveobjects but the agent so therefore we
 they might they would exhibit gold directed behaviorokay so this is this is one area then i
 will talk about say understanding the world in 3d nowthis may seem
 paradoxical because in a way we have been able to do 3d understanding evenlike
 30 years ago right but i don't think we currently have the richness of3d understanding in our computer vision
 system that we would like because ah so let me elaborate on that abit
 so currently we have two kinds of techniques which arenot fully unified so there are the kinds
 of techniques from multi-view geometry that you havemultiple pictures of a scene and you do
 a reconstruction using stereoscopic visionor structure from motion
 but these techniques do not they totally fail if you just have asingle view because they are relying
 on this this multiple geometry okay then we have some techniques thatwe have developed in the computer vision
 community which try to guess 3d from single views and thesetechniques are based
 on on supervised learning and they are based on having a trainingtime
 3d models of objects available and this is completely unnaturalsupervision
 right that's not cad models are not injected into your brainokay so what would i like what i would
 like would be a kind of uh learning as youmove around the world uh notion of 3d
 so so we we have our succession of visual experiencesand from those we
 so in as part of that i might see a chair from different viewpointsor a table from viewpoint different
 viewpoints and so on now as part that enables me to buildsome internal representation and then
 next time i just see a single photograph and it may not evenbe of that chair it's of some other
 chair and i have a guess of what its 3d shapeis like


1:20:43
Speaker 1 :so you're almost learning the cad model
 kind of

1:20:46
Speaker 0 :yeah implicitly i mean implicitly i mean
 the cad model need not be in the same form asused by computer graphics hidden in the


1:20:53
Speaker 1 :representation
 it's hidden in the representation the

1:20:54
Speaker 0 :ability to predict new views
 and what i would see if i went to such and such position by the

1:21:04
Speaker 1 :way and
 on a small tangent on that are you uncomforta are youokay or comfortable with
 neural networks that do achieve visual understanding that do for exampleachieve this kind of 3d understanding
 and you don't know how they you don't knowthe rep you're not able to interest but
 you're not able to visualize or understand or interact withthe representation
 so the fact that they're not or may not be explainable

1:21:36
Speaker 0 :yeah i think that's fine i to me that is
 uh so so let me put some caveats on thatso it depends on the setting so first of
 all i think uh uh theuh humans are not explainable
 so yeah that's a really good point yeah so we weone human to another human is not fully
 explainable i think there are settings whereexplainability matters
 and these might these are these might be for example questions on medicaldiagnosis
 so i'm in a setting where maybe the doctor maybe a computerprogram has made a certain diagnosis
 and then depending on the diagnosis perhaps i should have treatment day ortreatment b
 right so now is the computer programs diagnosis based on datawhich was data collected of
 for american males who are in their 30s and 40sand maybe not so relevant to me
 maybe it is relevant you know et cetera et cetera and we i mean inmedical diagnosis we have major issues
 to do with the reference class so we may have acquired statistics fromone group of people and applying it to
 a different group of people who may not share all the same characteristicsthe data might have there might be error
 bars in the prediction so that prediction should really betaken with
 a huge grain of salt and but this has an impact on what treatmentsshould be picked right so
 so there are settings where i want to know more than justthis is the answer but what i
 acknowledge is that so so so so i in that senseexplainability and interpretability may
 matter it's about giving error bounds and abetter sense of the quality of the
 decision where what i where i'm willing tosacrifice interpretability is that
 i believe that there can be systems which can be highly performant but whichare internally
 black boxes and and that seems to be

1:23:57
Speaker 1 :words headed some of the best performing
 systems are essentially black boxes yeah uhfundamentally by their construction you


1:24:04
Speaker 0 :and i are
 black boxes to each other yeah so the

1:24:07
Speaker 1 :nice thing about the black boxes we are
 is so we ourselves are black boxes but we're also those of us who arecharming
 are able to convince others like explain the blackwhat's going on inside the black box
 with narratives with stories so in some sense uh neural networksdon't have to actually
 explain what's going on inside they just have to come up with stories real orfake
 that convince you that they know what's going onand i'm sure we can do that we can


1:24:39
Speaker 0 :create those nearer
 those stories neural networks can create those stories yeah

1:24:47
Speaker 1 :and the transformer will be involved do
 you think we will ever build a system of human level orsuperhuman level intelligence
 we've kind of defined what it takes to try to approach that but do you thinkwe'll
 do you think that's within our reach the thing that we thought we could dowhat touring thought actually we could
 do by a year 2000 right what do you think we'll ever beable to do so


1:25:11
Speaker 0 :i think there are two answers here one
 question one answer is in principle can we do this at some timeand my answer is yes the second
 answer is a pragmatic one do you think we will be able to do it in the next 20years
 or whatever and to that man says no so and of course that's a wild guess i ii i think that
 you know donald trump's felt is not a favorite person of mine butone of his lines is very good which is
 about known knowns known unknowns and unknownunknowns
 so in the business we are in there are known unknowns and we haveunknown unknowns
 so i think with respect to a lot of what the case invision and robotics i feel like
 we have known unknowns so i have a sense of where we need to goand what the problems that need to be
 solved are i feel with respect to natural languageunderstanding and high level cognition
 it's not just known unknowns but also unknown unknownsso it is very difficult to put any kind
 of uh time frame to that uh do you think some

1:26:30
Speaker 1 :of the
 unknown unknowns might be positive in that they'll surprise us and make thejob much easier
 so fundamental breakthroughs i think

1:26:40
Speaker 0 :that is possible because certainly i
 have been very positively surprised by howeffective these deep learning systems
 have been because i certainly would not have believed thatin
 2010 i think what we knew from the mathematicaltheory
 was that convex optimization works when there's a single global optima thenthese gradient descent techniques would
 work now these are non-linear systems with non-convexsystems
 huge number of variables so

1:27:17
Speaker 1 :over-parametrized over-parameterized


1:27:18
Speaker 0 :and the people who used to play with
 them a lot the ones who are totally immersed in thelore and the
 black magic they knew that they worked uh well even though they were really

1:27:35
Speaker 1 :i thought like everybody no the claim


1:27:35
Speaker 0 :that
 i hear from my friends like yan lacoon and so forthnow yeah that they feel that they were
 comfortable with them well he says but the community as awhole
 was certainly not and i think uh we were to me that was the surprise thatthey actually worked robustly
 for a wide range of problems from a wide range of initializations and so onand uh so that was that that was
 certainly more rapid progress than uh we expectedbut then there are certainly lots of
 times in fact most of the history and fear is when wehave made less pro
 progress at a slower rate than we expectedso uh we just keep going
 i think uh what i regard as uh really unwarranted are thesethese fears of uh you know agi in 10
 years and 20 years and that kind of stuff because that's basedon completely unrealistic models of how
 rapidly we will make progress in this

1:28:49
Speaker 1 :field so i agree with you but i've also
 gotten a chance to interact with very smart people who really worry about theexistential threats of ai
 and i as an open-minded person and sort of takingand taking it in do you think
 if ai systems in some way the unknown unknownsnot super intelligent ai but in ways we
 don't quite understand uh the nature of superintelligence willhave a detrimental effect on society
 do you think this is something we should be worried aboutor we need to first allow the unknown
 our nose to become known unknowns i think we need to be

1:29:29
Speaker 0 :worried about ai today
 i think that it is not just a worry we need to have when we get thatagi i think that ai is being used in
 many systems today and there might be settings for examplewhen it causes
 biases or decisions which could be harmful i mean decisions which couldbe unfair to some people
 or it could be a self-driving cars which kills a pedestrianso ai systems are being deployed today
 right and they're being deployed in many different settings maybe in medicaldiagnosis maybe in a self-driving car
 maybe in selecting applicants for an interviewso
 i would argue that when these systems make mistakes there are consequencesand we are in a certain sense
 responsible for those consequences so i would argue that this is acontinuous effort
 it is we and and this is something that in a way is not so surprising it's aboutall
 engineering and scientific progress which uhgreat power comes great responsibility
 so as these systems are deployed we have to worry about them andit's a continuous problem i don't think
 of it as something which will suddenly happen on some dayin 2079
 for which i need to design some clever tricki'm saying that these problems exist
 today yeah and we need to be continuously on thelookout for
 worrying about safety biases risks right i mean the self-driving car killsare pedestrian
 and they have right i mean the this uber incident in arizona yeah right it hashappened
 right this is not about agi it in fact it's about a very dumb intelligencewhich is also killing people the worry
 people have with agi

1:31:25
Speaker 1 :is the scale and i but i think you're
 100 right is like the thing that worries meabout ai
 today and it's happening in a huge skills recommendrecommender systems recommendation
 systems so if you look at twitter or facebook or youtube theircontrolling the ideas that we have
 access to the news and so on and that's afundamentally machine learning algorithm
 behind each of these recommendations and they i mean my life would not be thesame without
 these sources of information i'm a totally new human being andthe ideas that i know are very much
 because of the internet because of the algorithm that irecommend those ideas and so
 as they get smarter and smarter i mean that is the agiyeah is that's the the algorithm that's
 recommending the next youtube video you should watchhas control of millions of billions of
 people that that algorithm is already superintelligent and
 has complete control of the population not a complete butvery strong control for now we can turn
 off youtube we can just go have a normal life outside of thatbut the more and more that
 gets into our life it's that algorithm we startdepending on it in the different
 companies that are working on the algorithm so i think it'syou're right it's already it's already
 there and youtube in particular is usingcomputer vision
 doing their hardest to try to understand the content of videos so they couldbe able to connect videos with the
 people who would benefit from those videos the most and so that developmentcould go in a bunch of different
 directions some of which might be harmfulso yeah you're right the the the threats
 of ai are here already we should be thinking about themon a philosophical notion
 if you could personal perhaps if you could relive a moment in yourlife outside of family
 because it made you truly happy or was a profound moment that impacted thedirection of your life
 what would you go to

1:33:41
Speaker 0 :i don't think of single moments but i
 look over the long haul i feel that i've been very lucky becausei feel that i think that in
 scientific research a lot of it is about being at the right place at the righttime
 and you can you can work on problems at a time whenthey're just too premature you know you
 butt your head against them and and nothing happensbecause it's
 the prerequisites for success are not there and then there are times when youare in a field which is all
 pretty mature and you can only solve curricules upon colloquius i'vebeen lucky to have been in this field
 which for 34 years 35 well actually 34 yearsas a professor at berkeley so
 longer than that uh which when i started in it was justlike some little crazy absolutely
 useless field which couldn't really do anythingto a time when it's really really
 solving a lot of practical problems has a lothas offered a lot of tools for
 scientific research right because computer vision isimpactful for
 images in biology or astronomy and and so on and so forthand we have so we have made great
 scientific progress which has had real practical impact in the world and ifeel lucky that
 i i got in at a time when the field was very young and at a time when it isit's now mature but not fully mature
 it's mature but not done i mean it's really in still in a ina productive phase yes
 yeah yeah i think people 500 years from

1:35:41
Speaker 1 :now would laugh are you calling this
 field mature

1:35:45
Speaker 0 :yeah that is very possible yeah so but
 you're also

1:35:48
Speaker 1 :lest i forget to mention you've also
 mentored some of the biggest names of computervision computer science and ai
 today uh there's so many questions i could ask but really iswhat what is it how did you do it what
 does it take to be a good mentor what does it take to be agood guide


1:36:10
Speaker 0 :yeah i i think what i feel i've been
 lucky to have had very very smart and hardworking andcreative students i think
 some part of the credit just belongs to being at berkeleyi think those of us who are at top
 universities are blessed because we havevery very smart and capable students
 coming on knocking on our door so so i have to behumble enough to acknowledge that
 but what have i added i think i have added somethingwhat i have added is uh i think
 what i've always tried to teach them is a sense of picking the right problemsso i think that in science in the short
 run success is always based on technicalcompetence
 your you know you're quick with math or you arewhatever i mean there's certain
 technical capabilities which make for short-rangeprogress long-range progress is really
 determined by asking the right questions andfocusing on the right problems
 and i feel that what i've been able to bring to thetable in terms of
 advising these students is some sense of taste of what are goodproblems
 what are problems that are worth attacking now as opposed to waiting10 years what's a good problem if you


1:37:42
Speaker 1 :could summarize
 if is that possible to even summarize like what what's your sense of a goodproblem


1:37:49
Speaker 0 :i i think uh i think uh i have a sense
 of what is a good problem which is uh there is a british scientist uhin fact he won a nobel prize peter
 medover who has a a book on on this and uh basically hecalls
 it the research is the art of the solubleso we need to sort of find problems
 which are which are not yet solved but which areapproachable
 and he sort of refers to this sense that there is this problem whichisn't quite solved yet but it has a soft
 underbelly there is some place where you can youknow
 spear the beast yes and having that intuition that this problem is ripe isis a good thing because otherwise you
 can just beat your head and not make progressso i think that is that is important so
 if if i have that and if i can convey thatto students
 it's not just that they do great research while they're working with mebut that they continue to do great
 research so in a sense i'm proud of my studentsand their achievements and their great
 research even 20 years after they've seized being mystudent
 so it's in part developing helping them

1:39:06
Speaker 1 :develop that sense that a problem
 is not yet solved but it's solvable correct

1:39:13
Speaker 0 :the other thing which i have which i i
 think i bring to the table uh is i is a certainintellectual breadth i i've
 spent a fair amount of time studying psychologyneuroscience relevant areas of applied
 math and so forth so i can probably help them see someconnections
 to disparate things which they might not have otherwise soso the smart students coming into
 berkeley can be very uh deep in the sense they can thinkvery deeply meaning very
 hard down one particular path but where i could help them is the theshallow breadth
 but uh whereas they would have the the narrow depth and uh butthat's that's of some value well it was


1:40:09
Speaker 1 :beautifully refreshing just to hear you
 naturally jump to psychology back to computer science and this conversationback and forth
 i mean that that's uh that's actually a rare quality and i think it'scertainly for students empowering to
 think about problems in a new way so for that and for many other reasons ireally enjoyed this conversation thank
 you so much it was a huge honor thanks for talking today

1:40:32
Speaker 0 :it's been my pleasure thanks for


1:40:34
Speaker 1 :listening to this conversation
 with jitendra malik and thank you to our sponsorsbetterhelp and expressvpn
 please consider supporting this podcast by going to betterhelp.comlex and signing up at expressvpn.com
 lexpod click the links buy the stuff it's how they know i sent you and itreally is the best way to support this
 podcast and the journey i'm on if you enjoy thisthing
 subscribe on youtube review 5 stars on apple podcastsupport it on patreon or connect with me
 on twitter at lex friedman don't ask me how tospell that i don't remember
 myself and now let me leave you with some words from prince mishkinand the idiot by dostoyevsky beauty
 will save the world thank you for listening

