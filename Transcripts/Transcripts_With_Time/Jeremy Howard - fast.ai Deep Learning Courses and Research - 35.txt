0:00:00
Speaker 0 :the following is a conversation with
 Jeremy Howard he's the founder of fast AI a Research Institute dedicated to

0:00:06
Speaker 1 :making deep learning more accessible


0:00:10
Speaker 0 :he's also a distinguished research


0:00:11
Speaker 1 :scientist at the University of San


0:00:13
Speaker 0 :

0:00:14
Speaker 1 :Francisco a former president of Kegel as


0:00:15
Speaker 0 :

0:00:16
Speaker 1 :well as the top ranking competitor there


0:00:17
Speaker 0 :

0:00:19
Speaker 1 :and in general he's a successful


0:00:21
Speaker 0 :entrepreneur educator researcher and an
 inspiring personality in the AI

0:00:26
Speaker 1 :community when someone asked me how do I


0:00:29
Speaker 0 :get started with deep learning fast AI


0:00:30
Speaker 1 :is one of the top places that point them


0:00:32
Speaker 0 :to it's free it's easy to get started


0:00:34
Speaker 1 :

0:00:35
Speaker 0 :it's insightful and accessible and if I
 may say so it has very little BS they can sometimes dilute the value ofeducational content on popular topics
 like deep learning fast AI has a focus on practical application of deep

0:00:49
Speaker 1 :learning and hands-on exploration of the


0:00:51
Speaker 0 :cutting edge that is incredibly both
 accessible to beginners and useful to

0:00:55
Speaker 1 :

0:00:56
Speaker 0 :experts this is the artificial


0:00:59
Speaker 1 :

0:01:00
Speaker 0 :intelligence podcast if you enjoy it


0:01:01
Speaker 1 :

0:01:02
Speaker 0 :subscribe on YouTube give it five stars


0:01:03
Speaker 1 :and iTunes supported on patreon or
 simply connect with me on Twitter Alex

0:01:09
Speaker 0 :Friedman spelled Fri D ma N and now


0:01:13
Speaker 1 :

0:01:14
Speaker 0 :here's my conversation with Jeremy


0:01:17
Speaker 1 :Howard what's the first program you've


0:01:20
Speaker 0 :ever ridden this program I wrote that I
 remember would be at high school I did an assignment where I decided to try tofind out if there were sand like better
 musical scales and the normal twelve tone twelve interval scale so I wrote aprogram on my Commodore 64 in basic
 let's search through other scale sizes to see if you could find one where theywere more accurate you know harmonies


0:01:51
Speaker 1 :

0:01:52
Speaker 0 :like mid tone like sliding like he won
 an actual exactly 3 to 2 ratio where else with a 12 interval scale it's notexactly 3 to 2 for example so that's in
 the car well tempered as I say you know

0:02:04
Speaker 1 :and basic on a Commodore 64 yeah where
 was the interest in music from or is it

0:02:10
Speaker 0 :just I took music all my life so I
 played the phone and clarinet and piano andguitar and drums and whatever so how


0:02:17
Speaker 1 :does that threat go through your life


0:02:23
Speaker 0 :where's music today yeah it's not where
 I wish it was I for various reasons couldn't really keep it goingparticularly because I had a lot of
 problems with RSI with my fingers and so I had to kind of like cut back anythingthat used hands and fingers I hope one
 day I'll be able to get back to it

0:02:43
Speaker 1 :health-wise so there's a love for music
 underlying it all yeah what's your

0:02:48
Speaker 0 :favorite instrument sex the phone sex
 baritone saxophone well probably bass saxophone but they're awkward well I'm I

0:02:56
Speaker 1 :always love it when music is coupled
 with programming there's something about a brain that utilizes those that emergeswith creative ideas so you've used and
 studied quite a few programming languages can you given an overview ofwhat you've used one of the pros and


0:03:16
Speaker 0 :cons of each well my favorite
 programming environment almost certainly was Microsoft Access back in like theearliest days so that was Visual Basic
 for applications which is not a good programming language for the programmingenvironment fantastic it's like the
 ability to create you know user interfaces and tie data and actions tothem and create reports and all that as
 I've never seen anything as good there's things nowadays like air table whichyou're like small subsets of that which
 people love for good reason but unfortunately nobody's ever achieved

0:04:01
Speaker 1 :anything like that what is that if you
 could pause in there for a second no access this is it a database database

0:04:05
Speaker 0 :program that Microsoft produced part of
 office and the kind of wizard you know but basically it lets you in a totallygraphical way create tables and
 relationships and queries and tie them to forms and set up you know eventhandlers and calculations and it was
 very plate powerful system designed for notmassive scalable things but fair like
 useful little applications that I loved

0:04:36
Speaker 1 :so what's the connection between excel
 and access so very close

0:04:40
Speaker 0 :so access kind of was the relational
 database equivalent if you like so people still do a lot of that stuff itshould be an access in Excel excels they
 don't know what Excel is great as well so but it's just not as rich aprogramming model as VBA combined with a
 relational database and so I've always loved relational databases but todayprogramming on top of a relational
 database is just a lot more of a headache you know you generally eitherneed to kind of you know you need
 something that connects that that runs some kind of database server unless youuse circle light which has its own
 issues then you can often if you want to get anice programming model you'll need to
 like create and add an ORM on top and then I don't know there's all thesepieces tie together and it's just a lot
 more awkward than it should be there are people that are trying to make it easierso in particular I think of if sharp you
 know Don Syme who him and his team have done a great job of making somethinglike a database appear in the type
 system so you actually get like tab completion for fields and tables andstuff like that anyway so that was kind
 of anyway so like that whole VBA office thing I guess was a starting point whichI still miss I got into standard Visual


0:06:07
Speaker 1 :Basic that's interesting just to pause
 on them for a second it's interesting that you're connecting programminglanguages to the ease of management of
 data yeah so in your use of programming languages you always had a love and a

0:06:23
Speaker 0 :connection with data I've always been
 interested in doing useful things for myself and for others which generallymeans getting some data and doing
 something with it and putting it out there again so that's been my interestthroughout so I also did a lot of stuff
 with Apple script back in the early days so it's kind of nice being able to getthe computer and computers to talk to
 each other and to do things for you and then I could think that one night theprogramming language I most loved then
 would have been Delphi which was object pascal created by under sales berg whopreviously did to it by pascal and then
 went on to create dotnet and then went on create typescript delphi was amazingbecause it was like a compiled fast
 language that was as easy to use as

0:07:19
Speaker 1 :Visual Basic Delphi what is it similar
 to in in more modern languages Visual

0:07:26
Speaker 0 :Basic Visual Basic yeah that a compiled
 fast version so I'm not sure there's anything quite like it anymoreif you took like C
 shop or Java and got rid of the virtual machine and replaced it with somethingyou could compile a small type binary I
 feel like it's where um Swift could get to with the new Swift UI and thecross-platform development going on like
 that's one of my dreams is that will hopefully get back to where Delphi wasthere is actually a free Pascal project
 nowadays called Lazarus which is also attempting to kind of recreate Delphithough they're making good progress so


0:08:15
Speaker 1 :ok Delphi that's one of your favorite
 programming languages programming

0:08:20
Speaker 0 :environments again I hate Pascal's not a
 nice language if you wanted to know specifically about what languages I likethey would definitely pick J there's
 being an amazingly wonderful language

0:08:34
Speaker 1 :

0:08:35
Speaker 0 :well woods j.j are you aware of APL I am


0:08:39
Speaker 1 :NOT okay so from doing a little research


0:08:43
Speaker 0 :on work you've done okay so not at all
 surprising you're not familiar with it cuz it's not well known but it'sactually one of the main families of
 programming languages going back to the late 50s early 60s so there was a coupleof major directions one was the kind of
 lambda calculus Alonzo Church direction which I guess kind of listensgame and whatever which has a history
 going back to the early days of computing the second was the kind ofimperative /o o you know algo Simula
 going under C C++ so forth there was a third which Accord array orientedlanguages which started with a paper by
 a guy called Ken Iverson which was actually a math theory paper not aprogramming paper it was called notation
 as a tool for thought and it was the development of a new way a new type ofmath notation and the idea is that this
 math notation would be was was much more flexible expressiveand also well-defined then traditional
 math notation which is none of those things math notation is awful and so heactually turned that into a programming
 language and because this was the early 50s although that's very late 50salthough names were available so he
 called his language a programming language or APL ABL APL is aimplementation of notation as a tool for
 thought by which he means math notation and Ken and his son went on to do manythings but eventually they actually
 produced you know a new language that was built on top of all the learnings ofAPL that was called J and J is the most
 expressive composable language of you know beautifully designed language I'veever seen this didn't have


0:10:42
Speaker 1 :object-oriented components deserve that


0:10:45
Speaker 0 :kind of thing there's not really it's an
 array oriented language it's a new it's a it's an it's it's the third half using

0:10:50
Speaker 1 :

0:10:52
Speaker 0 :array array oriented yes so I need to be


0:10:54
Speaker 1 :

0:10:55
Speaker 0 :a ray warrior so arranged it means that
 you generally don't use any loops but the whole thing is done with kind of aextreme version of broadcasting if
 you're familiar with that none got an umpire slash Python concept so you do alot with one line of code it looks a lot
 like math notation basically I'll compactmm-hm and the idea is that you can kind
 of because you can do so much with one line of code a single screen of code isvery unlikely to you very rarely need
 more than that to in the rest your program and so you can kind of keep itall in your head and you can kind of
 clearly communicate it it's interesting that the APL created two main branches kand j j is this kind of like open source
 niche community of crazy enthusiasts like me and then the other path k wasfascinating it's an astonishingly
 expensive programming language which many of the world's most ludicrous arich hedge funds use so the entire
 machine is so small it sits inside level 3 cache on your CPU and and it easilywins every benchmark I've ever seen in
 terms of data processing speed hey you don't come across it very muchbecause it's like $100,000 per CPU to to
 run it yeah but it's like this this this this path of programming languages it'sjust so much that are not so much more
 powerful in every way than the ones that almost anybody uses every day

0:12:33
Speaker 1 :so though it's all about computation
 it's really focused pretty heavily

0:12:38
Speaker 0 :focused on computation I mean so much of
 programming is data processing by definition and so there's a lot ofthings you can do with it but yeah
 there's not much work being done on making like use user interface talkingus or whatever I mean this some but it's


0:12:58
Speaker 1 :they're not great at the same time
 you've done a lot of stuff with Perl and Python yeah so where does that fit intothe picture of J and K and APO and well


0:13:08
Speaker 0 :you know it's much more pragmatic like
 in the end you kind of have to end up where the where the libraries are youknow like because to me my my focus is
 on productivity I just want to get stuff done and solve problems so Perl wasgreat for I created an email company
 called fast mail and Perl was great cuz back in the late 90s early 2000s it justhad a lot of stuff it could do I still
 had to write my own monitoring system and my own web framework my own whateverbecause like none of that stuff existed
 but it was the super flexible language

0:13:49
Speaker 1 :to do that in and you used Perl
 fast ball used as a back-end think so

0:13:55
Speaker 0 :everything was written in Perl yeah yeah
 everything everything was fell why do

0:13:58
Speaker 1 :you think Perl hasn't succeeded or
 hasn't dominated the market where Python really takes over a lot yeah well I mean

0:14:07
Speaker 0 :it felt did dominate it was for time
 everything everywhere but then the guy thatPal Larry will kind of just didn't put
 the time in anymore and no project can be successful ifthere isn't you know it's particularly
 one that's data with a strong leader that that loses that strong leadershipso then python is kind of replaced - you
 know python is a lot less elegant language in nearly every way but it hasthe data science libraries and a lot of
 them are pretty great so I kind of use it because it's the best we have butit's definitely not good enough what do


0:15:01
Speaker 1 :you think the future programming looks
 like what do you hope the future programming looks like if we zoom in onthe computational fields on data science


0:15:11
Speaker 0 :on machine learning I hope Swift is
 successful because the goal is Swift the way Chris Lattner describes it is to beinfinitely hackable and that's what I
 want I want something where me and the people I do research with and mystudents can look at and change
 everything from top to bottom there's nothing mysterious and magical andinaccessible unfortunately with Python
 it's the opposite of that because pythons so slow it's extremelyunhackable you get to a point where it's
 like okay from here on down at sea so your debugger doesn't works in the sameway your profiler doesn't work in the
 same way your build system doesn't work in the same way it's really not veryhappy ball at all what's the part you


0:15:53
Speaker 1 :would like to be hackable is it for the
 objective of optimizing training of neural networks inference in yournetworks is it performance of the system
 or is there some non performance related

0:16:08
Speaker 0 :just it's it's a greater thing I'm in
 the end I want to be productive as a practitioner so that means that so likeat the moment our understanding of deep
 learning is incredibly primitive there's very little we understand most thingsdon't work very well even though it
 works better than anything else out there there's so many opportunities tomake it
 so you look at any domain area like I don't know speech recognition with deeplearning or natural language processing
 classification with deep learning or whatever every time I look at an areawith deep learning I always see like oh
 it's terrible there's lots and lots of obviously stupid ways to do things thatneed to be fixed so then I want to be
 able to jump in there and quickly experiment and make them better using

0:16:55
Speaker 1 :the programming language is has a role


0:16:58
Speaker 0 :in a huge role yes so currently Python
 has a big gap in terms of our ability to innovate particularly around recurrentneural networks and natural language
 processing because it because it's so slow the the actual loop where weactually loop through words we have to
 do that whole thing in CUDA C so we actually can't innovate with the kernelthe heart of that most important
 algorithm and it's just a huge problem and this happens all over the place sowe hit you know research limitations
 another example convolutional neural networks which actually the most populararchitecture for lots of things maybe
 most things in declining we almost certainly should be using spaceconvolutional neural networks but only
 like two people are because to do it you have to rewrite all of that CUDA sealevel stuff and yeah this researchers
 and practitioners don't so like there's just big gaps in like what peopleactually research on what people
 actually implement because of the programming language problem so you

0:18:12
Speaker 1 :think you think it's it's just too
 difficult to write in CUDA see that a programming like a higher levelprogramming language like Swift should
 enable the the easier input fooling around creative stuff with RN ends orwas parse convolution your noise kind of
 who's a who's at fault who's who's a charge of making it easy for a research

0:18:41
Speaker 0 :- player I mean no one's at fault just
 know what he's got around to it yet or it's just it's hard right and I meanpart of the fault is that we ignored
 that whole APL kind of direction most prominently everybody did for 60 years50 years but recently people have been
 starting to reinvent pieces of that and kind of create some interesting newdirections in the compiler technology so
 the place where that's particularly happening right now is something calledml ir which is something that ok I'm
 Kris lat know this rift guy is leading and because it's actually not gonna beswift on its own that solves his problem
 because the problem is they're currently writing a acceptable fast you know GPUprogram is too complicated regardless of
 what language you use no and that's just because if you have to deal with thefact that I've got you know 10,000
 threads and I have to synchronize between them all and I have to put mything in to grid blocks and think about
 warps and all this stuff it's just it's just so much boilerplate to do that wellyou have to be a specialist at that and
 it's going to be a year's work to you know optimize that algorithm in that waybut with things like tensor
 comprehensions and tile and ml ir and t vm there's all these various projectswhich are all about saying let's let
 people create like domain-specific languages for tensor computations theseare the kinds of things we do are
 generally in on the GPU for deep learning and then have a compiler whichcan optimize that tensor computation a
 lot of this work is actually sitting on top of a project called halide which wasis a mind-blowing project where they
 came up with such a domain-specific language in fact true onedomain-specific language for expressing
 this is what my tensor computation is and another domain-specific language forexpressing this is the kind of the way I
 want you to structure the compilation of that like do it block by block and dothese bits in parallel
 they were able to show how you can compress the amount of code by 10xcompared to optimized GPU code and get
 the same performance so that's like so these other things are kind of sittingon top of that kind of research and ml
 ir is pulling a lot of those best practices together and now we'restarting to see work done on making all
 of that directly accessible through Swift so that I could use Swift to kindof write those domain-specific languages
 and hopefully we'll get them Swift CUDA kernels written in a very expressive andconcise way that looks a bit like J in
 APL and then Swift layers on top of that and then a swift UI on top of that andyou know it'll be so nice if we can get


0:21:42
Speaker 1 :to that point that does it all
 eventually boil down to CUDA and NVIDIA

0:21:47
Speaker 0 :GPUs unfortunately at the moment it does
 but one of the nice things about ml ir if AMD ever gets their act togetherwhich they probably won't is that they
 or others could write MLA our backends for other GPUs or other or other tensorcomputation devices of which today there
 are increasing number are like graph core or vertex AI or whatever so yeahbeing able to target lots of backends
 would be another benefit of this and the market really needs competitions at themoment NVIDIA is massively overcharging
 for their kind of enterprise class cards because there is no serious competitionbecause nobody else is doing the
 software properly in the cloud there is

0:22:39
Speaker 1 :some competition right but not really


0:22:42
Speaker 0 :other than TP used for heavy use are
 almost unprogrammed well at the moment

0:22:47
Speaker 1 :you can't the GPUs has the same problem


0:22:50
Speaker 0 :the case is even worse so TP use the
 Google actually made an explicit decision to make them almost entirelyunprogrammed ball because they felt that
 there was too much IP in there and if they gave people direct access toprogram them people would learn their
 secrets yeah so you can't actually directlyprogram the memory in a teepee you you
 can't even directly like create code that runs on and that you look at on themachine that has the GPU it all goes
 through a virtual machine so all you can really do is this kind of cookie cutterthing of like plug into high-level stuff
 together which is just super tedious and annoying and totally unnecessary so what

0:23:33
Speaker 1 :was the tell me if you could the origin
 story of fast AI what is the motivation its mission its dream so I guess the

0:23:44
Speaker 0 :founding story is heavily tied in my
 previous startup which is a company called in lytic which was the firstcompany to focus on deep learning for
 medicine and I created that because I saw that was a huge opportunity tothere's a there's a about a 10x shortage
 of the number of doctors in the world and the developing world that we needexpected it would take about three
 hundred years to train enough doctors to meet that gap but I guess that maybe ifwe used deep learning for some of the
 analytics we could maybe make it so you don't need as highly trained doctorsdiagnosis diagnosis and treatment


0:24:30
Speaker 1 :planning where's the biggest benefit
 just before get the first day I was where's the biggest benefit of AI inmedicine DC today and not much not much


0:24:37
Speaker 0 :happening today in terms of like stuff
 that's actually out there it's very early but in terms of the opportunityit's to take markets like India and
 China and Indonesia which have big populations Africa small numbers ofdoctors and provide diagnostic
 particularly treatment planning and triage kind of on device so that if youdo a you know test for malaria or
 tuberculosis or whatever you immediately get something that even a health careworker that's had a month of training
 can get a very high quality assessment of whether thepatient might be at risk until you know
 okay we'll send them off to a hospital so for example in Africa outside ofSouth Africa there's only five pediatric
 radiologists for the entire continent so most countries don't have any so if yourkid is sick and they need something
 diagnose your medical imaging the person even if you're able to get medicalimaging done the person that looks at it
 will be you know a nurse at best yeah but actually in India for example and inChina almost no x-rays are read by
 anybody by any trained professional because they don't have enough so ifinstead we had a algorithm that could
 take the most likely high-risk 5% and say triage basically say okay somebodyneeds to look at this it would massively
 change the kind of way that what's possible with medicine in the developingworld and remember they have
 increasingly they have money there the developing world they're not importedApella people so they have the money so
 that they're building the hospitals they're getting the diagnostic equipmentbut they just there's no way for a very
 long time will they be able to have the

0:26:37
Speaker 1 :expertise shortage of their sweeties
 okay and that's where the deep learning systems could step in and magnify theexpertise they do exactly yeah so you do
 see just a longer it a little bit longer yeah the interaction you still see thehuman expert still at the core of these
 systems yeah absolutely there's something in medicine that can beautomated almost completely I don't see


0:27:03
Speaker 0 :the point of even thinking about that
 because we have such a shortage of people why would we not why would wewant to find a way not to use them like
 we have people so the idea of like even from an economic point of view if youcan make them 10x more productive
 getting rid of the person doesn't impact your unit economics at all and ittotally ignores effect that there are
 things people do better than machines so it's just to me that's not a useful way

0:27:33
Speaker 1 :of framing the problem I guess
 just to clarify I guess I meant there may be some problems where you can avoideven going to the expert ever sort of
 maybe preventive care or some basic stuff flowing and food allowing theexpert to focus on the things that are
 that are really that well that's what

0:27:51
Speaker 0 :the triage would do right so the triage
 would say okay it's ninety ninety nine percent sure there's nothing here rightso you know that can be done on device
 and they can just say okay go home so the experts are being used to look atthe stuff which has some chance it's
 worth looking at which most things is it's not you know it's fine why do you

0:28:16
Speaker 1 :think we haven't quite made progress on
 that yet in terms of the the scale of how much AI is applied in the middle

0:28:27
Speaker 0 :there's a lot of reasons I mean one is
 it's pretty new I only started and let it can like 2014 and before that likeit's hard to express to what degree the
 medical world was not aware of the opportunities here so I went to iris nawhich is the world's largest radiology
 conference and I told everybody I could you know like I'm doing this thing thisdeep learning please come and check it
 out and no one had any idea what I was talking about and no one had anyinterest in it so like we've come from
 absolute zero which is hard and then the whole regulatory framework educationsystem everything is just set up to
 think of doctoring in a very different way so today there is a small number ofpeople who are deep learning
 practitioners and doctors at the same time and that we're starting to see thefirst ones come out of their PhD
 programs so that Kinane over in fostering Cambridge has a number ofstudents now who are data data science
 experts deep learning experts and and actual medical doctors quite a fewdoctors have completed
 first day of course now and are publishing papers and creating journalreading groups in the American Council
 of radiology and like it's just starting out but it's going to be a long processthey regulators have to learn how to
 regulate this they have to build you know guidelines and then the lawyers athospitals have to develop a new way of
 understanding that sometimes it makes sense for data to be you know looked atin raw form in large quantities in order
 to create world-changing results he has

0:30:26
Speaker 1 :a regulation around data all that it
 sounds it was probably the hardest problem but sounds reminiscent ofautonomous vehicles as well many of the
 same regulatory challenges meaning the same data challenges yeah I mean funnily

0:30:40
Speaker 0 :enough that problem is less their
 regulation and more the interpretation of that regulation by by lawyers inhospital so hipper is actually was
 designed to its it to P and hipper is not standing does not stand for privacyit stands for portability it's actually
 meant to be a way that data can be used and it was created with lots of grayareas because the idea is that would be
 more practical and would help people to use this this legislation to actuallyshare data in a more thoughtful way
 unfortunately it's done the opposite because when a lawyer sees a gray areathey see oh if we don't know we won't
 get sued then we can't do it today so hipper is not exactly the problem theproblem is more than there's hospital
 lawyers are not incentive to make bold decisions about data portability or even

0:31:36
Speaker 1 :to embrace technology that saves lives
 no they more want to not get in trouble for embracing the right but also it is

0:31:44
Speaker 0 :also so slaves in a very abstract way
 which is like oh we've been able to release these hundred thousand and onmost records I can't point at the
 specific person whose life that's saved I can say like oh we've ended up withthis paper which found this result which
 you know diagnosed a thousand more peopleotherwise but it's like which ones were
 helped it's it's very abstract and on

0:32:06
Speaker 1 :the counter side of that you may be able
 to point to a life that was taken

0:32:13
Speaker 0 :because of something though yeah or or
 or a person whose privacy was violated it was like oh this specific person youknow there was de-identified so we've
 identified just a fascinating topic

0:32:26
Speaker 1 :we're jumping around I'll get back to
 fast AI but on the question of privacy data is the fuel for so much innovationin deep learning what's your sense and
 privacy whether we're talking about Twitter Facebook YouTube just thetechnologies like in the medical field
 that rely on people's data in order to create impact how do we get that rightrespecting people's privacy and yet
 creating technology that just learns

0:33:03
Speaker 0 :from data one of my areas of focus is on
 doing more with less data which so most vendors unfortunately are stronglyincented to find ways to require more
 data and more computation so Google and IBM being the most obvious IBM yeah soWatson you know so Google and IBM both
 strongly push the idea that you have to be you know that they have more data andmore computation and more intelligent
 people than anybody else and so you have to trust them to do things becausenobody else can do it and Google's very
 upfront about this like Geoff Dana's going out there and given talks and saidour goal is to require a thousand times
 more computation but less people our goal is to use the people that you havebetter and the data you have better in
 the computation you have better so one of the things that we've discovered isor or at least highlighted is that you
 very very very often don't need much data at all and so the data you alreadyhave in your organization
 we'll be enough to get state-of-the-art results so like my starting point wouldbe this going to say around privacy is a
 lot of people are looking for ways to share data and aggregate data but Ithink often that's unnecessary they
 assume that they need more data than they do because they're not familiarwith the basics of transfer learning
 which is this critical technique for needing orders of magnitude less data is

0:34:41
Speaker 1 :your sense one reason you might want to
 collect data from everyone is like in the recommender system context whereyour individual Jeremy Howard's
 individual data is the most useful for freeing for providing a product that'simpactful for you so for giving you
 advertisements for recommending to your movies for doing medical diagnosis isyour sense we can build with a small
 amount of data general models they will have a huge impact for most people thatwe don't need to have data from punching


0:35:18
Speaker 0 :on the whole I'd say yes I mean they're
 things like you know recommender systems have this cold-start problem where youknow Jeremy is a new customer we haven't
 seen him before so we can't recommend him things based on what else he'sbought and liked with us and there's
 various workarounds to that like in a lot of music programs we'll start out bysaying which of these artists you like
 which of these albums do you like which of these songs do you like Netflix usedto do that nowadays they they tend not
 to people kind of don't like that because they think oh we don't want tobother the user so you could work around
 that by having some kind of data sharing where you get my marketing record fromaxiom or whatever and try to guess from
 that to me the the benefit to me and to society of saving me five minutes onanswering some questions versus the
 negative externalities of if the privacy issue doesn't add up so I think like alot of the time the places where people
 are invading our privacy in order to provideconvenience is really about just trying
 to make them more money and and they move these negative externalities and toplaces that they don't have to pay for
 them so when you actually see regulations appear that actually causethe companies that create these negative
 externalities to have to pay for it themselvesthey say well we can't do it anymore so
 the cost is actually too high right but for something like medicine yeah I meanthe hospital has my you know medical
 imaging my pathology studies my medical records and also I own my medical dataso you can so I I helped a startup
 called doc AI one of the things doc AI does is that this has an app you canconnect to you know Sutter Health's and
 webcore and Walgreens and download your medical data to your phone and thenupload it again at your discretion to
 share it as you wish so with that kind of approach we can share our medicalinformation with the people we want to


0:37:44
Speaker 1 :yes of control I mean it really being
 able to control who you share with us on yeah so that that has a beautifulinteresting tangent but to return back
 to uh the origin story of fast they act

0:37:59
Speaker 0 :right so so before I started fast AI I
 spent a year researching where the biggest opportunities for deep learningbecause I knew from my time at Cal in
 particular that deep learning had kind of hit this threshold point where it wasrapidly becoming the state of the art
 approach in every areas that looked at it and I've been working with neuralnets for over 20 years I knew that from
 a theoretical point of view once it hit that point it would do that in kind ofjust about every domain and so I kind of
 spent a year researching what are the domains it's going to have the biggestlow-hanging fruit in the shortest time
 period medicine but there were so many I couldhave picked and so there was a kind of
 level of frustration for me of like okay I'm really glad we've opened up themedical deep learning world and today is
 huge as you know but we can't do you know I can't do everything I don't evenknow like it took like in medicine it
 took me a really long time to even get a sense of like what kind of problems tomedical practitioners solve what kind of
 data do they have who has that data so I kind of felt like I need to approachthis differently if I want to maximize
 the positive impact of deep mourning rather than me picking an area andtrying to become good at it and building
 something I should let people who are already domain experts in those areasand who already have the data do it
 themselves mm-hmm so that was the reason for fast AI is to basically try andfigure out how to get deep learning into
 the hands of people who could benefit from it and help them to do so in asquick and easy and effective way as
 possible god it's all sort of empowered

0:39:47
Speaker 1 :the the domain expert yeah and like


0:39:50
Speaker 0 :partly it's because like unlike most
 people in this field my background is very applied andindustrial that my first job at MIT was
 at McKinsey and company I spent 10 years in management consulting I I spend a lotof time with domain experts you know so
 I kind of respect them and appreciate them and know I know that's where thevalue generation in society is and so I
 also know how most of them can't code and most of them don't have the time toinvest you know three years and a
 graduate degree or whatever so it's like how do i skill those two main experts Ithink it would be a super powerful thing
 you know biggest societal impact I could have so that yeah that was the thinking

0:40:42
Speaker 1 :so so much a fast AI students and
 researchers and the things you teach are pragmatically minded right practicallyminded freaking figuring out ways how to
 solve real problems and fast right so fromyour experience what's the difference
 between theory and practice of deep

0:41:01
Speaker 0 :learning well most of the research in
 the deep mining world is a total waste of time all right that's what I wasgetting at yeah it's it's a problem in
 science in general scientists need to be published which means they need to workon things that their peers are extremely
 familiar with and can recognize in advance in that area so that means thatthey all need to work on the same thing
 and so it really Inc and and the thing they work on there's nothing toencourage them to work on things that
 are practically useful so you get just a whole lot of research which is minoradvances and stuff that's been very
 highly studied and has no significant practical impact where else the thingsthat really make a difference like I
 mentioned transfer learning like if we can do better at transfer learning thenit's this like world-changing thing
 we're suddenly like lots more people can do world-class work with less resourcesand less data and but almost nobody
 works on that or another example active learning which is the study of like howdo we get more out of the human beings
 in the loop where's my favorite topic

0:42:15
Speaker 1 :

0:42:16
Speaker 0 :yeah so active learning is great but
 it's almost nobody working on it because it's just not a trendy thing right now

0:42:23
Speaker 1 :you know what somebody's suicide
 interrupt you're saying that nobody is publishingan active learning but there's people
 inside companies anybody who actually has to solve a problem they're going toinnovate an active learning yeah


0:42:39
Speaker 0 :everybody kind of reinvents active
 learning when they actually have to work in practice because they start labelingthings and they think gosh this is
 taking a long time and it's very expensive and then they start thinkingwell why am i labeling everything I'm
 only the machines only making mistakes on those two classes they're the hardones maybe I ought to start labeling
 those two classes and then you start thinking well why did I do that manuallywhy kind of just get the system to tell
 me which things are going to be hardest it's an obvious thing to do butyeah it's it's just like like transplant
 learning it's it's under studied and the academic world just has no reason tocare about practical results the funny
 thing is like I've only really ever written one paper I hate writing papersand I didn't even write it it was my
 colleague sebastian ruder who actually wrote it I just knew did the researchfor it but it was basically introducing
 transfer learning successful transfer learning to NLP for the first time thealgorithm is called GLM fit and it
 actually I actually wrote it for the course for the first day of course Iwanted to teach people in LP and I
 thought I only want to teach people practical stuff and I think the onlypractical stuff is transfer learning and
 I couldn't find any examples of transfer learning and NLP so I just did it and Iwas shocked to find that as soon as I
 did it was you know the basic prototype took a couple of days smashed thestate-of-the-art on one of the most
 important data sets in a field that I knew nothing about and I just thoughtwell this is ridiculous
 and so I spoke to the best unit and he kindly offered to write it up theresults and so it ended up being
 published in a CL which is the top link with a computational linguisticsconference so like people do actually
 care once you do it but I guess it's difficult for maybe like juniorresearchers or like like I don't care
 whether I get citations or papers whatever I was right there's nothing inmy life that makes that important which
 is why I've never actually bothered to write a pic of myself now for people whodo I guess they have to pick the kind of
 safe option which is like yeah make a slight improvement on something thateverybody is already working on yeah


0:44:54
Speaker 1 :nobody does anything interesting or
 succeeds in life or the safe option

0:45:00
Speaker 0 :speed I mean the nice thing is nowadays
 everybody is now working on you know a transfer learning because since thattime we've had GPT and GPT too and Burt
 and you know it's like it's so yeah once you show that something is possible ifnobody jumps you and I guess I


0:45:17
Speaker 1 :hope to be a part of and I hope to see
 more innovation and active learning in the same way I think yeah try learningan active learning are fascinating
 public open were I actually helped start

0:45:26
Speaker 0 :a startup called platform AI which is
 really all about active learning and yeah it's very interesting trying tokind of see what research is out there
 and make the most of it and there's basically none so we've had to do all

0:45:40
Speaker 1 :our own research once again and just as
 easy described can you tell the story of the stanford competition dawn bench andfast day eyes achievement on it sure so


0:45:51
Speaker 0 :something which I really enjoy is that I
 basically teach two courses a year the practical deep money for coderswhich is kind of the introductory course
 and then cutting-edge tech mining for coders which is the kind of researchlevel course and while I teach those
 courses I have a I basically have a big office at the University of SanFrancisco big enough for like 30 people
 and I invite anybody any student who wants to come and hang out with me wellI built the course and so generally it's
 full and so we have twenty or thirty people in a big office with nothing todo but study deep learning so it was
 during one of these times that somebody in the group said oh there's a thingcalled Don benched it looks interesting
 and I was like what the hell is that is it about some competition to see howquickly you can train a model seems kind
 of not exactly relevant to what we're doing but it sounds like the kind ofthing which you might be interested in I
 checked it out and I said oh crap there's only ten days till it's overit's pretty too late and we're kind of
 busy trying to teach this course yeah maybe like oh it would make aninteresting case study for the course
 like it's all the stuff where you're already doing why don't you just puttogether our current best practices and
 ideas so me and I guess about four students just decided to give it a goand we focused on this more one called
 Sipho ten which is that all 32 by 32

0:47:24
Speaker 1 :pixels can you say word on benches yeah


0:47:26
Speaker 0 :so it's a competition to train a model
 as fast as possible I was run by Stanfordas cheap as possible - that's also
 another one first cheap as possible and there was a couple of categoriesimagenet and so far 10 so image nets is
 big 1.3 million image thing that took a couple of days to train remember afriend of mine Pete worden who's now at
 Google I remember he told me how he trained imagenet a few years ago and hebasically like had this little granny
 flat out the back that he turned into his image net training center and hefigured you know after like a year of
 work he figured out how to train it and like ten days or something it's likethat was a big job well so far ten at
 that time you could train in a few hours you know it's much smaller and easier sowe thought would try so far 10 and yeah
 I've really never done that before like I've never really liked things likeusing more than one gpgpu at a time was
 something I tried to avoid cuz to me it's like very against the whole idea ofaccessibility is she better to do things
 with 1gb here I mean have you asked in

0:48:35
Speaker 1 :the past before after having
 accomplished something how do I do this faster much faster Oh always but it's

0:48:42
Speaker 0 :always for me it's always how do I make
 it much faster on a single genus you that a normal person could afford intheir day-to-day life it's not how could
 I do it faster I you know having a huge data center because up to me it's allabout like as many people should be to
 use something as possible without fussing around with infrastructure soanyway so in this case it's like well we
 can use eight GPUs just by renting a AWS machine so we thought we'd try that andyeah basically using the stuff we were
 already doing we were able to get you know the speed you know within a fewdays we had to speed down to I don't
 know that's a very small number of minutes I can't remember exactly howmany minutes it was but I might have in
 like 10 minutes or something and so yeah we found ourselves at the top of theleaderboard easily for both time and
 money which really shocked me because the other people competing this werelike Google and Intel and stuff we're
 like know a lot more about this stuff I think we do so that we were emboldenedwe thought let's try the imagenet one
 two way out of our league but our goal was to get under 12 hours yeah and wedid which was really exciting and but we
 didn't put anything up on the leaderboard but we were down to like 10hours but then Google put in some like 5
 hours or something about us like oh they're so screwed but we kind ofthought we'll keep trying you know if
 Google can do it info I mean Google did on five hours on someone like a TPU podor something like a lot of hardware but
 we kind of like had a bunch of ideas to try like a really simple thing was whyare we using these big images they're
 like 224 256 by 256 pixels you know why

0:50:37
Speaker 1 :don't we try smaller ones and just
 elaborate there's a constraint on the accuracy that your training model issupposed to achieve yeah you got to


0:50:43
Speaker 0 :achieve 93% I think it was for imagenet


0:50:48
Speaker 1 :exactly which is very tough so you have


0:50:51
Speaker 0 :to yeah 93% like they think that they
 picked a good threshold it was a little bit higher than what the most commonlyused ResNet 50 model could achieve at
 that time so yeah so it's quite a difficult problem to solve but yeah werealized if we actually just use 64 by
 64 images it trained a pretty good model and then we could take that same modeland just give it a couple of epochs to
 learn 224 by 224 images and it was basically already trained it makes a lotof sense like if you teach somebody like
 here's what a dog looks like and you show them low res versions and then yousay here's a really clear picture of a
 dog they already know what a dog looks like so that like just we jumped to thefront and we ended up winning
 parts of that competition we actually ended up doing a distributed versionover multiple machines a couple of
 months later and ended up at the top of the leaderboard we had 18 minutes in ityeah and it was and people have just
 kept on blasting through again and again

0:52:02
Speaker 1 :since then so so what's your view on
 multi-gpu or multiple machine training in general as as a way to speed code up

0:52:12
Speaker 0 :I think it's largely a waste of time


0:52:13
Speaker 1 :both multi-gpu on a single machine and


0:52:15
Speaker 0 :yeah particularly multi machines because
 it's just clunky motogp use is less clunky than it used to be but to meanything that slows down your iteration
 speed is a waste of time so you could maybe do your very last you knowperfecting of the model on Motty GPUs if
 you need to that so for example I think doing stuff on imagenet is generally awaste of time why test things on 1.3
 million images most of us don't use 1.3 million images and we've also doneresearch that shows that doing things on
 a smaller subset of images gives you the same relative answers anyway so from aresearch point of view why waste that
 time so actually I released a couple of new data setsrecently one is called imaginet the
 French image net which is a small subset of image net which is designed to beeasy to classify I would highly spell


0:53:15
Speaker 1 :

0:53:17
Speaker 0 :imaginer it's got an extra T and e at
 the end because it's very French am i okay yeah I'm okay and then another onecalled image Wharf which is a subset of
 the image net that only contains dog breedsthat's a hard one right that's a hard
 one yeah and I've discovered that if you just look at these two subsets you cantrain things on a single GPU in ten
 minutes and the results you get directly transferable to imagenet nearly all thetime and so now I'm starting to see some
 researchers start to use these holidays

0:53:48
Speaker 1 :that's so deeply love the way you think
 because I think you might have written a blog post saying that sort of goingthese big data sets is encouraging
 people to not think creatively absolutely so you're - it's sort ofconstrained you to Train on large
 resources and because you have these resources you think more research willbe bit better and then you start like
 for some somehow you kill the creativity

0:54:17
Speaker 0 :yeah and even worse than that Lex I keep
 hearing from people who say I decided not to get into deep learning because Idon't believe it's accessible to people
 outside of Google to do useful work so like I see a lot of people make anexplicit decision to not learn this
 incredibly valuable tool because they've they've drunk the Google kool-aid whichis that only Google's big enough and
 smart enough to do it and I just find that so disappointing and it's so wrong

0:54:45
Speaker 1 :and I think all the major breakthroughs
 in AI in the next twenty years will be doable on a single GPUlike I would say my sense is all the big
 sort of well let's put it this way none

0:54:57
Speaker 0 :of the big breakthroughs of the last 20
 years or acquired multiple GPUs so like fetch norm well you drop outdid you demonstrate to everyone of them
 yeah this is five multiple GPUs against

0:55:11
Speaker 1 :the original Gans didn't require


0:55:15
Speaker 0 :multiple ups well and and we've actually
 recently shown that you don't even need gains so we've developed gained leveloutcomes without knitting Gans and we
 can now do it with again by using transfer learning we can do it in acouple of hours on a single generator


0:55:30
Speaker 1 :might like without the other serial port


0:55:33
Speaker 0 :yeah
 so we've found loss functions that work super well without the adversarial partand then one of our students guy called
 Jason antic has created Cordiale defi which uses this techniqueto colorize old black-and-white movies
 you can do it on a single GPU color as a whole movie in a couple of hours and oneof the things that Jason and I did
 together was we figured out how to add a little bit of n at the very end which itturns out for colorization makes it just
 a bit brighter and nicer and then Jason did masses of experiments to figure outexactly how much to do but it's still
 all done on his home machine on a single GPU in his lounge room and like if youthink about like colorizing Hollywood
 movies that sounds like something a huge studio it would have to do but he hasthe world's best results on this there's


0:56:25
Speaker 1 :this problem of microphones we're just
 talking two microphones now yeah it's such a pain in the ass to have thesemicrophones to get good quality audio
 and I tried to see if it's possible to plop down a bunch of cheap sensors andreconstruct higher quality audio from
 multiple sources because right now I haven't seen work from okay we can sayinexpensive mics automatically combining
 audio from multiple sources to improve the combined audio right people haven'tdone that and that feels like a learning
 problem alright so hopefully somebody

0:56:56
Speaker 0 :can well I mean it's it's eminently
 doable and it should have been done by nowI feel I felt the same way about
 computational photography four years ago that's rightwhy are we investing in big lenses when
 three cheap lenses plus actually a little bit of intentional movement solike Holden you don't like take a few
 frames gives you enough information to get excellent sub pixel resolution whichparticularly with deep learning
 you would know exactly what you meant to be looking at we can totally do the samething with audio I think there's a
 madness that it hasn't been done yet I

0:57:30
Speaker 1 :live in progress on the photographer tog
 Rafik um yeah the dog photography is

0:57:33
Speaker 0 :basically standard now so the the Google
 picks all night light I don't know if you've ever tried it but it's it'sastonishing you take a picture in almost
 pitch black and you get back a very high quality image and it's not because ofthe lens same stuff is like adding the
 bouquet to the you know the background wearing have donecomputationally this depicts over here
 yeah basically the everybody now is doing most of the fanciest stuff ontheir phones with computational
 photography and also increasingly people are putting more than one lens on theback of the camera so the same will
 happen for audio for sure and there's

0:58:14
Speaker 1 :applications in the audio side if you
 look at an Alexa type device most people have seen especially I worked at Googlebefore when you look at noise background
 removal you don't think of multiple sources of audio you don't play withthat as much as I would hope people I


0:58:31
Speaker 0 :mean you can still do it even with one
 like again it's not not much works being done in this area so we're actuallygoing to be releasing an audio library
 soon which hopefully will encourage development of this because it's sounderused the basic approach we used for
 our super resolution in which Jason uses video defy of generating high qualityimages the exact same approach would
 work for audio no-one's done it yet but it would be a couple of months work okay

0:58:57
Speaker 1 :are also learning rate in terms of Don
 bench there's some magic on learning rate that you played around with yeah

0:59:05
Speaker 0 :interesting yeah so this is all work
 that came from a guy called Leslie Smith Leslie's a researcher who like us caresa lot about just the practicalities of
 training neural networks quickly and accurately which i think is whateverybody should care about but almost
 nobody does and he discovered something very interesting which he calls superconvergence which is there are certain
 networks that with certain settings of high parameters could suddenly betrained ten times faster by using a ten
 times higher learning rate now no one published that paper because it's not anarea of kind of active research in the
 academic world no academics recognized this is important and also deep learningin academia is not considered a
 experimental science so unlike in physics where you could say like I justsaw as a subatomic particle do something
 which the theory doesn't explain you could publish thatwithout an explanation and then in the
 next 60 years people can try to work out how to explain itwe don't allow this in the deep learning
 world so it's it's literally impossible for Leslie to publish a paper that saysI've just seen something amazing happen
 this thing trained ten times faster than it should have I don't know whyand so the reviewers were like we can't
 publish that because you don't know why so anyway that's important to pause on

1:00:31
Speaker 1 :because there's so many discoveries that
 would need to start like that every

1:00:35
Speaker 0 :every other scientific field I know of
 works is that way I don't know why ours is uniquely disinterested in publishingunexplained experimental results but
 there it is so it wasn't published having said that I read a lot moreunpublished papers and published papers
 because that's where you find the interesting insights so I absolutelyread this paper and I was just like this
 is astonishingly mind-blowing and weird and awesome and like why isn't everybodyonly talking about this because like if
 you can train these things ten times faster they also generalized betterbecause you're you're doing less epochs
 which means you look at the data less you get better accuracy so I've beenkind of studying that ever since and
 eventually Leslie kind of figured out a lot of how to get it's done and we addedminor tweaks and a big part of the trick
 is starting at a very low learning rate very gradually increasing it so asyou're training your model you would
 take very small steps at the start and it gradually makes them bigger andbigger and tall eventually you're taking
 much bigger steps than anybody thought as possiblea few other little tricks to make it
 work but ever ever basically we can reliably get super convergence and sofor the dawn bench thing we were using
 just much higher learning rates than people expected to work what do you

1:02:02
Speaker 1 :think the future of I mean makes so much
 sense for that to be a critical hyper parameter learning rate that you verywhat do you think the future of learning
 rate magic looks like well there's been

1:02:12
Speaker 0 :a lot of great work in the last 12
 months in this area it's and people are increasingly realizing that up to mightlike we just have no idea really how
 optimizers work and the combination of weight decay which is how we regularizeoptimizers and the learning rate and
 then other things like the epsilon we use in in the atom optimizer they allwork together in weird ways and
 different parts of the model this is another thing we've done a lot of workon is research into how different parts
 of the model should be trained at different rates in different ways so wedo something we call discriminative
 learning rates which is really important particularly for transfer learning soreally I think in the last 12 months a
 lot of people have realized that this all this stuff is important there's beena lot of great work coming out and we're
 starting to see algorithms here which have very very few dials if any that youhave to touch selector I think what's
 going to happen is the idea of a learning rate well it almost already hasdisappeared in the latest research and
 instead it's just like you know we we know enough about how to interpret thegradients and the change of gradients we
 see to know how to set every parameter

1:03:25
Speaker 1 :you can await it so you see the future
 of of deep learning where really where's the input of a human expert needed well

1:03:34
Speaker 0 :hopefully the input of the human expert
 will be almost entirely unneeded from the deep learning point of view so againlike Google's approach to this is to try
 and use thousands of times more compute to run lots and lots of models at thesame time and hopefully one of them is
 good at or male CONUS yeah I don't know kind of stuff which i think is insanewhen you better understand the mechanics
 of how models learn you don't have to trythousand different models to find which
 one happens to work the best you can just jump straight to the best one whichmeans that it's more accessible in terms
 of compute cheaper and also with less hyper parameters to set it means youdon't need deep learning experts to
 train your deep learning model for you which means that domain experts can domore of the work which means that now
 you can focus the human time on the kind of interpretation data gatheringidentifying what all errors and stuff


1:04:31
Speaker 1 :like that yeah the data side how often
 do you work with data these days in terms of the cleaning looking at likeDarwin looked at different species while
 traveling about do you look at data I have you in your roots and cargo always

1:04:49
Speaker 0 :yeah good data I mean it's a key part of
 our course it's like before we train a model in the course we see how to lookat the data and then after the first
 thing we do after we train our first model which we fine-tune an image netmodel for five minutes and then the
 thing we immediately do after that is we learn how to analyze the results of themodel by looking at examples of
 misclassified images and looking at a classification matrix and then doinglike research on Google to learn about
 the kinds of things that it's misclassifying so to me one of the threecool things about machine learning
 models in general is that you can interpret when you interpret them theytell you about things like what are the
 most important features which groups you misclassifying and they help you becomea domain expert more quickly because you
 can focus your time on the bits that the model is telling you it is important soit lets you deal with things like data
 leakage for example if it says all the main feature I'm looking at is customerID you know and you're like oh customer
 ID should be predictive and then you can talk to the people that manage customerIDs and they'll tell you like oh yes as
 soon as a customer's application is accepted we add a one on the end oftheir customer arm or something you know
 yeah so yeah model looking at data particularly from the lens of whichparts of the date of the model says is
 important is super important yeah and

1:06:09
Speaker 1 :using kind of using the model to almost
 debug the data yeah you have learn more

1:06:14
Speaker 0 :about exactly


1:06:16
Speaker 1 :what are the different cloud options for
 training y'all networks it's the last question related to dawn bench well it'spart of a lot of the work we do but from
 a perspective of performance I think you've written this in a blog postthere's AWS there's TPU from Google
 what's your sense what the future holds what would you recommend now right there

1:06:36
Speaker 0 :was a so from a halfway point of view
 Google's TP use and the best nvidia gpus are similar I mean maybe the TP is like30% faster but they're also much harder
 to program with there isn't a clear leader in terms of hardware right nowalthough much more importantly the GPU
 nvidia gpus a much more programmable they've got much more written for allthem so like that's the clear leader for
 me and where I would spend my time as a researcher and practitioner millingtonto the platform
 I mean we're super lucky now with stuff like Google TCP Google Cloud and AWSthat you can access a GPU pretty quickly
 and easily but I mean for AWS it's still too hard like you have to find an amiand get the instance running and then
 install the software you want blah blah blah GCP is still is currently the thebest way to get started on if the server
 environment because they have a fantastic fast AI in pi torch ready togo instance which has all the courses
 pre-installed it has Jupiter notebook pre running Jupiter notebook is thiswonderful interactive computing system
 which everybody basically should be using for any kind of data-drivenresearch but then even better than there
 are there are platforms like salamander which we own and paper space whereliterally you click a single button and
 it pops up a Jupiter notebook straight away without any kind of installation oranything and all the course notebooks
 are all pre-installed so like for me we this is one of the things we spenta lot of time kind of curating and
 working on because when we first started our courses the biggest problem waspeople dropped out of lesson one because
 they couldn't get an AWS instance running so things are so much better nowand like we actually have if you got a
 cost up faster day I the first thing it says is here's how to get started withyour GPU and there's like you just click
 on a link and you click start and and

1:08:54
Speaker 1 :it's going it will you a go GCP I have
 to confess I've never used the Google DCP yeah JCP gives you three hundred

1:08:59
Speaker 0 :dollars of compute for free which is
 really nice that as I say a salamander and paper spacer even even easier stillokay so the from the perspective of deep


1:09:12
Speaker 1 :learning frameworks you work with fast
 AI to go to this framework and PI torch intensive flow what are the strengths ofeach platform your perspective so in


1:09:25
Speaker 0 :terms of what we've done our research on
 and taught in our course we started with Theano and care us and then we switch totensor flow and care us and then we
 switch to PI torch and then we switched to PI torch and fast AI and that thatkind of reflects a growth and
 development of the ecosystem of dig learning libraries siano intensive flowwere great but we're much harder to
 teach and do research and development on because they define what's called acomputational graph upfront less data
 graph well you basically have to say here are all the things that I'm goingto eventually do in my model and then
 later on you say okay do those things with this data and you can't like debugthem you can't do them step-by-step you
 can't program them interactively in a Jupiter notebook and so forthpi torch was not the first four pi torch
 was certainly the the strongest entrant to come along and say let's not do itthat way let's just use normal Python
 and everything you know about in Python is just going to work and we'll figureout how to make that run on the GPU as
 in when and necessary that turned out to be a huge a huge leap in terms of whatwe could do with our research and what
 we could weigh with our teaching and

1:10:49
Speaker 1 :because it was a limiting yeah I mean it


1:10:51
Speaker 0 :was critical for us for something like
 dawn Bench to be able to rapidly try things it's just so much harder to be aresearcher and practitioner when you
 have to do everything up front and you can inspect it problem with pay torch isit's not at all accessible to newcomers
 because you have to like write your own training loop and manage the gradientsand all their stuff and it's also like
 not great for researchers because you're spending your time dealing with all thisboilerplate and overhead rather than
 thinking about your algorithm so we ended up writing this very multi-layeredAPI that at the top level you can train
 a state-of-the-art neural network in three lines of code and which kind oftalks to an API which talks to an API
 which talks from API which like you can deep dive into at any level and getprogressively closer to the Machine kind
 of levels of control and this is the first AI library that's been criticalfor us and for our students and for lots
 of people that have one big learning competitions with it and writtenacademic papers with it it's made a big
 difference we're still limited though by Python and particularly this problemwith things like recurrent neural nets a
 where you just can't change things unless you accept it going so slowlythat it's impractical so in the latest
 incarnation of the course and with some of the research risked out now startingto do we're starting to do stuff some
 stuff in Swift I think we're three years away from thatbeing super practical but I'm in no
 hurry I'm very happy to invest the time to get there but you know with with thatwe actually already have a nascent
 version of the first AI library for vision running on special knowledge andso flow
 because a Python for tensorflow is not going to cut it it's just a disasterwhat they did was they tried to
 replicate the bits that people were saying they like about a torch the iskind of interactive computation but they
 didn't actually change their foundational runtime components so theykind of added this like syntax sugar
 they call TF eager tend to flow again which makes it look a lot like pay torchbut it's 10 times slower than pi torch
 to actually hmm do a step so because they didn't invest the time and likeretooling the foundations cuz their code
 base is so horribly copy yeah I think

1:13:23
Speaker 1 :it's probably very difficult to do that
 kind of rejoin yeah well particularly

1:13:26
Speaker 0 :the way tensorflow was written it was
 written by a lot of people very quickly in a very disorganized way so like whenyou actually look in the code as I do it
 often I'm always just like oh god what were they thinking it's just it's prettyawful so I'm really extremely negative
 about the potential future if it by the

1:13:48
Speaker 1 :

1:13:49
Speaker 0 :flaws of the fet swift for tensorflow
 can be a different beast altogether it can be like it can basically be a layeron top of M lar that takes advantage of
 you know all the great compiler stuff that Swift builds on with LLVM and yeahit could be a thing kit will be
 absolutely fantastic well you're

1:14:10
Speaker 1 :inspiring me to try evan Roo truly felt
 the pain of tensorflow 2.0 python it's fine by me but yeah but it does the job

1:14:21
Speaker 0 :if you're using like predefined things
 that somebody's already written but if you actually compare you know like I'vehad to do because I've been having to do
 a lot of stuff with tensorflow recently you actually compare like okay I want towrite something from scratch yeah like I
 just kick fighting is like oh it's running ten times slower than pi torch

1:14:42
Speaker 1 :so is the biggest cost let's throw
 running time out the window how long it

1:14:48
Speaker 0 :takes you to program that's not too
 different now thanks to transfer flow eager that's not too different butbecause because so many things take so
 long to run yeah you wouldn't run it at tentimes slower like you just go like oh
 this is taking so long yeah and also there's a lot of thingswhich are just less programmable like TF
 data which is the way they do processing works intensive flow is just this bigmess it's incredibly inefficient and
 they kind of had to write it that way because of the TPU problems I describedearlier so I just you know I just feel
 like they've got this huge technical debt which they're not gonna solvewithout starting from scratch so here's


1:15:28
Speaker 1 :an interesting question then if there's
 a new student starting today what would you recommend they use well I mean we

1:15:37
Speaker 0 :obviously recommend fast AI and pi torch
 because we teach new students and that's what we teach with so we would verystrongly recommend that because it will
 let you get on top of the concepts much more quickly so then you'll become anextra and you'll also learn the actual
 state-of-the-art techniques you know so you actually get world-class resultshonestly it doesn't much matter what
 library you learn because switching from China to MX net to tensorflow to PItorch is going to be a couple of days
 work as few long as you understand the foundation as well but you think we'll

1:16:15
Speaker 1 :Swift creep in there as a thing that
 people start using not for a few years

1:16:22
Speaker 0 :particularly because like Swift has no
 data science community libraries Oh basil wing and the Swift community has aa total lack of appreciation and
 understanding of numeric computing so like they keep on making stupiddecisions you know for years they've
 just done dumb things around performance and prioritizationthat's clearly changing now because the
 developer of Chris Christie at developer of Swift Chris Latner is working atGoogle on the Swift Potenza flows so
 like that's that's a priority it'll be interesting to see what happens withApple because like Apple hasn't shown
 any sign of caring about numeric programming in Swift so I meanhopefully they'll get off their ass and
 start appreciating this because currently all of their low-levellibraries are not written in Swift
 they're not particularly swifty at all stuff like [ __ ] ml they're reallypretty rubbish so yeah so there's a long
 way to go but at least one nice thing is that Swift for tensorflowcan actually directly use Python code
 and Python libraries you know literally the entire lesson one notebook a fast AIruns in Swift right now in Python mode
 so that's that's a nice intermediate

1:17:52
Speaker 1 :thing how long does it take the look at
 the two two facile courses how long does it take to get from point zero to

1:18:02
Speaker 0 :completing both courses it varies a lot
 somewhere between two months and two years generally

1:18:13
Speaker 1 :so for two months how many hours a day


1:18:16
Speaker 0 :so I sound like a somebody who is a very
 competent coder can can do 70 hours per course and seventy seven zero yeah

1:18:28
Speaker 1 :

1:18:30
Speaker 0 :that's it okay but a lot of people I
 know take a year off to study first day I full-time and say at the end of theyear they feel pretty competent because
 generally there's a lot of other things you do like they're generally they'll beentering cowgirl competitions they you
 might be reading in Goodfellows books they might you know they'll be doing abunch of stuff and often you know
 particularly if they are domain expert they're coding skills might be a littleon the pedestrian side so part of it's
 just like doing a lot more writing what

1:19:05
Speaker 1 :do you find is the bottleneck for people
 usually except getting started and

1:19:11
Speaker 0 :setting stuff up I would say coding just
 yeah I would say the best the people who are strong coders pick it up the bestalthough another bottleneck is people
 who have a lot of experience of classic statistics can really struggle becauseit the intuition is so the opposite of
 what they used to they're very used to like trying to reduce the number ofparameters in their model and looking at
 individual coefficients and stuff like that so I find people who have a lot ofcoding background and know nothing about
 statistics are generally going to be the best off so you taught several course on

1:19:48
Speaker 1 :deep learning and as Fineman says the
 best way to understand something is to teach it what have you learned aboutdeep learning from teaching it a lot


1:19:59
Speaker 0 :it's a key reason for me to to teach the
 courses I mean obviously it's going to be necessary to achieve our goal ofgetting two main experts to be familiar
 with deep learning but it was also necessary for me to achieve my goal ofbeing really familiar with deep learning
 I I mean to see so many domain experts from so many different backgrounds it'sdefinitely I wouldn't say taught me but
 convinced me something that I like to believe was true which was anyone can doit so there's a lot of kind of
 snobbishness out there about only certain people can learn to code onlycertain people are going to be smart
 enough to like do AI that's definitely [ __ ] you know I've seen so manypeople from so many different
 backgrounds get state-of-the-art results in their domain areas now the it'sdefinitely taught me that the key
 differentiator between people that succeed and people that fail is tenacitythat seems to be basically the only
 thing that matters the people a lot of people give up and but if the ones whodon't give up pretty much everybody
 succeeds you know even if at first I'm just kind of like thinking like wowthey're really not quite getting it yet
 are they but eventually people get it and they succeed so I think that's beenany they're both things I'd like to
 believe was true but I don't feel like I really had strong evidence with them tobe true but now I can say I've seen it
 again and again so what advice do you

1:21:35
Speaker 1 :have for someone who wants to get
 started in deep learning train lots of

1:21:41
Speaker 0 :models that's that's how you that's how
 you learn it so like so I would you know I think it's not just me I think I thinkour course is very good but also lots of
 people independently I said it's very good it recently won the cog X award forAI courses as being the best in the
 world let's say come to our course cost up faster day I and the thing I keep onhopping on in my lessons is train models
 print out the inputs to the models print out to the outputs to the models likestudy you know change change the inputs
 of it look at how the outputs very just run lots of experiments to get a youknow an intuitive understanding of
 what's going on to get hooked do

1:22:25
Speaker 1 :think you mentioned training do you
 think just running the models inference like if we talk about getting started no

1:22:35
Speaker 0 :you've got to find cheering the models
 so that's that's that's the critical thing because at that point you now havemodel that's in your domain area so
 there's there's there's no point running somebody else's model because it's notyour model like so it only takes five
 minutes to fine-tune a model for the data you care about and in lesson two ofthe course we teach you how to create
 your own data set from scratch by scripting Google Image Search yeah soand we show you how to actually create a
 web application running online so I create one in the course thatdifferentiates between a teddy bear or
 grizzly bear and a brown bear and it does it with basically a hundred percentaccuracy took me about four minutes to
 scrape the images from Google search in the script there's a little graphicalwidgets we have in the notebook that
 help you clean up the data set there's other widgets that help you study theresults to see where the errors are
 happening and so now we've had got over a thousand replies in our share yourwork here thread of students saying
 here's the thing I built and so those people who like and a lot of them arestate of the art like somebody said oh I
 tried looking at Devon Gehry characters and I couldn't believe it the thing thatcame out was more accurate than the best
 academic paper after lesson one and then there's others which are just more kindof fun like somebody who's doing
 Trinidad and Tobago hummingbirds she said that's kind of their national birdand she's got something that can now
 classify Trinidad and Tobago hummingbirds so yeah train modelsfine-tune models with your data set and
 then study their inputs and outputs

1:24:05
Speaker 1 :how much is fast there of course is free


1:24:07
Speaker 0 :everything we do is free we have no
 revenue sources of any kind it's just a service to the community you're a saint

1:24:15
Speaker 1 :okay once the person understands the
 basics trains a bunch of models if we look at the scale of years what advicedo you have for someone wanting to
 eventually become an expert train lots

1:24:29
Speaker 0 :of models train lots of models in your
 domain area so an expert what right we don't need more expert likecreate slightly evolutionary research an
 area that everybody's studying we need experts at using deep learning todiagnose malaria well we need experts at
 using deep learning to analyze language to study media bias so we need expertsin analyzing fisheries to identify
 problem areas and you know the ocean you know that that's that's what we need solike become the expert in your passion
 area and this is a tool which you can use just about anything and you'll beable to do that thing better than other
 people particularly by combining it with your passion and domain expertise so

1:25:28
Speaker 1 :that's really interesting even if you do
 want to innovate on transfer learning or active learning your thought is thatmeans one I certainly share is you also
 need to find a domain or dataset that you actually really care for right if

1:25:42
Speaker 0 :you're not working on a real problem
 that you understand how do you know if you're doing it any good you know how doyou know if your results so good how do
 you know if you're getting bad results why you're getting bad results is it aproblem with the data or is like how do
 you know you're doing anything useful yeah the only to me the only reallyinteresting research is not the only but
 the vast majority of interesting research is like try and solve an actualproblem and solve it really well so both


1:26:07
Speaker 1 :understanding sufficient tools and the
 deep learning side and becoming a domain expert in a particular domain I reallythinks will then reach for anybody yeah


1:26:18
Speaker 0 :I mean to me I would compare it to like
 studying self-driving cars having never looked at a car or being in a car orturn the car on right you know which is
 like the way it is for a lot of people they'll study some academic data setwhere they literally have no idea about
 the other way I'm not sure how familiar

1:26:36
Speaker 1 :with the thomas vehicles but that is
 literally you describe a large percentage of robotics folks working ina self-driving cars as they actually
 haven't considered driving they haven't actually looked at what driving looksright they haven't driven it goes and
 enterprise because you know when you've

1:26:52
Speaker 0 :actually driven
 you know like these are the things that happened to me and I was driving it so

1:26:57
Speaker 1 :there's nothing that beats the
 real-world examples are just experiencing them you've created manysuccessful startups what does it take to


1:27:07
Speaker 0 :create a successful startup same thing
 is becoming successful deep learning practitioner which is not getting up soyou can
 right out of money or time or run out of something you knowbut if you keep costs super low and try
 and save up some money beforehand so you can afford to have some time then juststicking with it it's one important
 thing doing something you understand and care about is important that bysomething I don't mean the biggest
 problem I see with deep whining people is they do a PhD in deep learning andthen they try and commercialize their
 PhD it is a waste of time because that doesn't solve an actual problem youpicked your PhD topic because it was an
 interesting kind of engineering or math or research exercise but yeah if you'veactually spent time as a recruiter and
 you know that most of your time was spent sifting through resumes and youknow that most of the time you're just
 looking for certain kinds of things and you can try doing that with a model fora few minutes and see whether that
 something which your models be able to do as well as you could then you're onthe right track to creating a startup
 and then I think just yeah being just be pragmatic andtrain state
 to capital money as long as possible preferably forever so yeah on that point

1:28:39
Speaker 1 :do you venture capital so did you were
 able to successfully run startups with was self-funded yeah my first two was

1:28:48
Speaker 0 :self-funded and that was the right way
 to do it that's scary no species startups aremuch more scary because you have these
 people on your back who do this all the time and who have done it for yearstelling you grow grow grow grow and I
 don't they don't care if you fail they only care if you don't grow fast enoughso that's scary
 where else doing the ones myself well with with partners who were friends isnice because like we just went along at
 a pace that made sense and we were able to build it to something which was bigenough that we never had to work again
 but was not big enough that any VC would think it was impressive and that wasenough for us to be excited you know so
 I I thought that's a much better way to do things and most people in generally

1:29:40
Speaker 1 :speaking that for yourself but how do
 you make money during that process do you cut into savings if I guess so yeahso fir


1:29:48
Speaker 0 :so I started fast mail and optimal
 decisions at the same time in 1999 with two different friends and for fast mailI guess I spent $70 a month on the
 server and when the server ran out of space I put a payments button on thefront page and said if you want more
 than 10 makerspace you have to pay $10 a yeah and so run low like keep your cost

1:30:16
Speaker 1 :down yes I came across town and once you


1:30:19
Speaker 0 :know once once I needed to spend more
 money I asked people to spend the money for me and that that was that basicallyfrom then on oh we were making money and
 I was profitable from then for optimal decisions it was a bit harder because wewere trying to sell something that was
 more like a 1 million dollar sale but what we did was we would sell scopingprojects so kind of like prototype he
 projects but rather than to be free we would sell them 50 to $100,000 so againwe were covering our costs and also
 making the client feel like we were doing something valuable so in bothcases we were profitable from six months


1:31:05
Speaker 1 :in yeah nevertheless is scary I mean


1:31:09
Speaker 0 :yeah sure it's it's Gary before you jump
 in and I just I guess I was comparing it to this scariness of VC I felt like withVC stuff it was more scary you kind of
 much more in somebody else's hands you know will they fund you or not and whatdo they think of what you're doing I
 also found it very difficult with VC's bet startups to actually do the thingwhich I thought was important for the
 company rather than doing the thing which I thought would make the VC happynow VCS always tell you not to do the
 thing that makes them happy but then if you don't do the thing that makes themhappy they get set so and do you think


1:31:46
Speaker 1 :optimizing for the whatever they call it
 they exit is uh as a good thing to optimize for I think

1:31:54
Speaker 0 :I can be but not at the VC level because
 the VC exit needs to be you know a thousand x so where else the lifestyleexit if you can sell something for ten
 million dollars I think you've made it right so I don't it depends if you wantto build something that's gonna you kind
 of happy to do forever then fine if you want to build somethingyou want to sell then three is time
 that's fine too I mean they're both perfectly good outcomes so you're

1:32:21
Speaker 1 :learning Swift now in a way I mean you
 were a writer and I read that you use at least in some cases spaced repetition asa mechanism for learning new things yeah
 I used Anki quite a lot yourself sure I

1:32:37
Speaker 0 :

1:32:38
Speaker 1 :actually don't never talk to anybody
 about it don't don't know how many people do it but it works incrediblywell for me can you talk to your
 experience like how did you what what do you like first of all okay let's back itup what is space repetition so spaced


1:32:55
Speaker 0 :repetition is an idea created by a
 psychologist named Epping house must be a couple hundred years ago or somethinghundred and fifty years ago he did
 something which sounds pretty damn tedious he wrote down random sequencesof letters on cards and tested how well
 he would remember those random sequences a day later or a week later whatever hediscovered that there was this kind of a
 curve where his probability of remembering one of them would bedramatically smaller the next day and
 then a little bit smaller the next day a little bit smaller next day what hediscovered is that if he revised those
 cards after a day the probabilities would decrease at a smaller rate andthen if he revised them again a week
 later they would decrease it a smaller rate again and so he basically figuredout a roughly optimal equation for when
 you should revise something you want to remember so spaced repetition learningis using this simple algorithm just
 something like revise something after a day and then three days and then a weekand then three weeks and so forth
 and so if you use a program like Anki as you know it will just do that for youand if you and it will say did you
 remember this and if you say no it will reschedule it back to be up here againlike ten times faster than it otherwise
 would have it's a kind of a way of being guaranteed to learn somethingbecause by definition if you're not
 learning it it will be rescheduled to be revised more quickly unfortunatelythough it's also like it doesn't let you
 for yourself if you not learning something you you know like it yourrevisions will just get more and more so
 you have to find ways to learn things productively and effectively like treatyour brain well so using like mnemonics
 and stories and context and stuff like that so yeah it's it's a super greattechnique is like learning how to loan
 is something which everybody should learn before they actually learnanything but almost nobody does what


1:35:08
Speaker 1 :have you so certainly works well for
 learning new languages for I mean for learned like small projects almost butdo you you know I started using it for
 if you had who wrote a blog post about this inspired meI went Ben you I'm not sure is I started
 when I read papers all all concepts and ideas I'll put them was it Michael

1:35:31
Speaker 0 :Nelson in my Illinois muscle strains
 that Michael started doing this recently and he's been writing about it I so thekind of today's evening house is a guy
 called Peter was niak who developed a system called super memo and he's beenbasically trying to become like the
 world's greatest Renaissance man over the last few decades he's basicallylived his life with spaced repeated
 repetition learning for everything I and sort of like Michaels only very recentlygot into this but he started really
 getting excited about doing it for a lot of different things for me personally Iactually don't use it for anything
 except Chinese and the reason for that is that Chinese is specifically a thingI made a conscious decision that I want
 to continue to remember even if I don't get much of a chance to exercise itbecause like I'm not often in China so I
 I don't or else something like programming languages or papers I have avery different approach which is I try
 not to learn anything from them but instead Itry to identify the important concepts
 and like actually ingest them so like really understand that concept deeplyand study it carefully I will decide if
 it really is important if it is like incorporated into our libraryyou know incorporated into how I do
 things or decide it's not worth it say so Ifind I find I didn't remember the things
 that I care about because I'm using it all the time so I've fell at last 25years I've committed to spending at
 least half of everyday learning or practicing something new which is all mycolleagues have always hated because it
 always looks like I'm not working I mean if what I meant to be working on but italways means I do everything faster
 because I've been practicing a lot of stuff so I kind of give myself a lot ofopportunity to practice new things and
 so I find now I don't yeah I don't often kind of find myselfwishing I could remember something
 because if it's something that's useful then I've been using it a lot that'seasy enough to look it up on google fit
 speaking Chinese you can't look it up on Google so do you have advice for people

1:37:59
Speaker 1 :learning new things so if you what have
 you learned is a process does it I mean it all starts is just making the hoursin the day available yeah you gotta


1:38:09
Speaker 0 :stick with it which is again the number
 one thing that 99% of people don't do so the people I started learning Chinesewith none of them were still doing it
 twelve months later I'm still doing a ten years later I tried to stay in touchwith them but they just no one did it
 yeah for something like Chinese like study how human learning works so myevery one of my Chinese flashcards is
 associated with a story and that story is specifically designed to be memorableand we find things memorable which are
 like funny or disgusting or sexy or related to people that we know will careabout so I try to make sure all those
 stories that are in my head have those characteristicsyeah so you have to you know you won't
 remember things well if they don't have some context and yeah you won't rememberthem well if you don't regularly
 practice them whether it be just part of your day to day life or the Chinese forme flashcards I mean the other thing is
 I'll let yourself fail sometimes so like I've had various medicalproblems over the last few years and
 basically my flashcards just stopped for about three years and then they've beenother times I've stopped for a few
 months and it's so hard because you get back to it and it's like you have 18,000cards June and so you just have to go
 alright well I can either stop and give up everything or just decide to do thisevery day for the next two years until I
 get back to it the amazing thing has been that evenafter three years I you know the Chinese
 were still in there like yeah it was so much faster to relearn than it was tolearn the first time yeah absolutely


1:39:50
Speaker 1 :it's it's in there the same with with
 guitar with music and so on it's sad because the work sometimes takes awayand then you won't play for a year but
 really if you then just get back to it every day you're right through rightthere again what do you think is the
 next big breakthrough in artificial intelligence what are your hopes in deeplearning or beyond that people should be
 working on or you hope there'll be breakthroughs I don't think it's

1:40:16
Speaker 0 :possible to predict I think yeah I think
 what we already have is an incredibly powerful platform to solve lots ofsocietally important problems that are
 currently unsolved so I just hope that people will lots of people will learnthis toolkit and try to use it I don't
 think we need a lot of new technological breakthroughs to do a lot of great workright now and when do you think we're


1:40:39
Speaker 1 :going to create a human level
 intelligence system do you think know how hard is it how far away are we don'tknow don't have no way to know I don't


1:40:50
Speaker 0 :know like I don't know why people make
 predictions about this because there's no data and nothing to go on and theSenate that's right it's just like
 there's so many societally important problems to solveright now I just don't find it a really
 interesting question to even answer so

1:41:10
Speaker 1 :in terms of societally important
 problems what's the problem well is within reached for it

1:41:16
Speaker 0 :well I mean for example there are
 problems that AI creates right so most specificallylabor force displacement is going to be
 huge and people keep making this frivolous econometrics argument of beinglike oh there's been other things that
 aren't AI that have come along before and haven't created massive labor forcedisplacement therefore AI want it slow


1:41:40
Speaker 1 :so there's a serious concern for you oh
 yeah Andrew yang is running on it yeah

1:41:43
Speaker 0 :it's it's it's I'm desperately concerned
 and you see already that the changing workplace has lived to a hollowing outof the middle class you're seeing that
 students coming out of school today have a less rosy financial future ahead ofthem and the parents did which has never
 happened in recent in the last few hundred years you know we've always hadprogress before and you see this turning
 into anxiety and despair and and even violence so I very much worry about that

1:42:23
Speaker 1 :quite a bit about ethics too I do think


1:42:26
Speaker 0 :that every data scientist working with
 deep learning needs to recognize they have an incredibly high leverage toolthat they're using that can influence
 society in lots of ways and if they're doing research that that research isgoing to be used by people doing this
 kind of work and they have a responsibility to consider theconsequences and to think about things
 like how will humans be in the loop here how do we avoid runaway feedback loopshow do we ensure an appeals process for
 humans that are impacted by my algorithm how do I ensure that the constraints ofmy algorithm are ethically explained to
 the people that end up using them there's all kinds of human issues whichonly data scientists are actually in the
 right place to educate people about but data scientists tend to think ofthemselves as just engineers and that
 they don't need to be part of that process just know yeah which is wrong

1:43:26
Speaker 1 :well you're in the perfect position to
 educate them better to read literature to read history to learn from history

1:43:34
Speaker 0 :

1:43:35
Speaker 1 :well Jeremy thank you so much for
 everything you do for inspiring huge amount of people getting them into deeplearning and having the ripple effects
 the the flap of a butterfly's wings that will probably change the world so thank

