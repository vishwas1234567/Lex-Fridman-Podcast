{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open playlist page and load html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lex_Podcast_Playlist_Link = \"https://www.youtube.com/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(Lex_Podcast_Playlist_Link)\n",
    "\n",
    "# We need to load the full page and extract the html to get the titles, URLs and other details. \n",
    "#To do so, we need to scroll \n",
    "# through the page when opening it. The following code serves this purpose. \n",
    "\n",
    "for i in range(10):\n",
    "    driver.execute_script('window.scrollTo(0,(window.pageYOffset+3000))')\n",
    "    time.sleep(1)\n",
    "\n",
    "Page_Source_HTML = str(driver.page_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract video details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might very well be the worst piece of code you read this week, or even this year, but I couldn't be bothered to look for a more elegant version to do it. I manually looked through the html code and found the strings systematically preceding the video titles, times and urls. From then, I looked for the occurences of these strings and got the details after. If you have a better way to do this, do not hesitate to message me and I will modify it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all(a_str, sub):\n",
    "    start = 0\n",
    "    while True:\n",
    "        start = a_str.find(sub, start)\n",
    "        if start == -1: return\n",
    "        yield start\n",
    "        start += len(sub) # use start += 1 to find overlapping matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract videos URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_find = str('nline-block style-scope ytd-thumbnail\" aria-hidden=\"true\" tabindex=\"-1\" rel=\"null\" href=')\n",
    "Raw_URLs = find_all(Page_Source_HTML, string_to_find)\n",
    "URLs = []\n",
    "for url in Raw_URLs:\n",
    "    Individual_URL = 'https://www.youtube.com/' + str(Page_Source_HTML[url+90:url+110].split(';')[0])\n",
    "    URLs.append(Individual_URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extact videos Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_find = 'video-title\" class=\"style-scope ytd-playlist-video-renderer\" aria-label='\n",
    "Raw_Titles = find_all(Page_Source_HTML, string_to_find)\n",
    "Titles = []\n",
    "for t in Raw_Titles:\n",
    "    Individual_Title = Page_Source_HTML[t+73:t+200].split('by')[0]\n",
    "    Titles.append(Individual_Title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract videos Durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_find = 'ytd-thumbnail-overlay-time-status-renderer\" aria-label='\n",
    "Raw_Durations = find_all(Page_Source_HTML, string_to_find)\n",
    "Durations = []\n",
    "for dur in Raw_Durations:\n",
    "    Individual_Duration = Page_Source_HTML[dur+56:dur+100].split('\">')[0]\n",
    "    Durations.append(Individual_Duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking everything makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 140 available podcasts by Lex!\n",
      "Here are some example for sanity check:\n",
      "\n",
      "\n",
      "Title: Jim Keller: Moore's Law, Microprocessors, and First Principles | Lex Fridman Podcast #70   - Duration: 1 hour, 34 minutes, 44 seconds - URL: https://www.youtube.com/watch?v=Nb2tebYAaOA&\n",
      "\n",
      "Title: Elon Musk: Neuralink, AI, Autopilot, and the Pale Blue Dot | Lex Fridman Podcast #49   - Duration: 36 minutes, 10 seconds - URL: https://www.youtube.com/watch?v=smK9dgdTl40&\n",
      "\n",
      "Title: David Chalmers: The Hard Problem of Consciousness | Lex Fridman Podcast #69   - Duration: 1 hour, 38 minutes, 49 seconds - URL: https://www.youtube.com/watch?v=LW59lMvxmY4&\n",
      "\n",
      "Title: Brian Kernighan: UNIX, C, AWK, AMPL, and Go Programming | Lex Fridman Podcast #109   - Duration: 1 hour, 43 minutes, 10 seconds - URL: https://www.youtube.com/watch?v=O9upVbGSBFo&\n",
      "\n",
      "Title: Matt Botvinick: Neuroscience, Psychology, and AI at DeepMind | Lex Fridman Podcast #106   - Duration: 2 hours, 32 seconds - URL: https://www.youtube.com/watch?v=3t06ajvBtl0&\n"
     ]
    }
   ],
   "source": [
    "if len(Durations)!= len(URLs) or len(URLs) != len(Titles):\n",
    "    print('Something is wrong... ')\n",
    "else:\n",
    "    print('We have', len(URLs), 'available podcasts by Lex!')\n",
    "    print('Here are some example for sanity check:\\n')\n",
    "    ran = np.int_(np.random.rand(5,)*100)\n",
    "    for r in ran:\n",
    "        print('\\nTitle:', Titles[r], ' - Duration:', Durations[r], '- URL:', URLs[r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save details for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "URLs_File = open('URLs_Lex_Fridman.pkl', 'wb')\n",
    "pickle.dump(URLs, URLs_File)\n",
    "URLs_File.close()\n",
    "\n",
    "Titles_File = open('Titles_Lex_Fridman.pkl', 'wb')\n",
    "pickle.dump(Titles, Titles_File)\n",
    "Titles_File.close()\n",
    "\n",
    "\n",
    "Durations_File = open('Durations_Lex_Fridman.pkl', 'wb')\n",
    "pickle.dump(Durations, Durations_File)\n",
    "Durations_File.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
