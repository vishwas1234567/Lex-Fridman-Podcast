{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydub\n",
    "import numpy as np\n",
    "import youtube_dl\n",
    "import pickle as pkl\n",
    "from pydub import AudioSegment\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "from spectralcluster import SpectralClusterer\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, 'C:\\\\Users\\\\ybenc\\\\Documents\\\\Lex_Fridman_Podcasts\\\\Resemblyzer-master')  ## The resemblyzer \n",
    "## module is publicly available at https://github.com/resemble-ai/Resemblyzer\n",
    "# I am not importing it as I do with other modules because I have cloned the repo and modified it to match the needs of this \n",
    "# project. Mainly, I have modified the maximum silence duration allowed to avoid removing silences and modifying podcast time. \n",
    "# For more info about how I used this module and proceeded with the diarization, check out this excellent tutorial:\n",
    "# https://medium.com/saarthi-ai/who-spoke-when-build-your-own-speaker-diarization-module-from-scratch-e7d725ee279\n",
    "\n",
    "from resemblyzer import *\n",
    "import re\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function turns the length of the youtube video, declared in special\n",
    "## format into a time in seconds\n",
    "def Convert_Time_To_Seconds(Time):\n",
    "    Decomposed_Time = np.int_(re.findall(r'\\d+', Time)) \n",
    "    if len(Decomposed_Time) == 2: ##Podcast shorter than 1 hour\n",
    "        Minutes = Decomposed_Time[0]\n",
    "        Seconds = Decomposed_Time[1]\n",
    "        Total_Time = Minutes*60+Seconds\n",
    "    elif len(Decomposed_Time) == 3: ##Podcast longer than 1 hour\n",
    "        Hours = Decomposed_Time[0]\n",
    "        Minutes = Decomposed_Time[1]\n",
    "        Seconds = Decomposed_Time[2]\n",
    "        Total_Time = Hours*3600 + Minutes*60 + Seconds\n",
    "    else:\n",
    "        print('Something fishy happened with this:', Time)\n",
    "    return Total_Time\n",
    "\n",
    "def Convert_Seconds_To_Human_Time(Time):\n",
    "\n",
    "    Int_Time = np.int_(Time) # get rid of any miliseconds and such \n",
    "    \n",
    "    if Time > 3600: \n",
    "        Hour = np.int_(Int_Time/60/60)\n",
    "        Minute = Int_Time%60\n",
    "        Second = Int_Time%3600\n",
    "        return str(str(Hour) + ':', str(Second) + ':', str(Second))\n",
    "    elif Time < 3600:\n",
    "        Minute = np.round((Int_Time/60, 0))\n",
    "        Second = np.round((Int_Time/60 - Minute)*60, 0)\n",
    "        return str(str(Minute) + ':' + str(Second))\n",
    "\n",
    "def find_nearest_index (Array, value):\n",
    "    #\"Element in nd 'Array' closest to the scalar 'value'\"\n",
    "    idx = np.abs(np.array(Array) - value).argmin()\n",
    "    return Array.index(np.array(Array).flat[idx])\n",
    "\n",
    "def Get_Youtube_Download_Parameters(filename):\n",
    "    filepath = str('Podcasts_Audio_Files/' + filename + '.wav')\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'quiet': True,\n",
    "        'outtmpl': filepath,\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'wav',\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "    }\n",
    "    return ydl_opts, filepath\n",
    "\n",
    "def Audio_File_Processing(filename):\n",
    "    ## I have modified the source code of the resemblyzer module to avoid having deleted silences, which is what is \n",
    "    ## normally done in audio processing etc.. to save up space. Here, we keep even long moments of silences to avoid \n",
    "    ## losing track of the right time and not being able to correctly assign text to diarized speech\n",
    "    audio_file_path = 'Podcasts_Audio_Files\\\\' + filename + '.wav'\n",
    "    wav = preprocess_wav(audio_file_path)\n",
    "    encoder = VoiceEncoder(\"cpu\", verbose = False)\n",
    "    _, cont_embeds, wav_splits = encoder.embed_utterance(wav, return_partials=True)\n",
    "    del wav, _, wav_splits #Deleting unused variable to save memory\n",
    "    \n",
    "    return cont_embeds\n",
    "\n",
    "def Spectral_Clustering(embedding):\n",
    "    clusterer = SpectralClusterer(\n",
    "        min_clusters=2,\n",
    "        max_clusters=3,\n",
    "        p_percentile=0.95,\n",
    "        gaussian_blur_sigma=1) #Sometimes, Lex's voice will be recognized as different between the introduction and the conv\n",
    "            # we hence put between 2 and 3 clusters\n",
    "    labels = clusterer.predict(embedding)\n",
    "    return labels\n",
    "\n",
    "\n",
    "## This functions turns the predictions from the clustering algorithm to a [[start_time, speaker]] array, with each row\n",
    "## representing a change in speaker\n",
    "def From_Clustering_To_Time_And_Speaker_Segmenting(predictions, video_duration_in_seconds):\n",
    "    \n",
    "    n_samples = predictions.shape[0]\n",
    "    sampling_rate = n_samples/video_duration_in_seconds #how many sample per second\n",
    "    \n",
    "    time_and_speaker_segment = []\n",
    "    time_and_speaker_segment.append([0, predictions[0]]) # Append beginning of the audio file and first Speaker\n",
    "    \n",
    "    for i in range(predictions.shape[0]):\n",
    "        second =  i/sampling_rate \n",
    "        if i>0 and local_prediction[i] != local_prediction[i-1]: #If change in speaker, write new line. \n",
    "            time_and_speaker_segment.append([second, predictions[i]])\n",
    "            \n",
    "    return time_and_speaker_segment\n",
    "\n",
    "def Get_Podcast_Details(url, duration, title):\n",
    "    local_guest = title.split(':')[0]\n",
    "    local_podcast_number = title.split('#')[1].split(' ')[0]\n",
    "    local_theme = title.split(': ')[1].split(' |')[0]\n",
    "    local_filename = str(local_guest + ' - ' + local_theme + ' - ' + local_podcast_number)\n",
    "\n",
    "    return url, duration, local_guest, local_podcast_number, local_theme, local_filename\n",
    "\n",
    "def Save_As_Pickle(Variable, Folder, File):\n",
    "    with open(str(Folder + File + '.pickle'), 'wb') as f:\n",
    "        pkl.dump(Variable, f)\n",
    "        \n",
    "def Diarize_Transcript(segmented_time_speaker, URL, guest, folder, filename):\n",
    "    \n",
    "    #This function is about fusing the clustering predictions we have with the youtube transcripts. \n",
    "    #Youtube send the transcripts as segments of speechs of usually 3-6 seconds. Issues will often occur with beginning\n",
    "    # and ends of speech segments which will not be attributed to the correct speaker due to the mismatch between youtube\n",
    "    # segment times and clustering times by our diarization algorithm\n",
    "    \n",
    "    youtube_transcript = YouTubeTranscriptApi.get_transcript(URL.split('v=')[1]) #getting transcript\n",
    "    \n",
    "    speech_segment_starts = [] #This is start time of each segment of speech (usually 3-6 seconds segments)\n",
    "    speech_segment_texts = [] #This is the speech segment\n",
    "    \n",
    "    for speech_segment in youtube_transcript:\n",
    "        speech_segment_starts.append(speech_segment['start'])\n",
    "        speech_segment_texts.append(speech_segment['text'])\n",
    "        \n",
    "    # These are text only transcripts   \n",
    "    with open(str('Diarization/Transcripts/' + filename + '.txt'), 'a') as f1:\n",
    "        \n",
    "        for i in range(len(segmented_time_speaker) - 1):\n",
    "            local_speaker = str('Speaker ' + str(segmented_time_speaker[i][1]) + ' ')\n",
    "\n",
    "            intervention_start_time = segmented_time_speaker[i][0]\n",
    "            intervention_end_time = segmented_time_speaker[i+1][0]\n",
    "\n",
    "            transcript_start_index = find_nearest_index(speech_segment_starts, intervention_start_time)\n",
    "            transcript_end_index = find_nearest_index(speech_segment_starts, intervention_end_time)\n",
    "\n",
    "            intervention_text = ''\n",
    "            for k in range(transcript_end_index-transcript_start_index):\n",
    "                if k%3 == 0: ## This is to have new lines every once in a while to avoid having long lines on text files\n",
    "                    intervention_text += str(speech_segment_texts[transcript_start_index+k] + '\\n')\n",
    "                else:\n",
    "                    intervention_text += str(' ' + speech_segment_texts[transcript_start_index+k])\n",
    "                    \n",
    "            f1.write(local_speaker + ':' + intervention_text)\n",
    "            f1.write('\\n\\n')\n",
    "    f1.close()\n",
    "    \n",
    "    # These are text only transcripts   \n",
    "    with open(str('Diarization/Transcripts_With_Time/' + filename + '.txt'), 'a') as f2:\n",
    "        \n",
    "        for i in range(len(segmented_time_speaker) - 1):\n",
    "            local_speaker = str('Speaker ' + str(segmented_time_speaker[i][1]) + ' ')\n",
    "\n",
    "            intervention_start_time = segmented_time_speaker[i][0]\n",
    "            intervention_end_time = segmented_time_speaker[i+1][0]\n",
    "            \n",
    "            transcript_start_index = find_nearest_index(speech_segment_starts, intervention_start_time)\n",
    "            transcript_end_index = find_nearest_index(speech_segment_starts, intervention_end_time)\n",
    "            \n",
    "            human_start_time = str(datetime.timedelta(seconds= int(intervention_start_time)))\n",
    "\n",
    "            intervention_text = ''\n",
    "            for k in range(transcript_end_index-transcript_start_index):\n",
    "                if k%3 == 0: ## This is to have new lines every once in a while to avoid having long lines on text files\n",
    "                    intervention_text += str(speech_segment_texts[transcript_start_index+k] + '\\n')\n",
    "                else:\n",
    "                    intervention_text += str(' ' + speech_segment_texts[transcript_start_index+k])\n",
    "                    \n",
    "            f2.write(human_start_time + '\\n')\n",
    "            f2.write(local_speaker + ':' + intervention_text)\n",
    "            f2.write('\\n\\n')\n",
    "    f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load URLs, podcast titles and duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 140 podcasts\n"
     ]
    }
   ],
   "source": [
    "with open('URLs_Lex_Fridman.pkl', 'rb') as f:\n",
    "    URLs = pkl.load(f)\n",
    "    \n",
    "with open('Titles_Lex_Fridman.pkl', 'rb') as f:\n",
    "    Titles = pkl.load(f)\n",
    "    \n",
    "with open('Durations_Lex_Fridman.pkl', 'rb') as f:\n",
    "    Verbose_Times = pkl.load(f)\n",
    "    \n",
    "Times_In_Seconds = []\n",
    "for verbose_ti in Verbose_Times:\n",
    "    Times_In_Seconds.append(Convert_Time_To_Seconds(verbose_ti))\n",
    "    \n",
    "print('There are', len(Titles), 'podcasts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each podcast, we download the audio from youtube into a wav file and run the diarization process on it. The diarization process is done in two main phases: 1) audio data preprocessing and 2) spectral clustering to separate between two distinct speakers. \n",
    "We then save the predictions into distinct variables named after the podcast, with speaker information (speaker 0 or speaker 1) and start of each speaker intervention. \n",
    "\n",
    "The diarization is then fused with the youtube transcripts to generate the full conversation - this is done in the next section of this notebook\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Buggy_Podcasts = []\n",
    "for i in range(len(URLs)):\n",
    "    \n",
    "    #Getting podcast details\n",
    "    local_url, local_duration_in_seconds, local_guest, local_podcast_number, local_theme, \\\n",
    "                        local_filename = Get_Podcast_Details(URLs[i], Times_In_Seconds[i], Titles[i])\n",
    "    \n",
    "    if not os.path.exists(str('Diarization/Transcripts/' + str(local_filename + '.txt'))): ## If transcript has not already been generated:\n",
    "    \n",
    "        try:\n",
    "\n",
    "            YouTubeTranscriptApi.get_transcript(local_url.split('v=')[1]) ## Check if Youtube Transcript exists\n",
    "\n",
    "            print(i, '- Diarizing podcast', local_podcast_number, 'with guest:', local_guest, 'and theme:', local_theme)\n",
    "\n",
    "            print('Download Youtube audio into a wav file...')\n",
    "            youtube_download_options, local_file_path = Get_Youtube_Download_Parameters(local_filename) #This get the download parameters\n",
    "            with youtube_dl.YoutubeDL(youtube_download_options) as ydl:\n",
    "                ydl.download([local_url])\n",
    "\n",
    "            print('Audio preprocessing...')\n",
    "            local_embedding = Audio_File_Processing(local_filename) \n",
    "            Save_As_Pickle(local_embedding, 'Diarization/Embeddings/', local_filename)\n",
    "\n",
    "            print('Spectral clustering...')\n",
    "            local_prediction = Spectral_Clustering(local_embedding)\n",
    "            Save_As_Pickle(local_prediction, 'Diarization/Clustering_Predictions/', local_filename)\n",
    "            del local_embedding ##Deleting variables to free up memory\n",
    "\n",
    "            print('Getting the predictions in a segmented [[start_time, speaker]] format...')\n",
    "            segmented_time_and_speaker = From_Clustering_To_Time_And_Speaker_Segmenting(local_prediction, local_duration_in_seconds)\n",
    "            Save_As_Pickle(segmented_time_and_speaker, 'Diarization/Segmented/', local_filename)\n",
    "\n",
    "            print('Generating diarized transcript...')\n",
    "            Diarize_Transcript(segmented_time_and_speaker, local_url, local_guest, 'Diarization/Transcripts/', local_filename)\n",
    "\n",
    "            print('Completed this transcription.')\n",
    "\n",
    "        except:\n",
    "            print('----Could not generate transcript of podcast:', local_podcast_number, 'with guest:', local_guest)\n",
    "            print('----This is likely because there was no available youtube transcript of this podcast')\n",
    "            Buggy_Podcasts.append(Titles[i])\n",
    "        print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Buggy_Podcasts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
